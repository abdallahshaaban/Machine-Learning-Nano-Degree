{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.estimator package not installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-fa795f0e3825>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwavfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m  \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[1;31m# resolution to succeed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[1;31m# pylint: disable=undefined-variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mpython\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;31m# pylint: enable=undefined-variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import  Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#l=['silence','unknown']\n",
    "file_path=\"audio\"\n",
    "max_pad_length=40\n",
    "random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_wavfile(path=\"audio/go/0d53e045_nohash_1.wav\"):\n",
    "    \n",
    "    sample_rate, samples = wavfile.read(path)\n",
    "    frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
    "    plt.pcolormesh(times, frequencies, spectrogram)\n",
    "    plt.imshow(spectrogram)\n",
    "    plt.ylabel('Frequecy [HZ]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_wave_to_mfcc(filePath):\n",
    "    wav, sr =librosa.load(filePath, mono = True, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(wav, sr=sr)\n",
    "    ##padding\n",
    "    mfcc_feature = np.pad(mfcc, pad_width=((0, 0), (0, max_pad_length - mfcc.shape[1])), mode='constant')\n",
    "    return mfcc_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(path=file_path):\n",
    "    return os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mfcc_files(path=file_path):\n",
    "    labels = get_labels(path)\n",
    "    #print(labels)\n",
    "    for label in labels:\n",
    "        mfcc_features=[]\n",
    "        audfiles=[path+'/'+label+'/'+aud for aud in os.listdir(path+'/'+label)]\n",
    "        for audio in audfiles:\n",
    "            mfcc_features.append(convert_wave_to_mfcc(audio))\n",
    "        #print(len(mfcc_vectors))\n",
    "        np.save(label+'.npy', mfcc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_labels()\n",
    "#print(len(get_labels()))\n",
    "#save_mfcc_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_testing_set(path=file_path,test_size=.3):\n",
    "    labels = get_labels(path)\n",
    "    #features=[]\n",
    "    features=np.load(labels[0]+'.npy')\n",
    "    classes=np.zeros(features.shape[0])\n",
    "    #for i,label in zip(range(0,len(labels)-1),labels):\n",
    "    for i in range(1,len(labels)):\n",
    "        x=np.load(labels[i]+'.npy')\n",
    "        features=np.vstack((features,x))\n",
    "        classes=np.append(classes, np.full(x.shape[0], fill_value= (i)))\n",
    "    return train_test_split(features, classes, test_size= test_size, random_state=random_state, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_testing_set1(path=file_path,test_size=.3):\n",
    "    labels=get_labels(path)\n",
    "    features=np.load(labels[0]+'.npy')\n",
    "    classes=np.zeros(features.shape[0])\n",
    "    print(classes[1])\n",
    "    feature_train, feature_test, labels_train, labels_test = train_test_split(features, classes,\n",
    "                                        test_size= test_size, random_state=random_state, shuffle=True)\n",
    "    for i in range(1,len(labels)):\n",
    "        x=np.load(labels[i]+'.npy')\n",
    "        #features=np.vstack((features,x))\n",
    "        y= np.full(x.shape[0], fill_value= (i))\n",
    "        #print(labels[i])\n",
    "        X_train, X_test, y_train, y_test=train_test_split(x, y,\n",
    "                                    test_size= test_size, random_state=random_state, shuffle=True)\n",
    "     \n",
    "        \n",
    "        feature_train=np.vstack((feature_train,X_train))\n",
    "        feature_test=np.vstack((feature_test,X_test))\n",
    "       \n",
    "        labels_train=np.append(labels_train,y_train)\n",
    "        labels_test=np.append(labels_test,y_test)\n",
    "        \n",
    "    return feature_train, feature_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = generate_training_testing_set()\n",
    "\n",
    "model =Sequential()\n",
    "model.add(Conv2D(32,kernel_size=(2,2),activation='relu',input_shape=(20,40,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Conv2D(64,kernel_size=(2,2),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Conv2D(128,kernel_size=(2,2),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(12,activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.adadelta(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19680 19680\n"
     ]
    }
   ],
   "source": [
    "print ( len(X_train), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13776 samples, validate on 5904 samples\n",
      "Epoch 1/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 8.7470 - acc: 0.0973Epoch 00001: val_loss improved from inf to 2.45424, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 2s 164us/step - loss: 8.5406 - acc: 0.0972 - val_loss: 2.4542 - val_acc: 0.0774\n",
      "Epoch 2/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.5021 - acc: 0.1001Epoch 00002: val_loss improved from 2.45424 to 2.43158, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.5007 - acc: 0.1005 - val_loss: 2.4316 - val_acc: 0.0969\n",
      "Epoch 3/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4508 - acc: 0.1089Epoch 00003: val_loss improved from 2.43158 to 2.42916, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 2.4510 - acc: 0.1087 - val_loss: 2.4292 - val_acc: 0.1114\n",
      "Epoch 4/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4402 - acc: 0.1064Epoch 00004: val_loss improved from 2.42916 to 2.42805, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 2.4399 - acc: 0.1061 - val_loss: 2.4281 - val_acc: 0.1116\n",
      "Epoch 5/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4352 - acc: 0.1095Epoch 00005: val_loss improved from 2.42805 to 2.42750, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 2.4347 - acc: 0.1092 - val_loss: 2.4275 - val_acc: 0.1116\n",
      "Epoch 6/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4325 - acc: 0.1070Epoch 00006: val_loss improved from 2.42750 to 2.42707, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 2.4324 - acc: 0.1071 - val_loss: 2.4271 - val_acc: 0.1116\n",
      "Epoch 7/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4306 - acc: 0.1079Epoch 00007: val_loss improved from 2.42707 to 2.42673, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.4316 - acc: 0.1075 - val_loss: 2.4267 - val_acc: 0.1116\n",
      "Epoch 8/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4302 - acc: 0.1085Epoch 00008: val_loss improved from 2.42673 to 2.42633, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.4300 - acc: 0.1084 - val_loss: 2.4263 - val_acc: 0.1116\n",
      "Epoch 9/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4284 - acc: 0.1081Epoch 00009: val_loss improved from 2.42633 to 2.42610, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 105us/step - loss: 2.4285 - acc: 0.1076 - val_loss: 2.4261 - val_acc: 0.1116\n",
      "Epoch 10/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4289 - acc: 0.1078Epoch 00010: val_loss improved from 2.42610 to 2.42584, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 105us/step - loss: 2.4277 - acc: 0.1082 - val_loss: 2.4258 - val_acc: 0.1116\n",
      "Epoch 11/250\n",
      "13056/13776 [===========================>..] - ETA: 0s - loss: 2.4279 - acc: 0.1083Epoch 00011: val_loss improved from 2.42584 to 2.42548, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 105us/step - loss: 2.4280 - acc: 0.1087 - val_loss: 2.4255 - val_acc: 0.1116\n",
      "Epoch 12/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4272 - acc: 0.1070Epoch 00012: val_loss improved from 2.42548 to 2.42532, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 106us/step - loss: 2.4264 - acc: 0.1079 - val_loss: 2.4253 - val_acc: 0.1116\n",
      "Epoch 13/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4254 - acc: 0.1083Epoch 00013: val_loss improved from 2.42532 to 2.42518, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 106us/step - loss: 2.4261 - acc: 0.1082 - val_loss: 2.4252 - val_acc: 0.1116\n",
      "Epoch 14/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4261 - acc: 0.1085Epoch 00014: val_loss improved from 2.42518 to 2.42502, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 2.4265 - acc: 0.1081 - val_loss: 2.4250 - val_acc: 0.1116\n",
      "Epoch 15/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4260 - acc: 0.1060Epoch 00015: val_loss improved from 2.42502 to 2.42480, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 2.4260 - acc: 0.1057 - val_loss: 2.4248 - val_acc: 0.0974\n",
      "Epoch 16/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4254 - acc: 0.1058Epoch 00016: val_loss improved from 2.42480 to 2.42467, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.4257 - acc: 0.1063 - val_loss: 2.4247 - val_acc: 0.0974\n",
      "Epoch 17/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4242 - acc: 0.1095Epoch 00017: val_loss improved from 2.42467 to 2.42448, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 2.4247 - acc: 0.1084 - val_loss: 2.4245 - val_acc: 0.0974\n",
      "Epoch 18/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4239 - acc: 0.1094Epoch 00018: val_loss improved from 2.42448 to 2.42434, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.4244 - acc: 0.1092 - val_loss: 2.4243 - val_acc: 0.0974\n",
      "Epoch 19/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4239 - acc: 0.1091Epoch 00019: val_loss improved from 2.42434 to 2.42433, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.4240 - acc: 0.1092 - val_loss: 2.4243 - val_acc: 0.0976\n",
      "Epoch 20/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4250 - acc: 0.1093Epoch 00020: val_loss improved from 2.42433 to 2.42419, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 2.4250 - acc: 0.1093 - val_loss: 2.4242 - val_acc: 0.0976\n",
      "Epoch 21/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4262 - acc: 0.1091Epoch 00021: val_loss improved from 2.42419 to 2.42415, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.4245 - acc: 0.1090 - val_loss: 2.4241 - val_acc: 0.0974\n",
      "Epoch 22/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4233 - acc: 0.1096Epoch 00022: val_loss improved from 2.42415 to 2.42414, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.4239 - acc: 0.1096 - val_loss: 2.4241 - val_acc: 0.0974\n",
      "Epoch 23/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4247 - acc: 0.1088Epoch 00023: val_loss improved from 2.42414 to 2.42404, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.4239 - acc: 0.1096 - val_loss: 2.4240 - val_acc: 0.0974\n",
      "Epoch 24/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4237 - acc: 0.1099Epoch 00024: val_loss improved from 2.42404 to 2.42395, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.4240 - acc: 0.1092 - val_loss: 2.4240 - val_acc: 0.0976\n",
      "Epoch 25/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4235 - acc: 0.1091Epoch 00025: val_loss improved from 2.42395 to 2.42380, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.4234 - acc: 0.1092 - val_loss: 2.4238 - val_acc: 0.0976\n",
      "Epoch 26/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4219 - acc: 0.1103Epoch 00026: val_loss improved from 2.42380 to 2.42367, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 2.4230 - acc: 0.1093 - val_loss: 2.4237 - val_acc: 0.0976\n",
      "Epoch 27/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4219 - acc: 0.1092Epoch 00027: val_loss improved from 2.42367 to 2.42356, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.4223 - acc: 0.1095 - val_loss: 2.4236 - val_acc: 0.0976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4219 - acc: 0.1101Epoch 00028: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 2.4223 - acc: 0.1093 - val_loss: 2.4236 - val_acc: 0.0976\n",
      "Epoch 29/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4240 - acc: 0.1088Epoch 00029: val_loss improved from 2.42356 to 2.42018, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.4225 - acc: 0.1095 - val_loss: 2.4202 - val_acc: 0.0976\n",
      "Epoch 30/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4155 - acc: 0.1149Epoch 00030: val_loss improved from 2.42018 to 2.40394, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 2.4148 - acc: 0.1152 - val_loss: 2.4039 - val_acc: 0.1111\n",
      "Epoch 31/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4079 - acc: 0.1181Epoch 00031: val_loss improved from 2.40394 to 2.39420, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 2.4074 - acc: 0.1185 - val_loss: 2.3942 - val_acc: 0.1113\n",
      "Epoch 32/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.4009 - acc: 0.1197Epoch 00032: val_loss improved from 2.39420 to 2.38365, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.3996 - acc: 0.1200 - val_loss: 2.3836 - val_acc: 0.1145\n",
      "Epoch 33/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.3937 - acc: 0.1240Epoch 00033: val_loss improved from 2.38365 to 2.37108, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.3921 - acc: 0.1251 - val_loss: 2.3711 - val_acc: 0.1203\n",
      "Epoch 34/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.3828 - acc: 0.1336Epoch 00034: val_loss improved from 2.37108 to 2.35752, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.3824 - acc: 0.1338 - val_loss: 2.3575 - val_acc: 0.1335\n",
      "Epoch 35/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.3661 - acc: 0.1457Epoch 00035: val_loss improved from 2.35752 to 2.33608, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.3664 - acc: 0.1455 - val_loss: 2.3361 - val_acc: 0.1513\n",
      "Epoch 36/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.3428 - acc: 0.1557Epoch 00036: val_loss improved from 2.33608 to 2.30079, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.3436 - acc: 0.1557 - val_loss: 2.3008 - val_acc: 0.1711\n",
      "Epoch 37/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.3234 - acc: 0.1665Epoch 00037: val_loss improved from 2.30079 to 2.26002, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.3236 - acc: 0.1664 - val_loss: 2.2600 - val_acc: 0.1846\n",
      "Epoch 38/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.3054 - acc: 0.1716Epoch 00038: val_loss improved from 2.26002 to 2.20682, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.3026 - acc: 0.1733 - val_loss: 2.2068 - val_acc: 0.2065\n",
      "Epoch 39/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.2810 - acc: 0.1839Epoch 00039: val_loss improved from 2.20682 to 2.20488, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.2792 - acc: 0.1846 - val_loss: 2.2049 - val_acc: 0.2085\n",
      "Epoch 40/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.2549 - acc: 0.1904Epoch 00040: val_loss improved from 2.20488 to 2.15474, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.2550 - acc: 0.1908 - val_loss: 2.1547 - val_acc: 0.2256\n",
      "Epoch 41/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.2255 - acc: 0.2018Epoch 00041: val_loss improved from 2.15474 to 2.13165, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.2270 - acc: 0.2012 - val_loss: 2.1317 - val_acc: 0.2378\n",
      "Epoch 42/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.1645 - acc: 0.2338Epoch 00042: val_loss improved from 2.13165 to 2.02742, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 2.1633 - acc: 0.2340 - val_loss: 2.0274 - val_acc: 0.3008\n",
      "Epoch 43/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.0906 - acc: 0.2614Epoch 00043: val_loss improved from 2.02742 to 1.94640, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 2.0902 - acc: 0.2620 - val_loss: 1.9464 - val_acc: 0.3401\n",
      "Epoch 44/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 2.0015 - acc: 0.2953Epoch 00044: val_loss improved from 1.94640 to 1.80523, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 1.9990 - acc: 0.2959 - val_loss: 1.8052 - val_acc: 0.3870\n",
      "Epoch 45/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 1.9063 - acc: 0.3271Epoch 00045: val_loss improved from 1.80523 to 1.70553, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 1.9047 - acc: 0.3275 - val_loss: 1.7055 - val_acc: 0.4246\n",
      "Epoch 46/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 1.8109 - acc: 0.3591Epoch 00046: val_loss improved from 1.70553 to 1.60249, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 1.8097 - acc: 0.3595 - val_loss: 1.6025 - val_acc: 0.4739\n",
      "Epoch 47/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 1.7276 - acc: 0.3895Epoch 00047: val_loss improved from 1.60249 to 1.50843, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 1.7279 - acc: 0.3903 - val_loss: 1.5084 - val_acc: 0.5129\n",
      "Epoch 48/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 1.6640 - acc: 0.4091Epoch 00048: val_loss improved from 1.50843 to 1.42856, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 1.6627 - acc: 0.4095 - val_loss: 1.4286 - val_acc: 0.5471\n",
      "Epoch 49/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 1.5887 - acc: 0.4386Epoch 00049: val_loss improved from 1.42856 to 1.31593, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 1.5880 - acc: 0.4391 - val_loss: 1.3159 - val_acc: 0.5825\n",
      "Epoch 50/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 1.5095 - acc: 0.4738Epoch 00050: val_loss improved from 1.31593 to 1.20181, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 1.5061 - acc: 0.4755 - val_loss: 1.2018 - val_acc: 0.6142\n",
      "Epoch 51/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 1.4280 - acc: 0.4935Epoch 00051: val_loss improved from 1.20181 to 1.12803, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 1.4269 - acc: 0.4940 - val_loss: 1.1280 - val_acc: 0.6446\n",
      "Epoch 52/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 1.3578 - acc: 0.5238Epoch 00052: val_loss improved from 1.12803 to 1.03894, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 1.3561 - acc: 0.5242 - val_loss: 1.0389 - val_acc: 0.6714\n",
      "Epoch 53/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 1.3019 - acc: 0.5465Epoch 00053: val_loss improved from 1.03894 to 0.97346, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 1.3006 - acc: 0.5467 - val_loss: 0.9735 - val_acc: 0.6909\n",
      "Epoch 54/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 1.2423 - acc: 0.5643Epoch 00054: val_loss improved from 0.97346 to 0.93289, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 1.2417 - acc: 0.5645 - val_loss: 0.9329 - val_acc: 0.7033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 1.1979 - acc: 0.5791Epoch 00055: val_loss improved from 0.93289 to 0.88049, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 1.1963 - acc: 0.5798 - val_loss: 0.8805 - val_acc: 0.7153\n",
      "Epoch 56/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 1.1429 - acc: 0.6021Epoch 00056: val_loss improved from 0.88049 to 0.83971, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 1.1411 - acc: 0.6023 - val_loss: 0.8397 - val_acc: 0.7283\n",
      "Epoch 57/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 1.0973 - acc: 0.6155Epoch 00057: val_loss improved from 0.83971 to 0.80670, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 1.0974 - acc: 0.6157 - val_loss: 0.8067 - val_acc: 0.7336\n",
      "Epoch 58/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 1.0691 - acc: 0.6299Epoch 00058: val_loss improved from 0.80670 to 0.79090, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 1.0673 - acc: 0.6304 - val_loss: 0.7909 - val_acc: 0.7481\n",
      "Epoch 59/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 1.0312 - acc: 0.6405Epoch 00059: val_loss improved from 0.79090 to 0.74904, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 1.0326 - acc: 0.6400 - val_loss: 0.7490 - val_acc: 0.7583\n",
      "Epoch 60/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 1.0080 - acc: 0.6527Epoch 00060: val_loss improved from 0.74904 to 0.71245, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 1.0070 - acc: 0.6524 - val_loss: 0.7125 - val_acc: 0.7690\n",
      "Epoch 61/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.9807 - acc: 0.6563Epoch 00061: val_loss improved from 0.71245 to 0.70578, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.9789 - acc: 0.6568 - val_loss: 0.7058 - val_acc: 0.7810\n",
      "Epoch 62/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.9378 - acc: 0.6783Epoch 00062: val_loss improved from 0.70578 to 0.66914, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.9402 - acc: 0.6770 - val_loss: 0.6691 - val_acc: 0.7917\n",
      "Epoch 63/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.9210 - acc: 0.6831Epoch 00063: val_loss improved from 0.66914 to 0.65362, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 105us/step - loss: 0.9205 - acc: 0.6829 - val_loss: 0.6536 - val_acc: 0.7915\n",
      "Epoch 64/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.9054 - acc: 0.6851Epoch 00064: val_loss improved from 0.65362 to 0.63790, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.9057 - acc: 0.6852 - val_loss: 0.6379 - val_acc: 0.8010\n",
      "Epoch 65/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.8951 - acc: 0.6904Epoch 00065: val_loss improved from 0.63790 to 0.62074, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.8960 - acc: 0.6902 - val_loss: 0.6207 - val_acc: 0.8061\n",
      "Epoch 66/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.8697 - acc: 0.6993Epoch 00066: val_loss improved from 0.62074 to 0.61945, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.8682 - acc: 0.6993 - val_loss: 0.6195 - val_acc: 0.8044\n",
      "Epoch 67/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.8535 - acc: 0.7034Epoch 00067: val_loss improved from 0.61945 to 0.59870, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.8546 - acc: 0.7032 - val_loss: 0.5987 - val_acc: 0.8123\n",
      "Epoch 68/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.8254 - acc: 0.7194Epoch 00068: val_loss improved from 0.59870 to 0.57918, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.8267 - acc: 0.7181 - val_loss: 0.5792 - val_acc: 0.8210\n",
      "Epoch 69/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.8076 - acc: 0.7201Epoch 00069: val_loss improved from 0.57918 to 0.55592, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.8063 - acc: 0.7208 - val_loss: 0.5559 - val_acc: 0.8281\n",
      "Epoch 70/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.7979 - acc: 0.7239Epoch 00070: val_loss improved from 0.55592 to 0.55367, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.7960 - acc: 0.7242 - val_loss: 0.5537 - val_acc: 0.8288\n",
      "Epoch 71/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.7793 - acc: 0.7339Epoch 00071: val_loss improved from 0.55367 to 0.54049, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.7816 - acc: 0.7330 - val_loss: 0.5405 - val_acc: 0.8318\n",
      "Epoch 72/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.7617 - acc: 0.7407Epoch 00072: val_loss improved from 0.54049 to 0.52926, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.7595 - acc: 0.7414 - val_loss: 0.5293 - val_acc: 0.8355\n",
      "Epoch 73/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.7592 - acc: 0.7426Epoch 00073: val_loss improved from 0.52926 to 0.52152, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.7569 - acc: 0.7434 - val_loss: 0.5215 - val_acc: 0.8381\n",
      "Epoch 74/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.7387 - acc: 0.7415Epoch 00074: val_loss improved from 0.52152 to 0.51471, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.7396 - acc: 0.7411 - val_loss: 0.5147 - val_acc: 0.8413\n",
      "Epoch 75/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.7281 - acc: 0.7526Epoch 00075: val_loss improved from 0.51471 to 0.50927, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.7292 - acc: 0.7522 - val_loss: 0.5093 - val_acc: 0.8423\n",
      "Epoch 76/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.7205 - acc: 0.7533Epoch 00076: val_loss improved from 0.50927 to 0.49398, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.7214 - acc: 0.7527 - val_loss: 0.4940 - val_acc: 0.8486\n",
      "Epoch 77/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6965 - acc: 0.7621Epoch 00077: val_loss improved from 0.49398 to 0.48862, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.6987 - acc: 0.7621 - val_loss: 0.4886 - val_acc: 0.8545\n",
      "Epoch 78/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6775 - acc: 0.7656Epoch 00078: val_loss improved from 0.48862 to 0.47693, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.6759 - acc: 0.7662 - val_loss: 0.4769 - val_acc: 0.8525\n",
      "Epoch 79/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6780 - acc: 0.7703Epoch 00079: val_loss improved from 0.47693 to 0.47209, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.6773 - acc: 0.7708 - val_loss: 0.4721 - val_acc: 0.8537\n",
      "Epoch 80/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6614 - acc: 0.7695Epoch 00080: val_loss improved from 0.47209 to 0.45653, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.6598 - acc: 0.7707 - val_loss: 0.4565 - val_acc: 0.8576\n",
      "Epoch 81/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6555 - acc: 0.7778Epoch 00081: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.6547 - acc: 0.7777 - val_loss: 0.4568 - val_acc: 0.8584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6379 - acc: 0.7801Epoch 00082: val_loss improved from 0.45653 to 0.44792, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.6415 - acc: 0.7790 - val_loss: 0.4479 - val_acc: 0.8606\n",
      "Epoch 83/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6370 - acc: 0.7804Epoch 00083: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.6396 - acc: 0.7801 - val_loss: 0.4576 - val_acc: 0.8579\n",
      "Epoch 84/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6318 - acc: 0.7847Epoch 00084: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.6342 - acc: 0.7843 - val_loss: 0.4491 - val_acc: 0.8625\n",
      "Epoch 85/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6156 - acc: 0.7900Epoch 00085: val_loss improved from 0.44792 to 0.42550, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.6144 - acc: 0.7904 - val_loss: 0.4255 - val_acc: 0.8696\n",
      "Epoch 86/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6118 - acc: 0.7911Epoch 00086: val_loss improved from 0.42550 to 0.42067, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.6107 - acc: 0.7918 - val_loss: 0.4207 - val_acc: 0.8667\n",
      "Epoch 87/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5972 - acc: 0.7951Epoch 00087: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.5978 - acc: 0.7954 - val_loss: 0.4208 - val_acc: 0.8674\n",
      "Epoch 88/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5968 - acc: 0.7981Epoch 00088: val_loss improved from 0.42067 to 0.41528, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.5963 - acc: 0.7978 - val_loss: 0.4153 - val_acc: 0.8721\n",
      "Epoch 89/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5918 - acc: 0.7979Epoch 00089: val_loss improved from 0.41528 to 0.41317, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.5928 - acc: 0.7977 - val_loss: 0.4132 - val_acc: 0.8709\n",
      "Epoch 90/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5725 - acc: 0.8050Epoch 00090: val_loss improved from 0.41317 to 0.40358, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.5719 - acc: 0.8050 - val_loss: 0.4036 - val_acc: 0.8748\n",
      "Epoch 91/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5853 - acc: 0.8033Epoch 00091: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.5822 - acc: 0.8042 - val_loss: 0.4071 - val_acc: 0.8714\n",
      "Epoch 92/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5664 - acc: 0.8062Epoch 00092: val_loss improved from 0.40358 to 0.39837, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.5673 - acc: 0.8063 - val_loss: 0.3984 - val_acc: 0.8765\n",
      "Epoch 93/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5487 - acc: 0.8114Epoch 00093: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.5517 - acc: 0.8101 - val_loss: 0.4025 - val_acc: 0.8730\n",
      "Epoch 94/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5660 - acc: 0.8104Epoch 00094: val_loss improved from 0.39837 to 0.39364, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.5620 - acc: 0.8113 - val_loss: 0.3936 - val_acc: 0.8765\n",
      "Epoch 95/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5418 - acc: 0.8151Epoch 00095: val_loss improved from 0.39364 to 0.39199, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.5418 - acc: 0.8150 - val_loss: 0.3920 - val_acc: 0.8782\n",
      "Epoch 96/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5423 - acc: 0.8158Epoch 00096: val_loss improved from 0.39199 to 0.38295, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 105us/step - loss: 0.5423 - acc: 0.8151 - val_loss: 0.3829 - val_acc: 0.8797\n",
      "Epoch 97/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5374 - acc: 0.8189Epoch 00097: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.5364 - acc: 0.8186 - val_loss: 0.3842 - val_acc: 0.8806\n",
      "Epoch 98/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5304 - acc: 0.8185Epoch 00098: val_loss improved from 0.38295 to 0.37955, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.5285 - acc: 0.8191 - val_loss: 0.3795 - val_acc: 0.8794\n",
      "Epoch 99/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5186 - acc: 0.8206Epoch 00099: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.5187 - acc: 0.8208 - val_loss: 0.3815 - val_acc: 0.8796\n",
      "Epoch 100/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5238 - acc: 0.8202Epoch 00100: val_loss improved from 0.37955 to 0.37480, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.5239 - acc: 0.8200 - val_loss: 0.3748 - val_acc: 0.8835\n",
      "Epoch 101/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5191 - acc: 0.8256Epoch 00101: val_loss improved from 0.37480 to 0.36956, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.5160 - acc: 0.8267 - val_loss: 0.3696 - val_acc: 0.8845\n",
      "Epoch 102/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5026 - acc: 0.8316Epoch 00102: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.5005 - acc: 0.8329 - val_loss: 0.3698 - val_acc: 0.8836\n",
      "Epoch 103/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5041 - acc: 0.8291Epoch 00103: val_loss improved from 0.36956 to 0.36009, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.5030 - acc: 0.8293 - val_loss: 0.3601 - val_acc: 0.8872\n",
      "Epoch 104/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4960 - acc: 0.8242Epoch 00104: val_loss improved from 0.36009 to 0.35953, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.4975 - acc: 0.8242 - val_loss: 0.3595 - val_acc: 0.8880\n",
      "Epoch 105/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4861 - acc: 0.8367Epoch 00105: val_loss improved from 0.35953 to 0.35863, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.4842 - acc: 0.8370 - val_loss: 0.3586 - val_acc: 0.8879\n",
      "Epoch 106/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4856 - acc: 0.8376Epoch 00106: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.4851 - acc: 0.8369 - val_loss: 0.3589 - val_acc: 0.8840\n",
      "Epoch 107/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4776 - acc: 0.8397Epoch 00107: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.4765 - acc: 0.8403 - val_loss: 0.3685 - val_acc: 0.8769\n",
      "Epoch 108/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4739 - acc: 0.8381Epoch 00108: val_loss improved from 0.35863 to 0.34962, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 105us/step - loss: 0.4747 - acc: 0.8381 - val_loss: 0.3496 - val_acc: 0.8886\n",
      "Epoch 109/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4720 - acc: 0.8408Epoch 00109: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.4717 - acc: 0.8407 - val_loss: 0.3535 - val_acc: 0.8879\n",
      "Epoch 110/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4794 - acc: 0.8374Epoch 00110: val_loss improved from 0.34962 to 0.34490, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.4786 - acc: 0.8378 - val_loss: 0.3449 - val_acc: 0.8897\n",
      "Epoch 111/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4626 - acc: 0.8413Epoch 00111: val_loss improved from 0.34490 to 0.34162, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.4630 - acc: 0.8413 - val_loss: 0.3416 - val_acc: 0.8936\n",
      "Epoch 112/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4554 - acc: 0.8446Epoch 00112: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.4576 - acc: 0.8445 - val_loss: 0.3466 - val_acc: 0.8863\n",
      "Epoch 113/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4493 - acc: 0.8476Epoch 00113: val_loss improved from 0.34162 to 0.34015, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.4484 - acc: 0.8481 - val_loss: 0.3401 - val_acc: 0.8892\n",
      "Epoch 114/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4547 - acc: 0.8467Epoch 00114: val_loss improved from 0.34015 to 0.33946, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.4565 - acc: 0.8461 - val_loss: 0.3395 - val_acc: 0.8909\n",
      "Epoch 115/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4380 - acc: 0.8499Epoch 00115: val_loss improved from 0.33946 to 0.33784, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.4384 - acc: 0.8494 - val_loss: 0.3378 - val_acc: 0.8918\n",
      "Epoch 116/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4528 - acc: 0.8446Epoch 00116: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.4506 - acc: 0.8454 - val_loss: 0.3381 - val_acc: 0.8962\n",
      "Epoch 117/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4402 - acc: 0.8528Epoch 00117: val_loss improved from 0.33784 to 0.33096, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.4388 - acc: 0.8528 - val_loss: 0.3310 - val_acc: 0.8962\n",
      "Epoch 118/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4426 - acc: 0.8504Epoch 00118: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.4442 - acc: 0.8500 - val_loss: 0.3324 - val_acc: 0.8957\n",
      "Epoch 119/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4358 - acc: 0.8555Epoch 00119: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.4383 - acc: 0.8542 - val_loss: 0.3334 - val_acc: 0.8935\n",
      "Epoch 120/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4332 - acc: 0.8537Epoch 00120: val_loss improved from 0.33096 to 0.32373, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.4321 - acc: 0.8539 - val_loss: 0.3237 - val_acc: 0.8985\n",
      "Epoch 121/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4261 - acc: 0.8543Epoch 00121: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.4253 - acc: 0.8545 - val_loss: 0.3239 - val_acc: 0.8958\n",
      "Epoch 122/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4231 - acc: 0.8538Epoch 00122: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.4250 - acc: 0.8534 - val_loss: 0.3298 - val_acc: 0.8972\n",
      "Epoch 123/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4201 - acc: 0.8573Epoch 00123: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.4218 - acc: 0.8578 - val_loss: 0.3270 - val_acc: 0.8940\n",
      "Epoch 124/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4129 - acc: 0.8620Epoch 00124: val_loss improved from 0.32373 to 0.32119, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.4136 - acc: 0.8619 - val_loss: 0.3212 - val_acc: 0.8980\n",
      "Epoch 125/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4117 - acc: 0.8573Epoch 00125: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.4104 - acc: 0.8579 - val_loss: 0.3338 - val_acc: 0.8945\n",
      "Epoch 126/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4067 - acc: 0.8571Epoch 00126: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.4077 - acc: 0.8566 - val_loss: 0.3220 - val_acc: 0.8970\n",
      "Epoch 127/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4013 - acc: 0.8606Epoch 00127: val_loss improved from 0.32119 to 0.31720, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.4027 - acc: 0.8603 - val_loss: 0.3172 - val_acc: 0.8997\n",
      "Epoch 128/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4101 - acc: 0.8604Epoch 00128: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.4095 - acc: 0.8602 - val_loss: 0.3247 - val_acc: 0.8987\n",
      "Epoch 129/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4002 - acc: 0.8607Epoch 00129: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.4015 - acc: 0.8611 - val_loss: 0.3208 - val_acc: 0.8965\n",
      "Epoch 130/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3999 - acc: 0.8656Epoch 00130: val_loss improved from 0.31720 to 0.31041, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.3984 - acc: 0.8660 - val_loss: 0.3104 - val_acc: 0.9002\n",
      "Epoch 131/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3873 - acc: 0.8667Epoch 00131: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.3863 - acc: 0.8672 - val_loss: 0.3126 - val_acc: 0.9036\n",
      "Epoch 132/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3914 - acc: 0.8662Epoch 00132: val_loss improved from 0.31041 to 0.30844, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.3901 - acc: 0.8668 - val_loss: 0.3084 - val_acc: 0.9048\n",
      "Epoch 133/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3903 - acc: 0.8667Epoch 00133: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.3900 - acc: 0.8674 - val_loss: 0.3112 - val_acc: 0.9026\n",
      "Epoch 134/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3949 - acc: 0.8635Epoch 00134: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3950 - acc: 0.8627 - val_loss: 0.3131 - val_acc: 0.9013\n",
      "Epoch 135/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3933 - acc: 0.8649Epoch 00135: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3920 - acc: 0.8653 - val_loss: 0.3111 - val_acc: 0.9041\n",
      "Epoch 136/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3874 - acc: 0.8662Epoch 00136: val_loss improved from 0.30844 to 0.30558, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.3865 - acc: 0.8671 - val_loss: 0.3056 - val_acc: 0.9063\n",
      "Epoch 137/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3795 - acc: 0.8706Epoch 00137: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3800 - acc: 0.8707 - val_loss: 0.3063 - val_acc: 0.9033\n",
      "Epoch 138/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3677 - acc: 0.8756Epoch 00138: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.3683 - acc: 0.8753 - val_loss: 0.3096 - val_acc: 0.9035\n",
      "Epoch 139/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3697 - acc: 0.8699Epoch 00139: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3719 - acc: 0.8691 - val_loss: 0.3074 - val_acc: 0.9028\n",
      "Epoch 140/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3801 - acc: 0.8733Epoch 00140: val_loss improved from 0.30558 to 0.30418, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.3790 - acc: 0.8736 - val_loss: 0.3042 - val_acc: 0.9060\n",
      "Epoch 141/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3654 - acc: 0.8742Epoch 00141: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.3646 - acc: 0.8743 - val_loss: 0.3075 - val_acc: 0.9013\n",
      "Epoch 142/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3715 - acc: 0.8748Epoch 00142: val_loss improved from 0.30418 to 0.30044, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.3706 - acc: 0.8748 - val_loss: 0.3004 - val_acc: 0.9065\n",
      "Epoch 143/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3673 - acc: 0.8735Epoch 00143: val_loss improved from 0.30044 to 0.29574, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.3685 - acc: 0.8730 - val_loss: 0.2957 - val_acc: 0.9075\n",
      "Epoch 144/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3576 - acc: 0.8774Epoch 00144: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3582 - acc: 0.8767 - val_loss: 0.3049 - val_acc: 0.9080\n",
      "Epoch 145/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3651 - acc: 0.8765Epoch 00145: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.3650 - acc: 0.8765 - val_loss: 0.2997 - val_acc: 0.9057\n",
      "Epoch 146/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3621 - acc: 0.8787Epoch 00146: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3635 - acc: 0.8777 - val_loss: 0.3006 - val_acc: 0.9065\n",
      "Epoch 147/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3590 - acc: 0.8798Epoch 00147: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3590 - acc: 0.8796 - val_loss: 0.3018 - val_acc: 0.9072\n",
      "Epoch 148/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3574 - acc: 0.8748Epoch 00148: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3585 - acc: 0.8746 - val_loss: 0.3030 - val_acc: 0.9072\n",
      "Epoch 149/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3566 - acc: 0.8800Epoch 00149: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3560 - acc: 0.8800 - val_loss: 0.3008 - val_acc: 0.9074\n",
      "Epoch 150/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3593 - acc: 0.8776Epoch 00150: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.3592 - acc: 0.8773 - val_loss: 0.3000 - val_acc: 0.9046\n",
      "Epoch 151/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3485 - acc: 0.8805Epoch 00151: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3487 - acc: 0.8806 - val_loss: 0.3059 - val_acc: 0.9038\n",
      "Epoch 152/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3411 - acc: 0.8848Epoch 00152: val_loss improved from 0.29574 to 0.29281, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.3406 - acc: 0.8849 - val_loss: 0.2928 - val_acc: 0.9082\n",
      "Epoch 153/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3432 - acc: 0.8820Epoch 00153: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3427 - acc: 0.8820 - val_loss: 0.2933 - val_acc: 0.9102\n",
      "Epoch 154/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3356 - acc: 0.8803Epoch 00154: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3370 - acc: 0.8801 - val_loss: 0.3004 - val_acc: 0.9060\n",
      "Epoch 155/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3507 - acc: 0.8795Epoch 00155: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.3519 - acc: 0.8783 - val_loss: 0.2975 - val_acc: 0.9089\n",
      "Epoch 156/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3469 - acc: 0.8818Epoch 00156: val_loss improved from 0.29281 to 0.28879, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.3452 - acc: 0.8824 - val_loss: 0.2888 - val_acc: 0.9112\n",
      "Epoch 157/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3416 - acc: 0.8821Epoch 00157: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3431 - acc: 0.8816 - val_loss: 0.2926 - val_acc: 0.9099\n",
      "Epoch 158/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3395 - acc: 0.8834Epoch 00158: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.3390 - acc: 0.8840 - val_loss: 0.2914 - val_acc: 0.9102\n",
      "Epoch 159/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3420 - acc: 0.8821Epoch 00159: val_loss improved from 0.28879 to 0.28633, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.3405 - acc: 0.8829 - val_loss: 0.2863 - val_acc: 0.9124\n",
      "Epoch 160/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3331 - acc: 0.8836Epoch 00160: val_loss improved from 0.28633 to 0.28522, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.3324 - acc: 0.8838 - val_loss: 0.2852 - val_acc: 0.9136\n",
      "Epoch 161/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3368 - acc: 0.8856Epoch 00161: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.3364 - acc: 0.8859 - val_loss: 0.2917 - val_acc: 0.9121\n",
      "Epoch 162/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3364 - acc: 0.8887Epoch 00162: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.3349 - acc: 0.8889 - val_loss: 0.2915 - val_acc: 0.9111\n",
      "Epoch 163/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3325 - acc: 0.8866Epoch 00163: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.3318 - acc: 0.8865 - val_loss: 0.2884 - val_acc: 0.9109\n",
      "Epoch 164/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3273 - acc: 0.8873Epoch 00164: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.3284 - acc: 0.8873 - val_loss: 0.2883 - val_acc: 0.9124\n",
      "Epoch 165/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3158 - acc: 0.8926Epoch 00165: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3147 - acc: 0.8932 - val_loss: 0.2932 - val_acc: 0.9082\n",
      "Epoch 166/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3300 - acc: 0.8845Epoch 00166: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.3282 - acc: 0.8854 - val_loss: 0.2891 - val_acc: 0.9112\n",
      "Epoch 167/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3244 - acc: 0.8866Epoch 00167: val_loss improved from 0.28522 to 0.28278, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.3238 - acc: 0.8870 - val_loss: 0.2828 - val_acc: 0.9134\n",
      "Epoch 168/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3241 - acc: 0.8875Epoch 00168: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3240 - acc: 0.8873 - val_loss: 0.2864 - val_acc: 0.9107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3198 - acc: 0.8915Epoch 00169: val_loss improved from 0.28278 to 0.28076, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.3217 - acc: 0.8905 - val_loss: 0.2808 - val_acc: 0.9157\n",
      "Epoch 170/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3088 - acc: 0.8949Epoch 00170: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3083 - acc: 0.8950 - val_loss: 0.2846 - val_acc: 0.9109\n",
      "Epoch 171/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3176 - acc: 0.8918Epoch 00171: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3177 - acc: 0.8916 - val_loss: 0.2859 - val_acc: 0.9124\n",
      "Epoch 172/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3174 - acc: 0.8920Epoch 00172: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3149 - acc: 0.8929 - val_loss: 0.2846 - val_acc: 0.9143\n",
      "Epoch 173/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3165 - acc: 0.8887Epoch 00173: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3172 - acc: 0.8881 - val_loss: 0.2858 - val_acc: 0.9136\n",
      "Epoch 174/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3006 - acc: 0.8981Epoch 00174: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.3014 - acc: 0.8984 - val_loss: 0.2882 - val_acc: 0.9085\n",
      "Epoch 175/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3073 - acc: 0.8942Epoch 00175: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3083 - acc: 0.8940 - val_loss: 0.2828 - val_acc: 0.9128\n",
      "Epoch 176/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2999 - acc: 0.8962Epoch 00176: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2999 - acc: 0.8968 - val_loss: 0.2858 - val_acc: 0.9131\n",
      "Epoch 177/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3050 - acc: 0.8960Epoch 00177: val_loss improved from 0.28076 to 0.27937, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.3062 - acc: 0.8958 - val_loss: 0.2794 - val_acc: 0.9138\n",
      "Epoch 178/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3077 - acc: 0.8933Epoch 00178: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.3082 - acc: 0.8932 - val_loss: 0.2798 - val_acc: 0.9109\n",
      "Epoch 179/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3072 - acc: 0.8984Epoch 00179: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3058 - acc: 0.8987 - val_loss: 0.2840 - val_acc: 0.9124\n",
      "Epoch 180/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2977 - acc: 0.8989Epoch 00180: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.2969 - acc: 0.8994 - val_loss: 0.2843 - val_acc: 0.9128\n",
      "Epoch 181/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3087 - acc: 0.8950Epoch 00181: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3060 - acc: 0.8964 - val_loss: 0.2826 - val_acc: 0.9140\n",
      "Epoch 182/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2969 - acc: 0.9004Epoch 00182: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2965 - acc: 0.9005 - val_loss: 0.2833 - val_acc: 0.9112\n",
      "Epoch 183/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2921 - acc: 0.8986Epoch 00183: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2936 - acc: 0.8987 - val_loss: 0.2841 - val_acc: 0.9116\n",
      "Epoch 184/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2956 - acc: 0.8984Epoch 00184: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2928 - acc: 0.8995 - val_loss: 0.2811 - val_acc: 0.9163\n",
      "Epoch 185/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2958 - acc: 0.9011Epoch 00185: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2956 - acc: 0.9010 - val_loss: 0.2809 - val_acc: 0.9124\n",
      "Epoch 186/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3037 - acc: 0.8988Epoch 00186: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3020 - acc: 0.8993 - val_loss: 0.2807 - val_acc: 0.9138\n",
      "Epoch 187/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2994 - acc: 0.8972Epoch 00187: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2999 - acc: 0.8970 - val_loss: 0.2814 - val_acc: 0.9118\n",
      "Epoch 188/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2922 - acc: 0.9010Epoch 00188: val_loss improved from 0.27937 to 0.27870, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.2914 - acc: 0.9007 - val_loss: 0.2787 - val_acc: 0.9146\n",
      "Epoch 189/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2900 - acc: 0.9034Epoch 00189: val_loss improved from 0.27870 to 0.27567, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.2903 - acc: 0.9029 - val_loss: 0.2757 - val_acc: 0.9155\n",
      "Epoch 190/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2888 - acc: 0.9030Epoch 00190: val_loss improved from 0.27567 to 0.27406, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.2878 - acc: 0.9029 - val_loss: 0.2741 - val_acc: 0.9172\n",
      "Epoch 191/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2896 - acc: 0.9017Epoch 00191: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.2889 - acc: 0.9023 - val_loss: 0.2817 - val_acc: 0.9136\n",
      "Epoch 192/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2878 - acc: 0.9023Epoch 00192: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.2873 - acc: 0.9027 - val_loss: 0.2775 - val_acc: 0.9165\n",
      "Epoch 193/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2807 - acc: 0.9027Epoch 00193: val_loss improved from 0.27406 to 0.27126, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.2819 - acc: 0.9022 - val_loss: 0.2713 - val_acc: 0.9162\n",
      "Epoch 194/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2865 - acc: 0.9046Epoch 00194: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2863 - acc: 0.9047 - val_loss: 0.2824 - val_acc: 0.9126\n",
      "Epoch 195/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2765 - acc: 0.9050Epoch 00195: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.2766 - acc: 0.9053 - val_loss: 0.2727 - val_acc: 0.9153\n",
      "Epoch 196/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2823 - acc: 0.9048Epoch 00196: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.2817 - acc: 0.9050 - val_loss: 0.2741 - val_acc: 0.9167\n",
      "Epoch 197/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2846 - acc: 0.9020Epoch 00197: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2839 - acc: 0.9024 - val_loss: 0.2782 - val_acc: 0.9162\n",
      "Epoch 198/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2867 - acc: 0.9026Epoch 00198: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.2840 - acc: 0.9036 - val_loss: 0.2783 - val_acc: 0.9157\n",
      "Epoch 199/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2826 - acc: 0.9048Epoch 00199: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.2847 - acc: 0.9035 - val_loss: 0.2723 - val_acc: 0.9165\n",
      "Epoch 200/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2740 - acc: 0.9086Epoch 00200: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.2746 - acc: 0.9083 - val_loss: 0.2733 - val_acc: 0.9158\n",
      "Epoch 201/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2742 - acc: 0.9072Epoch 00201: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2753 - acc: 0.9070 - val_loss: 0.2739 - val_acc: 0.9158\n",
      "Epoch 202/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2770 - acc: 0.9090Epoch 00202: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2754 - acc: 0.9095 - val_loss: 0.2760 - val_acc: 0.9138\n",
      "Epoch 203/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2836 - acc: 0.9023Epoch 00203: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2814 - acc: 0.9033 - val_loss: 0.2725 - val_acc: 0.9155\n",
      "Epoch 204/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2732 - acc: 0.9053Epoch 00204: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2730 - acc: 0.9060 - val_loss: 0.2780 - val_acc: 0.9129\n",
      "Epoch 205/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2747 - acc: 0.9052Epoch 00205: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2753 - acc: 0.9050 - val_loss: 0.2773 - val_acc: 0.9173\n",
      "Epoch 206/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2796 - acc: 0.9042Epoch 00206: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2795 - acc: 0.9041 - val_loss: 0.2751 - val_acc: 0.9160\n",
      "Epoch 207/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2624 - acc: 0.9088Epoch 00207: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.2633 - acc: 0.9088 - val_loss: 0.2757 - val_acc: 0.9134\n",
      "Epoch 208/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2694 - acc: 0.9080Epoch 00208: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.2708 - acc: 0.9077 - val_loss: 0.2768 - val_acc: 0.9163\n",
      "Epoch 209/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2659 - acc: 0.9116Epoch 00209: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.2666 - acc: 0.9117 - val_loss: 0.2777 - val_acc: 0.9163\n",
      "Epoch 210/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2702 - acc: 0.9074Epoch 00210: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.2711 - acc: 0.9074 - val_loss: 0.2792 - val_acc: 0.9143\n",
      "Epoch 211/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2710 - acc: 0.9087Epoch 00211: val_loss improved from 0.27126 to 0.26822, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.2698 - acc: 0.9089 - val_loss: 0.2682 - val_acc: 0.9194\n",
      "Epoch 212/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2650 - acc: 0.9094Epoch 00212: val_loss improved from 0.26822 to 0.26762, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.2669 - acc: 0.9090 - val_loss: 0.2676 - val_acc: 0.9192\n",
      "Epoch 213/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2628 - acc: 0.9086Epoch 00213: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.2641 - acc: 0.9082 - val_loss: 0.2709 - val_acc: 0.9180\n",
      "Epoch 214/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2679 - acc: 0.9081Epoch 00214: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.2695 - acc: 0.9075 - val_loss: 0.2703 - val_acc: 0.9184\n",
      "Epoch 215/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2476 - acc: 0.9120Epoch 00215: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.2482 - acc: 0.9117 - val_loss: 0.2798 - val_acc: 0.9157\n",
      "Epoch 216/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2633 - acc: 0.9083Epoch 00216: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.2638 - acc: 0.9079 - val_loss: 0.2703 - val_acc: 0.9197\n",
      "Epoch 217/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2647 - acc: 0.9081Epoch 00217: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2643 - acc: 0.9080 - val_loss: 0.2742 - val_acc: 0.9165\n",
      "Epoch 218/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2556 - acc: 0.9127Epoch 00218: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 102us/step - loss: 0.2566 - acc: 0.9127 - val_loss: 0.2736 - val_acc: 0.9180\n",
      "Epoch 219/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2618 - acc: 0.9093Epoch 00219: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2624 - acc: 0.9100 - val_loss: 0.2689 - val_acc: 0.9201\n",
      "Epoch 220/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2679 - acc: 0.9068Epoch 00220: val_loss improved from 0.26762 to 0.26525, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 104us/step - loss: 0.2689 - acc: 0.9066 - val_loss: 0.2653 - val_acc: 0.9192\n",
      "Epoch 221/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2603 - acc: 0.9113Epoch 00221: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2602 - acc: 0.9109 - val_loss: 0.2673 - val_acc: 0.9211\n",
      "Epoch 222/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2598 - acc: 0.9120Epoch 00222: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2607 - acc: 0.9117 - val_loss: 0.2698 - val_acc: 0.9195\n",
      "Epoch 223/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2631 - acc: 0.9117Epoch 00223: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2615 - acc: 0.9122 - val_loss: 0.2704 - val_acc: 0.9180\n",
      "Epoch 224/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2493 - acc: 0.9159Epoch 00224: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.2474 - acc: 0.9168 - val_loss: 0.2688 - val_acc: 0.9184\n",
      "Epoch 225/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2558 - acc: 0.9135Epoch 00225: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2571 - acc: 0.9132 - val_loss: 0.2693 - val_acc: 0.9185\n",
      "Epoch 226/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2526 - acc: 0.9155Epoch 00226: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2531 - acc: 0.9156 - val_loss: 0.2693 - val_acc: 0.9177\n",
      "Epoch 227/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2458 - acc: 0.9185Epoch 00227: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2477 - acc: 0.9181 - val_loss: 0.2680 - val_acc: 0.9201\n",
      "Epoch 228/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2545 - acc: 0.9111Epoch 00228: val_loss improved from 0.26525 to 0.26036, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.2551 - acc: 0.9110 - val_loss: 0.2604 - val_acc: 0.9214\n",
      "Epoch 229/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2590 - acc: 0.9120Epoch 00229: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2589 - acc: 0.9122 - val_loss: 0.2632 - val_acc: 0.9201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2539 - acc: 0.9153Epoch 00230: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2540 - acc: 0.9155 - val_loss: 0.2649 - val_acc: 0.9199\n",
      "Epoch 231/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2463 - acc: 0.9128Epoch 00231: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2452 - acc: 0.9134 - val_loss: 0.2712 - val_acc: 0.9187\n",
      "Epoch 232/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2442 - acc: 0.9168Epoch 00232: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2446 - acc: 0.9165 - val_loss: 0.2683 - val_acc: 0.9182\n",
      "Epoch 233/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2523 - acc: 0.9162Epoch 00233: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2494 - acc: 0.9169 - val_loss: 0.2616 - val_acc: 0.9197\n",
      "Epoch 234/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2472 - acc: 0.9147Epoch 00234: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2483 - acc: 0.9145 - val_loss: 0.2626 - val_acc: 0.9207\n",
      "Epoch 235/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2471 - acc: 0.9123Epoch 00235: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2482 - acc: 0.9117 - val_loss: 0.2609 - val_acc: 0.9209\n",
      "Epoch 236/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2394 - acc: 0.9162Epoch 00236: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2401 - acc: 0.9158 - val_loss: 0.2695 - val_acc: 0.9190\n",
      "Epoch 237/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2372 - acc: 0.9196Epoch 00237: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2408 - acc: 0.9184 - val_loss: 0.2644 - val_acc: 0.9201\n",
      "Epoch 238/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2373 - acc: 0.9168Epoch 00238: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2370 - acc: 0.9173 - val_loss: 0.2635 - val_acc: 0.9190\n",
      "Epoch 239/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2411 - acc: 0.9181Epoch 00239: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2413 - acc: 0.9179 - val_loss: 0.2625 - val_acc: 0.9223\n",
      "Epoch 240/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2449 - acc: 0.9175Epoch 00240: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2460 - acc: 0.9175 - val_loss: 0.2664 - val_acc: 0.9179\n",
      "Epoch 241/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2423 - acc: 0.9160Epoch 00241: val_loss improved from 0.26036 to 0.26012, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 103us/step - loss: 0.2416 - acc: 0.9162 - val_loss: 0.2601 - val_acc: 0.9207\n",
      "Epoch 242/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2538 - acc: 0.9122Epoch 00242: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.2522 - acc: 0.9127 - val_loss: 0.2628 - val_acc: 0.9197\n",
      "Epoch 243/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2421 - acc: 0.9194Epoch 00243: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2418 - acc: 0.9194 - val_loss: 0.2657 - val_acc: 0.9184\n",
      "Epoch 244/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2386 - acc: 0.9176Epoch 00244: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.2380 - acc: 0.9181 - val_loss: 0.2666 - val_acc: 0.9223\n",
      "Epoch 245/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2407 - acc: 0.9173Epoch 00245: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2398 - acc: 0.9177 - val_loss: 0.2648 - val_acc: 0.9202\n",
      "Epoch 246/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2457 - acc: 0.9177Epoch 00246: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2463 - acc: 0.9172 - val_loss: 0.2626 - val_acc: 0.9204\n",
      "Epoch 247/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2477 - acc: 0.9174Epoch 00247: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2470 - acc: 0.9175 - val_loss: 0.2681 - val_acc: 0.9217\n",
      "Epoch 248/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2350 - acc: 0.9185Epoch 00248: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2348 - acc: 0.9188 - val_loss: 0.2683 - val_acc: 0.9209\n",
      "Epoch 249/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2401 - acc: 0.9189Epoch 00249: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2400 - acc: 0.9193 - val_loss: 0.2622 - val_acc: 0.9202\n",
      "Epoch 250/250\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2377 - acc: 0.9180Epoch 00250: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.2379 - acc: 0.9178 - val_loss: 0.2634 - val_acc: 0.9195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd2e15f8b00>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5',verbose=1,save_best_only=True)\n",
    "model.fit(X_train.reshape(X_train.shape[0],20,40,1), to_categorical(y_train),\n",
    "          batch_size=256, epochs=250, verbose=1,validation_split=0.3,callbacks=[checkpointer] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = [np.argmax(model.predict(np.expand_dims(example,axis=0))) for example in X_test.reshape(X_test.shape[0],20,40,1)]\n",
    "\n",
    "accuracy =100*np.sum(np.array(y_predictions)==np.argmax( to_categorical(y_test),axis=1))/len(y_predictions)\n",
    "\n",
    "print ('test accurac : %.4f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test (path):\n",
    "    #path=r\"C:\\Users\\abdal_000\\Downloads\\data-set\\test\\audio\\clip_0a77ea19a.wav\"\n",
    "    mfcc_feature=convert_wave_to_mfcc(path)\n",
    "    #print(mfcc_feature.shape)\n",
    "    \n",
    "    pre=model.predict(mfcc_feature.reshape(1,20,40,1))\n",
    "    return np.argmax(pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "audfiles=os.listdir(r\"C:\\Users\\abdal_000\\Downloads\\data-set\\test\\audio\")\n",
    "print (audfiles[0])\n",
    "with open('sample_submission.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames=['fname','label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    for audio in audfiles:\n",
    "        pred = test(r\"C:\\Users\\abdal_000\\Downloads\\data-set\\test\\audio\\\\\"+audio)\n",
    "        writer.writerow({'fname':audio,'label':labels[pred]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
