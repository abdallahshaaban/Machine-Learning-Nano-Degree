{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import  Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#l=['silence','unknown']\n",
    "file_path=\"audio\"\n",
    "max_pad_length=40\n",
    "random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_wavfile(path=\"audio/go/0d53e045_nohash_1.wav\"):\n",
    "    \n",
    "    sample_rate, samples = wavfile.read(path)\n",
    "    frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
    "    plt.pcolormesh(times, frequencies, spectrogram)\n",
    "    plt.imshow(spectrogram)\n",
    "    plt.ylabel('Frequecy [HZ]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_wave_to_mfcc(filePath):\n",
    "    wav, sr =librosa.load(filePath, mono = True, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(wav, sr=sr)\n",
    "    ##padding\n",
    "    mfcc_feature = np.pad(mfcc, pad_width=((0, 0), (0, max_pad_length - mfcc.shape[1])), mode='constant')\n",
    "    return mfcc_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(path=file_path):\n",
    "    return os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mfcc_files(path=file_path):\n",
    "    labels = get_labels(path)\n",
    "    #print(labels)\n",
    "    for label in labels:\n",
    "        mfcc_features=[]\n",
    "        audfiles=[path+'/'+label+'/'+aud for aud in os.listdir(path+'/'+label)]\n",
    "        for audio in audfiles:\n",
    "            mfcc_features.append(convert_wave_to_mfcc(audio))\n",
    "        #print(len(mfcc_vectors))\n",
    "        np.save(label+'.npy', mfcc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stop',\n",
       " 'left',\n",
       " 'unknown',\n",
       " 'off',\n",
       " 'yes',\n",
       " 'up',\n",
       " 'on',\n",
       " 'down',\n",
       " 'no',\n",
       " 'go',\n",
       " 'right',\n",
       " 'silence']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_labels()\n",
    "#print(len(get_labels()))\n",
    "#save_mfcc_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_testing_set(path=file_path,test_size=.3):\n",
    "    labels = get_labels(path)\n",
    "    #features=[]\n",
    "    features=np.load(labels[0]+'.npy')\n",
    "    classes=np.zeros(features.shape[0])\n",
    "    #for i,label in zip(range(0,len(labels)-1),labels):\n",
    "    for i in range(1,len(labels)):\n",
    "        x=np.load(labels[i]+'.npy')\n",
    "        features=np.vstack((features,x))\n",
    "        classes=np.append(classes, np.full(x.shape[0], fill_value= (i)))\n",
    "    return train_test_split(features, classes, test_size= test_size, random_state=random_state, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_testing_set1(path=file_path,test_size=.3):\n",
    "    labels=get_labels(path)\n",
    "    features=np.load(labels[0]+'.npy')\n",
    "    classes=np.zeros(features.shape[0])\n",
    "    print(classes[1])\n",
    "    feature_train, feature_test, labels_train, labels_test = train_test_split(features, classes,\n",
    "                                        test_size= test_size, random_state=random_state, shuffle=True)\n",
    "    for i in range(1,len(labels)):\n",
    "        x=np.load(labels[i]+'.npy')\n",
    "        #features=np.vstack((features,x))\n",
    "        y= np.full(x.shape[0], fill_value= (i))\n",
    "        #print(labels[i])\n",
    "        X_train, X_test, y_train, y_test=train_test_split(x, y,\n",
    "                                    test_size= test_size, random_state=random_state, shuffle=True)\n",
    "     \n",
    "        \n",
    "        feature_train=np.vstack((feature_train,X_train))\n",
    "        feature_test=np.vstack((feature_test,X_test))\n",
    "       \n",
    "        labels_train=np.append(labels_train,y_train)\n",
    "        labels_test=np.append(labels_test,y_test)\n",
    "        \n",
    "    return feature_train, feature_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = generate_training_testing_set()\n",
    "\n",
    "model =Sequential()\n",
    "model.add(Conv2D(32,kernel_size=(2,2),activation='relu',input_shape=(20,40,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Conv2D(64,kernel_size=(2,2),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Conv2D(128,kernel_size=(2,2),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(12,activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.adadelta(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ac4889b07f0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "print ( len(X_train), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13776 samples, validate on 5904 samples\n",
      "Epoch 1/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.9063 - acc: 0.6892Epoch 00001: val_loss improved from inf to 0.62718, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 108us/step - loss: 0.9061 - acc: 0.6900 - val_loss: 0.6272 - val_acc: 0.7988\n",
      "Epoch 2/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.8776 - acc: 0.6987Epoch 00002: val_loss improved from 0.62718 to 0.60877, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.8784 - acc: 0.6983 - val_loss: 0.6088 - val_acc: 0.8045\n",
      "Epoch 3/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.8505 - acc: 0.7091Epoch 00003: val_loss improved from 0.60877 to 0.58757, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.8501 - acc: 0.7093 - val_loss: 0.5876 - val_acc: 0.8091\n",
      "Epoch 4/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.8199 - acc: 0.7223Epoch 00004: val_loss improved from 0.58757 to 0.57421, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.8208 - acc: 0.7224 - val_loss: 0.5742 - val_acc: 0.8093\n",
      "Epoch 5/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.8266 - acc: 0.7171Epoch 00005: val_loss improved from 0.57421 to 0.57047, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.8245 - acc: 0.7178 - val_loss: 0.5705 - val_acc: 0.8161\n",
      "Epoch 6/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.7923 - acc: 0.7254Epoch 00006: val_loss improved from 0.57047 to 0.54213, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.7920 - acc: 0.7258 - val_loss: 0.5421 - val_acc: 0.8293\n",
      "Epoch 7/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.7809 - acc: 0.7312Epoch 00007: val_loss improved from 0.54213 to 0.53947, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.7811 - acc: 0.7306 - val_loss: 0.5395 - val_acc: 0.8281\n",
      "Epoch 8/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.7627 - acc: 0.7390Epoch 00008: val_loss improved from 0.53947 to 0.51777, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.7648 - acc: 0.7384 - val_loss: 0.5178 - val_acc: 0.8367\n",
      "Epoch 9/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.7510 - acc: 0.7434Epoch 00009: val_loss improved from 0.51777 to 0.51613, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.7523 - acc: 0.7430 - val_loss: 0.5161 - val_acc: 0.8325\n",
      "Epoch 10/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.7199 - acc: 0.7574Epoch 00010: val_loss improved from 0.51613 to 0.51094, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.7210 - acc: 0.7578 - val_loss: 0.5109 - val_acc: 0.8333\n",
      "Epoch 11/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.7160 - acc: 0.7562Epoch 00011: val_loss improved from 0.51094 to 0.49372, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.7180 - acc: 0.7560 - val_loss: 0.4937 - val_acc: 0.8408\n",
      "Epoch 12/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.7034 - acc: 0.7622Epoch 00012: val_loss improved from 0.49372 to 0.47976, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.7044 - acc: 0.7616 - val_loss: 0.4798 - val_acc: 0.8479\n",
      "Epoch 13/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6856 - acc: 0.7652Epoch 00013: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.6854 - acc: 0.7654 - val_loss: 0.4823 - val_acc: 0.8403\n",
      "Epoch 14/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6783 - acc: 0.7668Epoch 00014: val_loss improved from 0.47976 to 0.47388, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.6787 - acc: 0.7668 - val_loss: 0.4739 - val_acc: 0.8459\n",
      "Epoch 15/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6699 - acc: 0.7735Epoch 00015: val_loss improved from 0.47388 to 0.45217, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.6711 - acc: 0.7732 - val_loss: 0.4522 - val_acc: 0.8570\n",
      "Epoch 16/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6516 - acc: 0.7788Epoch 00016: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.6542 - acc: 0.7785 - val_loss: 0.4549 - val_acc: 0.8508\n",
      "Epoch 17/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6482 - acc: 0.7827Epoch 00017: val_loss improved from 0.45217 to 0.43405, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.6458 - acc: 0.7835 - val_loss: 0.4341 - val_acc: 0.8603\n",
      "Epoch 18/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6453 - acc: 0.7797Epoch 00018: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 96us/step - loss: 0.6445 - acc: 0.7791 - val_loss: 0.4370 - val_acc: 0.8582\n",
      "Epoch 19/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6194 - acc: 0.7873Epoch 00019: val_loss improved from 0.43405 to 0.43030, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.6207 - acc: 0.7869 - val_loss: 0.4303 - val_acc: 0.8613\n",
      "Epoch 20/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6234 - acc: 0.7851Epoch 00020: val_loss improved from 0.43030 to 0.42645, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.6228 - acc: 0.7855 - val_loss: 0.4265 - val_acc: 0.8623\n",
      "Epoch 21/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6068 - acc: 0.7969Epoch 00021: val_loss improved from 0.42645 to 0.41932, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.6060 - acc: 0.7968 - val_loss: 0.4193 - val_acc: 0.8637\n",
      "Epoch 22/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.6099 - acc: 0.7944Epoch 00022: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 96us/step - loss: 0.6103 - acc: 0.7942 - val_loss: 0.4293 - val_acc: 0.8592\n",
      "Epoch 23/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5969 - acc: 0.7983Epoch 00023: val_loss improved from 0.41932 to 0.41169, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.5950 - acc: 0.7991 - val_loss: 0.4117 - val_acc: 0.8679\n",
      "Epoch 24/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5938 - acc: 0.8000Epoch 00024: val_loss improved from 0.41169 to 0.40476, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.5938 - acc: 0.7996 - val_loss: 0.4048 - val_acc: 0.8720\n",
      "Epoch 25/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5715 - acc: 0.8058Epoch 00025: val_loss improved from 0.40476 to 0.39943, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.5721 - acc: 0.8060 - val_loss: 0.3994 - val_acc: 0.8689\n",
      "Epoch 26/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5800 - acc: 0.8045Epoch 00026: val_loss improved from 0.39943 to 0.39769, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.5815 - acc: 0.8044 - val_loss: 0.3977 - val_acc: 0.8750\n",
      "Epoch 27/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5654 - acc: 0.8090Epoch 00027: val_loss improved from 0.39769 to 0.39583, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.5652 - acc: 0.8089 - val_loss: 0.3958 - val_acc: 0.8706\n",
      "Epoch 28/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5743 - acc: 0.8045Epoch 00028: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.5746 - acc: 0.8048 - val_loss: 0.4000 - val_acc: 0.8711\n",
      "Epoch 29/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5621 - acc: 0.8086Epoch 00029: val_loss improved from 0.39583 to 0.38573, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.5631 - acc: 0.8082 - val_loss: 0.3857 - val_acc: 0.8789\n",
      "Epoch 30/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5419 - acc: 0.8142Epoch 00030: val_loss improved from 0.38573 to 0.37463, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.5417 - acc: 0.8137 - val_loss: 0.3746 - val_acc: 0.8784\n",
      "Epoch 31/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5325 - acc: 0.8184Epoch 00031: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 96us/step - loss: 0.5340 - acc: 0.8174 - val_loss: 0.3754 - val_acc: 0.8791\n",
      "Epoch 32/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5392 - acc: 0.8202Epoch 00032: val_loss improved from 0.37463 to 0.37250, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.5374 - acc: 0.8209 - val_loss: 0.3725 - val_acc: 0.8833\n",
      "Epoch 33/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5258 - acc: 0.8211Epoch 00033: val_loss improved from 0.37250 to 0.36946, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.5283 - acc: 0.8206 - val_loss: 0.3695 - val_acc: 0.8833\n",
      "Epoch 34/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5231 - acc: 0.8242Epoch 00034: val_loss improved from 0.36946 to 0.36575, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.5250 - acc: 0.8230 - val_loss: 0.3658 - val_acc: 0.8838\n",
      "Epoch 35/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5136 - acc: 0.8277Epoch 00035: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.5160 - acc: 0.8269 - val_loss: 0.3690 - val_acc: 0.8830\n",
      "Epoch 36/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4996 - acc: 0.8316Epoch 00036: val_loss improved from 0.36575 to 0.36239, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.4990 - acc: 0.8311 - val_loss: 0.3624 - val_acc: 0.8792\n",
      "Epoch 37/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.5101 - acc: 0.8287Epoch 00037: val_loss improved from 0.36239 to 0.36197, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.5109 - acc: 0.8281 - val_loss: 0.3620 - val_acc: 0.8845\n",
      "Epoch 38/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4991 - acc: 0.8320Epoch 00038: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.5005 - acc: 0.8306 - val_loss: 0.3626 - val_acc: 0.8828\n",
      "Epoch 39/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4990 - acc: 0.8318Epoch 00039: val_loss improved from 0.36197 to 0.35232, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.4989 - acc: 0.8319 - val_loss: 0.3523 - val_acc: 0.8886\n",
      "Epoch 40/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4930 - acc: 0.8352Epoch 00040: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.4945 - acc: 0.8345 - val_loss: 0.3547 - val_acc: 0.8891\n",
      "Epoch 41/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4957 - acc: 0.8319Epoch 00041: val_loss improved from 0.35232 to 0.34509, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.4933 - acc: 0.8323 - val_loss: 0.3451 - val_acc: 0.8891\n",
      "Epoch 42/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4813 - acc: 0.8384- ETA: 0s - loss: 0.4665 - accEpoch 00042: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.4781 - acc: 0.8393 - val_loss: 0.3464 - val_acc: 0.8875\n",
      "Epoch 43/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4811 - acc: 0.8371Epoch 00043: val_loss improved from 0.34509 to 0.34274, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.4829 - acc: 0.8365 - val_loss: 0.3427 - val_acc: 0.8891\n",
      "Epoch 44/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4696 - acc: 0.8396Epoch 00044: val_loss improved from 0.34274 to 0.33922, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.4709 - acc: 0.8390 - val_loss: 0.3392 - val_acc: 0.8911\n",
      "Epoch 45/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4712 - acc: 0.8397Epoch 00045: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.4723 - acc: 0.8392 - val_loss: 0.3460 - val_acc: 0.8896\n",
      "Epoch 46/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4601 - acc: 0.8451Epoch 00046: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.4605 - acc: 0.8449 - val_loss: 0.3421 - val_acc: 0.8863\n",
      "Epoch 47/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4642 - acc: 0.8426Epoch 00047: val_loss improved from 0.33922 to 0.33600, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.4653 - acc: 0.8425 - val_loss: 0.3360 - val_acc: 0.8908\n",
      "Epoch 48/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4533 - acc: 0.8468Epoch 00048: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.4520 - acc: 0.8473 - val_loss: 0.3373 - val_acc: 0.8909\n",
      "Epoch 49/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4541 - acc: 0.8470Epoch 00049: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.4551 - acc: 0.8466 - val_loss: 0.3417 - val_acc: 0.8896\n",
      "Epoch 50/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4478 - acc: 0.8449Epoch 00050: val_loss improved from 0.33600 to 0.33388, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.4510 - acc: 0.8440 - val_loss: 0.3339 - val_acc: 0.8908\n",
      "Epoch 51/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4565 - acc: 0.8459Epoch 00051: val_loss improved from 0.33388 to 0.33173, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.4595 - acc: 0.8451 - val_loss: 0.3317 - val_acc: 0.8904\n",
      "Epoch 52/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4433 - acc: 0.8528Epoch 00052: val_loss improved from 0.33173 to 0.33112, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.4451 - acc: 0.8529 - val_loss: 0.3311 - val_acc: 0.8930\n",
      "Epoch 53/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4391 - acc: 0.8538Epoch 00053: val_loss improved from 0.33112 to 0.32954, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.4396 - acc: 0.8539 - val_loss: 0.3295 - val_acc: 0.8928\n",
      "Epoch 54/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4411 - acc: 0.8498Epoch 00054: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.4397 - acc: 0.8501 - val_loss: 0.3317 - val_acc: 0.8926\n",
      "Epoch 55/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4323 - acc: 0.8547Epoch 00055: val_loss improved from 0.32954 to 0.32405, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.4318 - acc: 0.8550 - val_loss: 0.3240 - val_acc: 0.8940\n",
      "Epoch 56/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4220 - acc: 0.8581Epoch 00056: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.4230 - acc: 0.8577 - val_loss: 0.3283 - val_acc: 0.8948\n",
      "Epoch 57/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4240 - acc: 0.8544Epoch 00057: val_loss improved from 0.32405 to 0.32005, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.4279 - acc: 0.8534 - val_loss: 0.3200 - val_acc: 0.8987\n",
      "Epoch 58/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4293 - acc: 0.8540Epoch 00058: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.4288 - acc: 0.8542 - val_loss: 0.3289 - val_acc: 0.8985\n",
      "Epoch 59/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4211 - acc: 0.8582Epoch 00059: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.4204 - acc: 0.8583 - val_loss: 0.3208 - val_acc: 0.8994\n",
      "Epoch 60/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4131 - acc: 0.8592Epoch 00060: val_loss improved from 0.32005 to 0.31991, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.4110 - acc: 0.8595 - val_loss: 0.3199 - val_acc: 0.8963\n",
      "Epoch 61/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4162 - acc: 0.8580Epoch 00061: val_loss improved from 0.31991 to 0.31736, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.4156 - acc: 0.8577 - val_loss: 0.3174 - val_acc: 0.9006\n",
      "Epoch 62/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4050 - acc: 0.8634Epoch 00062: val_loss improved from 0.31736 to 0.31411, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.4047 - acc: 0.8640 - val_loss: 0.3141 - val_acc: 0.8982\n",
      "Epoch 63/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4097 - acc: 0.8598Epoch 00063: val_loss improved from 0.31411 to 0.30999, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.4095 - acc: 0.8600 - val_loss: 0.3100 - val_acc: 0.9029\n",
      "Epoch 64/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4013 - acc: 0.8612Epoch 00064: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.4013 - acc: 0.8614 - val_loss: 0.3260 - val_acc: 0.8975\n",
      "Epoch 65/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3961 - acc: 0.8667Epoch 00065: val_loss improved from 0.30999 to 0.30602, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.3969 - acc: 0.8668 - val_loss: 0.3060 - val_acc: 0.9023\n",
      "Epoch 66/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.4021 - acc: 0.8646Epoch 00066: val_loss improved from 0.30602 to 0.30326, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.4071 - acc: 0.8635 - val_loss: 0.3033 - val_acc: 0.9024\n",
      "Epoch 67/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3890 - acc: 0.8687Epoch 00067: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3919 - acc: 0.8682 - val_loss: 0.3049 - val_acc: 0.9024\n",
      "Epoch 68/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3984 - acc: 0.8642Epoch 00068: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.3977 - acc: 0.8643 - val_loss: 0.3132 - val_acc: 0.8992\n",
      "Epoch 69/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3839 - acc: 0.8690Epoch 00069: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3834 - acc: 0.8693 - val_loss: 0.3066 - val_acc: 0.9024\n",
      "Epoch 70/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3863 - acc: 0.8679Epoch 00070: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3867 - acc: 0.8681 - val_loss: 0.3071 - val_acc: 0.9024\n",
      "Epoch 71/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3948 - acc: 0.8648Epoch 00071: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3930 - acc: 0.8653 - val_loss: 0.3103 - val_acc: 0.9038\n",
      "Epoch 72/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3857 - acc: 0.8713Epoch 00072: val_loss improved from 0.30326 to 0.30112, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.3840 - acc: 0.8720 - val_loss: 0.3011 - val_acc: 0.9048\n",
      "Epoch 73/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3643 - acc: 0.8789Epoch 00073: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3663 - acc: 0.8788 - val_loss: 0.3036 - val_acc: 0.9035\n",
      "Epoch 74/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3759 - acc: 0.8720Epoch 00074: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3768 - acc: 0.8715 - val_loss: 0.3027 - val_acc: 0.9062\n",
      "Epoch 75/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3819 - acc: 0.8686Epoch 00075: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3808 - acc: 0.8692 - val_loss: 0.3055 - val_acc: 0.9051\n",
      "Epoch 76/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3755 - acc: 0.8719Epoch 00076: val_loss improved from 0.30112 to 0.29919, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.3776 - acc: 0.8709 - val_loss: 0.2992 - val_acc: 0.9080\n",
      "Epoch 77/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3726 - acc: 0.8738Epoch 00077: val_loss improved from 0.29919 to 0.29219, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.3726 - acc: 0.8745 - val_loss: 0.2922 - val_acc: 0.9090\n",
      "Epoch 78/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3601 - acc: 0.8761Epoch 00078: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 96us/step - loss: 0.3587 - acc: 0.8773 - val_loss: 0.2949 - val_acc: 0.9072\n",
      "Epoch 79/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3653 - acc: 0.8750Epoch 00079: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3642 - acc: 0.8758 - val_loss: 0.2939 - val_acc: 0.9070\n",
      "Epoch 80/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3653 - acc: 0.8738Epoch 00080: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.3663 - acc: 0.8736 - val_loss: 0.2966 - val_acc: 0.9068\n",
      "Epoch 81/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3613 - acc: 0.8803Epoch 00081: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.3633 - acc: 0.8794 - val_loss: 0.2993 - val_acc: 0.9055\n",
      "Epoch 82/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3592 - acc: 0.8788Epoch 00082: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.3587 - acc: 0.8793 - val_loss: 0.2926 - val_acc: 0.9055\n",
      "Epoch 83/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3535 - acc: 0.8824Epoch 00083: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.3527 - acc: 0.8829 - val_loss: 0.3054 - val_acc: 0.9063\n",
      "Epoch 84/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3592 - acc: 0.8789Epoch 00084: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.3571 - acc: 0.8796 - val_loss: 0.3027 - val_acc: 0.9051\n",
      "Epoch 85/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3534 - acc: 0.8787Epoch 00085: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3534 - acc: 0.8781 - val_loss: 0.2970 - val_acc: 0.9070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3609 - acc: 0.8779Epoch 00086: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.3614 - acc: 0.8778 - val_loss: 0.2952 - val_acc: 0.9065\n",
      "Epoch 87/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3467 - acc: 0.8845Epoch 00087: val_loss improved from 0.29219 to 0.28730, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.3459 - acc: 0.8846 - val_loss: 0.2873 - val_acc: 0.9101\n",
      "Epoch 88/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3527 - acc: 0.8819Epoch 00088: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.3516 - acc: 0.8825 - val_loss: 0.2887 - val_acc: 0.9084\n",
      "Epoch 89/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3475 - acc: 0.8835Epoch 00089: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3454 - acc: 0.8838 - val_loss: 0.2911 - val_acc: 0.9084\n",
      "Epoch 90/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3493 - acc: 0.8807Epoch 00090: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3485 - acc: 0.8809 - val_loss: 0.2909 - val_acc: 0.9099\n",
      "Epoch 91/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3418 - acc: 0.8797Epoch 00091: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3417 - acc: 0.8800 - val_loss: 0.2931 - val_acc: 0.9074\n",
      "Epoch 92/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3435 - acc: 0.8838Epoch 00092: val_loss improved from 0.28730 to 0.28677, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.3421 - acc: 0.8842 - val_loss: 0.2868 - val_acc: 0.9092\n",
      "Epoch 93/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3369 - acc: 0.8829Epoch 00093: val_loss improved from 0.28677 to 0.28367, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.3369 - acc: 0.8833 - val_loss: 0.2837 - val_acc: 0.9116\n",
      "Epoch 94/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3455 - acc: 0.8845Epoch 00094: val_loss improved from 0.28367 to 0.28301, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.3445 - acc: 0.8845 - val_loss: 0.2830 - val_acc: 0.9109\n",
      "Epoch 95/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3331 - acc: 0.8861Epoch 00095: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.3343 - acc: 0.8857 - val_loss: 0.2911 - val_acc: 0.9057\n",
      "Epoch 96/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3293 - acc: 0.8870Epoch 00096: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.3333 - acc: 0.8857 - val_loss: 0.2868 - val_acc: 0.9055\n",
      "Epoch 97/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3274 - acc: 0.8907Epoch 00097: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.3266 - acc: 0.8908 - val_loss: 0.2916 - val_acc: 0.9072\n",
      "Epoch 98/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3344 - acc: 0.8884Epoch 00098: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.3353 - acc: 0.8880 - val_loss: 0.2835 - val_acc: 0.9094\n",
      "Epoch 99/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3295 - acc: 0.8874Epoch 00099: val_loss improved from 0.28301 to 0.28131, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.3298 - acc: 0.8869 - val_loss: 0.2813 - val_acc: 0.9128\n",
      "Epoch 100/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3252 - acc: 0.8917Epoch 00100: val_loss improved from 0.28131 to 0.27834, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.3248 - acc: 0.8918 - val_loss: 0.2783 - val_acc: 0.9129\n",
      "Epoch 101/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3281 - acc: 0.8867Epoch 00101: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3278 - acc: 0.8873 - val_loss: 0.2811 - val_acc: 0.9109\n",
      "Epoch 102/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3244 - acc: 0.8882Epoch 00102: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.3237 - acc: 0.8891 - val_loss: 0.2862 - val_acc: 0.9079\n",
      "Epoch 103/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3301 - acc: 0.8878Epoch 00103: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.3296 - acc: 0.8878 - val_loss: 0.2916 - val_acc: 0.9077\n",
      "Epoch 104/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3207 - acc: 0.8925Epoch 00104: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3210 - acc: 0.8916 - val_loss: 0.2803 - val_acc: 0.9114\n",
      "Epoch 105/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3162 - acc: 0.8909Epoch 00105: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.3172 - acc: 0.8908 - val_loss: 0.2796 - val_acc: 0.9131\n",
      "Epoch 106/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3090 - acc: 0.8947Epoch 00106: val_loss improved from 0.27834 to 0.27515, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.3095 - acc: 0.8944 - val_loss: 0.2751 - val_acc: 0.9141\n",
      "Epoch 107/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3113 - acc: 0.8915Epoch 00107: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.3119 - acc: 0.8914 - val_loss: 0.2754 - val_acc: 0.9121\n",
      "Epoch 108/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3050 - acc: 0.8957Epoch 00108: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.3064 - acc: 0.8953 - val_loss: 0.2792 - val_acc: 0.9114\n",
      "Epoch 109/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3073 - acc: 0.8967Epoch 00109: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3064 - acc: 0.8964 - val_loss: 0.2801 - val_acc: 0.9128\n",
      "Epoch 110/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3103 - acc: 0.8933Epoch 00110: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3104 - acc: 0.8937 - val_loss: 0.2844 - val_acc: 0.9116\n",
      "Epoch 111/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3127 - acc: 0.8910Epoch 00111: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3110 - acc: 0.8912 - val_loss: 0.2776 - val_acc: 0.9121\n",
      "Epoch 112/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2957 - acc: 0.8974Epoch 00112: val_loss improved from 0.27515 to 0.27305, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2966 - acc: 0.8968 - val_loss: 0.2730 - val_acc: 0.9145\n",
      "Epoch 113/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3126 - acc: 0.8950Epoch 00113: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3127 - acc: 0.8944 - val_loss: 0.2848 - val_acc: 0.9075\n",
      "Epoch 114/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3030 - acc: 0.8981Epoch 00114: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3041 - acc: 0.8978 - val_loss: 0.2805 - val_acc: 0.9101\n",
      "Epoch 115/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3112 - acc: 0.8930Epoch 00115: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3115 - acc: 0.8930 - val_loss: 0.2775 - val_acc: 0.9109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3030 - acc: 0.8945Epoch 00116: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.3061 - acc: 0.8943 - val_loss: 0.2866 - val_acc: 0.9124\n",
      "Epoch 117/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2967 - acc: 0.8971Epoch 00117: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2983 - acc: 0.8966 - val_loss: 0.2822 - val_acc: 0.9138\n",
      "Epoch 118/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3050 - acc: 0.8968Epoch 00118: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.3044 - acc: 0.8970 - val_loss: 0.2750 - val_acc: 0.9138\n",
      "Epoch 119/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.3030 - acc: 0.8981Epoch 00119: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.3009 - acc: 0.8985 - val_loss: 0.2749 - val_acc: 0.9140\n",
      "Epoch 120/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2897 - acc: 0.9013Epoch 00120: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2911 - acc: 0.9008 - val_loss: 0.2813 - val_acc: 0.9157\n",
      "Epoch 121/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2941 - acc: 0.8998Epoch 00121: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2936 - acc: 0.8997 - val_loss: 0.2770 - val_acc: 0.9148\n",
      "Epoch 122/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2936 - acc: 0.8997Epoch 00122: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2948 - acc: 0.8992 - val_loss: 0.2786 - val_acc: 0.9131\n",
      "Epoch 123/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2894 - acc: 0.9008Epoch 00123: val_loss improved from 0.27305 to 0.27274, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.2895 - acc: 0.9008 - val_loss: 0.2727 - val_acc: 0.9168\n",
      "Epoch 124/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2911 - acc: 0.9008Epoch 00124: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2916 - acc: 0.9011 - val_loss: 0.2780 - val_acc: 0.9136\n",
      "Epoch 125/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2863 - acc: 0.8999Epoch 00125: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2854 - acc: 0.9003 - val_loss: 0.2756 - val_acc: 0.9151\n",
      "Epoch 126/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2891 - acc: 0.8999Epoch 00126: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2903 - acc: 0.8995 - val_loss: 0.2730 - val_acc: 0.9140\n",
      "Epoch 127/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2934 - acc: 0.9011Epoch 00127: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2918 - acc: 0.9012 - val_loss: 0.2759 - val_acc: 0.9150\n",
      "Epoch 128/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2881 - acc: 0.9033Epoch 00128: val_loss improved from 0.27274 to 0.27206, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.2894 - acc: 0.9027 - val_loss: 0.2721 - val_acc: 0.9179\n",
      "Epoch 129/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2826 - acc: 0.9050- ETA: 0s - loss: 0.2829 - acEpoch 00129: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2810 - acc: 0.9051 - val_loss: 0.2722 - val_acc: 0.9168\n",
      "Epoch 130/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2850 - acc: 0.9019Epoch 00130: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2820 - acc: 0.9032 - val_loss: 0.2734 - val_acc: 0.9160\n",
      "Epoch 131/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2829 - acc: 0.9010Epoch 00131: val_loss improved from 0.27206 to 0.27067, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.2828 - acc: 0.9010 - val_loss: 0.2707 - val_acc: 0.9160\n",
      "Epoch 132/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2838 - acc: 0.9025Epoch 00132: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2851 - acc: 0.9025 - val_loss: 0.2736 - val_acc: 0.9157\n",
      "Epoch 133/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2821 - acc: 0.9052Epoch 00133: val_loss improved from 0.27067 to 0.26361, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.2802 - acc: 0.9057 - val_loss: 0.2636 - val_acc: 0.9185\n",
      "Epoch 134/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2759 - acc: 0.9076Epoch 00134: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2758 - acc: 0.9072 - val_loss: 0.2707 - val_acc: 0.9165\n",
      "Epoch 135/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2753 - acc: 0.9038Epoch 00135: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2744 - acc: 0.9040 - val_loss: 0.2697 - val_acc: 0.9131\n",
      "Epoch 136/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2740 - acc: 0.9056Epoch 00136: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2746 - acc: 0.9052 - val_loss: 0.2710 - val_acc: 0.9151\n",
      "Epoch 137/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2832 - acc: 0.9047Epoch 00137: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2835 - acc: 0.9045 - val_loss: 0.2792 - val_acc: 0.9124\n",
      "Epoch 138/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2744 - acc: 0.9044Epoch 00138: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2743 - acc: 0.9046 - val_loss: 0.2677 - val_acc: 0.9153\n",
      "Epoch 139/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2718 - acc: 0.9087Epoch 00139: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2739 - acc: 0.9078 - val_loss: 0.2791 - val_acc: 0.9134\n",
      "Epoch 140/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2692 - acc: 0.9069Epoch 00140: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2693 - acc: 0.9069 - val_loss: 0.2719 - val_acc: 0.9165\n",
      "Epoch 141/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2784 - acc: 0.9060Epoch 00141: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2774 - acc: 0.9063 - val_loss: 0.2720 - val_acc: 0.9148\n",
      "Epoch 142/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2774 - acc: 0.9065Epoch 00142: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2753 - acc: 0.9071 - val_loss: 0.2672 - val_acc: 0.9162\n",
      "Epoch 143/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2715 - acc: 0.9079Epoch 00143: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2735 - acc: 0.9070 - val_loss: 0.2692 - val_acc: 0.9151\n",
      "Epoch 144/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2723 - acc: 0.9048Epoch 00144: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2735 - acc: 0.9046 - val_loss: 0.2678 - val_acc: 0.9158\n",
      "Epoch 145/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2644 - acc: 0.9071Epoch 00145: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2656 - acc: 0.9070 - val_loss: 0.2658 - val_acc: 0.9172\n",
      "Epoch 146/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2664 - acc: 0.9090Epoch 00146: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2656 - acc: 0.9096 - val_loss: 0.2726 - val_acc: 0.9175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2707 - acc: 0.9055Epoch 00147: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2701 - acc: 0.9058 - val_loss: 0.2708 - val_acc: 0.9175\n",
      "Epoch 148/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2610 - acc: 0.9115Epoch 00148: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2615 - acc: 0.9113 - val_loss: 0.2740 - val_acc: 0.9153\n",
      "Epoch 149/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2595 - acc: 0.9125Epoch 00149: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2592 - acc: 0.9120 - val_loss: 0.2747 - val_acc: 0.9158\n",
      "Epoch 150/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2530 - acc: 0.9139Epoch 00150: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2547 - acc: 0.9130 - val_loss: 0.2708 - val_acc: 0.9131\n",
      "Epoch 151/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2573 - acc: 0.9135Epoch 00151: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2583 - acc: 0.9127 - val_loss: 0.2709 - val_acc: 0.9148\n",
      "Epoch 152/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2682 - acc: 0.9090Epoch 00152: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2682 - acc: 0.9090 - val_loss: 0.2703 - val_acc: 0.9175\n",
      "Epoch 153/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2645 - acc: 0.9099Epoch 00153: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2637 - acc: 0.9104 - val_loss: 0.2705 - val_acc: 0.9173\n",
      "Epoch 154/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2667 - acc: 0.9081Epoch 00154: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2670 - acc: 0.9080 - val_loss: 0.2639 - val_acc: 0.9167\n",
      "Epoch 155/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2486 - acc: 0.9145Epoch 00155: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2509 - acc: 0.9138 - val_loss: 0.2676 - val_acc: 0.9172\n",
      "Epoch 156/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2573 - acc: 0.9134Epoch 00156: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2579 - acc: 0.9134 - val_loss: 0.2664 - val_acc: 0.9162\n",
      "Epoch 157/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2522 - acc: 0.9138Epoch 00157: val_loss improved from 0.26361 to 0.26345, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.2540 - acc: 0.9129 - val_loss: 0.2635 - val_acc: 0.9168\n",
      "Epoch 158/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2561 - acc: 0.9122Epoch 00158: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2572 - acc: 0.9119 - val_loss: 0.2749 - val_acc: 0.9163\n",
      "Epoch 159/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2625 - acc: 0.9123Epoch 00159: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2647 - acc: 0.9113 - val_loss: 0.2679 - val_acc: 0.9182\n",
      "Epoch 160/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2580 - acc: 0.9136Epoch 00160: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2580 - acc: 0.9133 - val_loss: 0.2682 - val_acc: 0.9163\n",
      "Epoch 161/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2585 - acc: 0.9135Epoch 00161: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2575 - acc: 0.9137 - val_loss: 0.2709 - val_acc: 0.9158\n",
      "Epoch 162/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2437 - acc: 0.9138Epoch 00162: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2444 - acc: 0.9138 - val_loss: 0.2667 - val_acc: 0.9157\n",
      "Epoch 163/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2577 - acc: 0.9141Epoch 00163: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2584 - acc: 0.9138 - val_loss: 0.2662 - val_acc: 0.9158\n",
      "Epoch 164/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2559 - acc: 0.9114Epoch 00164: val_loss improved from 0.26345 to 0.26100, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2553 - acc: 0.9114 - val_loss: 0.2610 - val_acc: 0.9190\n",
      "Epoch 165/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2438 - acc: 0.9183Epoch 00165: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2444 - acc: 0.9180 - val_loss: 0.2669 - val_acc: 0.9182\n",
      "Epoch 166/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2575 - acc: 0.9116Epoch 00166: val_loss improved from 0.26100 to 0.26074, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.2583 - acc: 0.9116 - val_loss: 0.2607 - val_acc: 0.9184\n",
      "Epoch 167/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2463 - acc: 0.9177Epoch 00167: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2467 - acc: 0.9176 - val_loss: 0.2626 - val_acc: 0.9185\n",
      "Epoch 168/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2354 - acc: 0.9206Epoch 00168: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2378 - acc: 0.9194 - val_loss: 0.2629 - val_acc: 0.9190\n",
      "Epoch 169/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2416 - acc: 0.9161Epoch 00169: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2427 - acc: 0.9154 - val_loss: 0.2616 - val_acc: 0.9184\n",
      "Epoch 170/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2340 - acc: 0.9223Epoch 00170: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2330 - acc: 0.9223 - val_loss: 0.2609 - val_acc: 0.9184\n",
      "Epoch 171/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2511 - acc: 0.9152Epoch 00171: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2506 - acc: 0.9157 - val_loss: 0.2621 - val_acc: 0.9182\n",
      "Epoch 172/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2468 - acc: 0.9148Epoch 00172: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2475 - acc: 0.9145 - val_loss: 0.2615 - val_acc: 0.9182\n",
      "Epoch 173/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2437 - acc: 0.9161Epoch 00173: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2426 - acc: 0.9162 - val_loss: 0.2632 - val_acc: 0.9195\n",
      "Epoch 174/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2476 - acc: 0.9166Epoch 00174: val_loss improved from 0.26074 to 0.25655, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2470 - acc: 0.9164 - val_loss: 0.2565 - val_acc: 0.9201\n",
      "Epoch 175/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2459 - acc: 0.9155Epoch 00175: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2467 - acc: 0.9151 - val_loss: 0.2670 - val_acc: 0.9189\n",
      "Epoch 176/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2393 - acc: 0.9159Epoch 00176: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2394 - acc: 0.9164 - val_loss: 0.2609 - val_acc: 0.9185\n",
      "Epoch 177/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2414 - acc: 0.9174Epoch 00177: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2406 - acc: 0.9174 - val_loss: 0.2584 - val_acc: 0.9182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2352 - acc: 0.9221- ETA: 0s - loss: 0.2288 - acc: Epoch 00178: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2331 - acc: 0.9227 - val_loss: 0.2665 - val_acc: 0.9197\n",
      "Epoch 179/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2323 - acc: 0.9223- ETA: 0s - loss: 0.2254 - acc: Epoch 00179: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2336 - acc: 0.9219 - val_loss: 0.2679 - val_acc: 0.9197\n",
      "Epoch 180/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2477 - acc: 0.9165Epoch 00180: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2480 - acc: 0.9162 - val_loss: 0.2638 - val_acc: 0.9175\n",
      "Epoch 181/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2429 - acc: 0.9171Epoch 00181: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2432 - acc: 0.9171 - val_loss: 0.2616 - val_acc: 0.9173\n",
      "Epoch 182/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2368 - acc: 0.9177Epoch 00182: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2385 - acc: 0.9173 - val_loss: 0.2644 - val_acc: 0.9184\n",
      "Epoch 183/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2335 - acc: 0.9195Epoch 00183: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2337 - acc: 0.9194 - val_loss: 0.2692 - val_acc: 0.9182\n",
      "Epoch 184/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2385 - acc: 0.9155Epoch 00184: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2389 - acc: 0.9157 - val_loss: 0.2652 - val_acc: 0.9197\n",
      "Epoch 185/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2386 - acc: 0.9196Epoch 00185: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2379 - acc: 0.9196 - val_loss: 0.2646 - val_acc: 0.9189\n",
      "Epoch 186/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2344 - acc: 0.9224Epoch 00186: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2348 - acc: 0.9219 - val_loss: 0.2681 - val_acc: 0.9177\n",
      "Epoch 187/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2370 - acc: 0.9184Epoch 00187: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2352 - acc: 0.9190 - val_loss: 0.2594 - val_acc: 0.9194\n",
      "Epoch 188/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2316 - acc: 0.9210Epoch 00188: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2317 - acc: 0.9209 - val_loss: 0.2608 - val_acc: 0.9189\n",
      "Epoch 189/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2297 - acc: 0.9220Epoch 00189: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2293 - acc: 0.9223 - val_loss: 0.2627 - val_acc: 0.9189\n",
      "Epoch 190/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2264 - acc: 0.9247Epoch 00190: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2261 - acc: 0.9249 - val_loss: 0.2648 - val_acc: 0.9194\n",
      "Epoch 191/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2398 - acc: 0.9190Epoch 00191: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2383 - acc: 0.9191 - val_loss: 0.2593 - val_acc: 0.9194\n",
      "Epoch 192/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2270 - acc: 0.9244Epoch 00192: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2258 - acc: 0.9246 - val_loss: 0.2631 - val_acc: 0.9165\n",
      "Epoch 193/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2299 - acc: 0.9204Epoch 00193: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2289 - acc: 0.9209 - val_loss: 0.2620 - val_acc: 0.9190\n",
      "Epoch 194/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2259 - acc: 0.9238Epoch 00194: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2250 - acc: 0.9240 - val_loss: 0.2632 - val_acc: 0.9173\n",
      "Epoch 195/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2322 - acc: 0.9213Epoch 00195: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2310 - acc: 0.9218 - val_loss: 0.2593 - val_acc: 0.9207\n",
      "Epoch 196/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2287 - acc: 0.9216Epoch 00196: val_loss improved from 0.25655 to 0.25567, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.2282 - acc: 0.9218 - val_loss: 0.2557 - val_acc: 0.9223\n",
      "Epoch 197/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2292 - acc: 0.9209Epoch 00197: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2288 - acc: 0.9212 - val_loss: 0.2638 - val_acc: 0.9209\n",
      "Epoch 198/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2258 - acc: 0.9216Epoch 00198: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2254 - acc: 0.9217 - val_loss: 0.2620 - val_acc: 0.9216\n",
      "Epoch 199/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2168 - acc: 0.9268Epoch 00199: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2148 - acc: 0.9276 - val_loss: 0.2585 - val_acc: 0.9231\n",
      "Epoch 200/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2206 - acc: 0.9251Epoch 00200: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2220 - acc: 0.9249 - val_loss: 0.2650 - val_acc: 0.9189\n",
      "Epoch 201/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2210 - acc: 0.9262Epoch 00201: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2231 - acc: 0.9255 - val_loss: 0.2665 - val_acc: 0.9197\n",
      "Epoch 202/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2339 - acc: 0.9201Epoch 00202: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2330 - acc: 0.9205 - val_loss: 0.2670 - val_acc: 0.9179\n",
      "Epoch 203/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2302 - acc: 0.9180Epoch 00203: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2309 - acc: 0.9177 - val_loss: 0.2617 - val_acc: 0.9224\n",
      "Epoch 204/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2175 - acc: 0.9264Epoch 00204: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2164 - acc: 0.9265 - val_loss: 0.2643 - val_acc: 0.9197\n",
      "Epoch 205/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2294 - acc: 0.9229Epoch 00205: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2290 - acc: 0.9236 - val_loss: 0.2628 - val_acc: 0.9185\n",
      "Epoch 206/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2220 - acc: 0.9246Epoch 00206: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2214 - acc: 0.9252 - val_loss: 0.2649 - val_acc: 0.9187\n",
      "Epoch 207/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2186 - acc: 0.9253Epoch 00207: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2204 - acc: 0.9243 - val_loss: 0.2606 - val_acc: 0.9231\n",
      "Epoch 208/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2277 - acc: 0.9225Epoch 00208: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2276 - acc: 0.9223 - val_loss: 0.2640 - val_acc: 0.9195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2284 - acc: 0.9207Epoch 00209: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2293 - acc: 0.9206 - val_loss: 0.2638 - val_acc: 0.9207\n",
      "Epoch 210/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2191 - acc: 0.9261Epoch 00210: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2188 - acc: 0.9260 - val_loss: 0.2637 - val_acc: 0.9202\n",
      "Epoch 211/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2096 - acc: 0.9274Epoch 00211: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2102 - acc: 0.9276 - val_loss: 0.2628 - val_acc: 0.9179\n",
      "Epoch 212/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2184 - acc: 0.9273Epoch 00212: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2201 - acc: 0.9267 - val_loss: 0.2633 - val_acc: 0.9202\n",
      "Epoch 213/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2312 - acc: 0.9226Epoch 00213: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2310 - acc: 0.9225 - val_loss: 0.2657 - val_acc: 0.9179\n",
      "Epoch 214/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2237 - acc: 0.9260Epoch 00214: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2230 - acc: 0.9261 - val_loss: 0.2680 - val_acc: 0.9182\n",
      "Epoch 215/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2247 - acc: 0.9235Epoch 00215: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2248 - acc: 0.9236 - val_loss: 0.2574 - val_acc: 0.9228\n",
      "Epoch 216/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2119 - acc: 0.9291Epoch 00216: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2136 - acc: 0.9287 - val_loss: 0.2635 - val_acc: 0.9199\n",
      "Epoch 217/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2242 - acc: 0.9225Epoch 00217: val_loss improved from 0.25567 to 0.25543, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2246 - acc: 0.9223 - val_loss: 0.2554 - val_acc: 0.9206\n",
      "Epoch 218/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2176 - acc: 0.9259- ETA: 0s - loss: 0.2149 - acc: 0.92Epoch 00218: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2181 - acc: 0.9255 - val_loss: 0.2627 - val_acc: 0.9185\n",
      "Epoch 219/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2228 - acc: 0.9264Epoch 00219: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2217 - acc: 0.9268 - val_loss: 0.2623 - val_acc: 0.9180\n",
      "Epoch 220/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2149 - acc: 0.9280- ETA: 0s - loss: 0.2155 - acc: 0.927Epoch 00220: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2146 - acc: 0.9283 - val_loss: 0.2570 - val_acc: 0.9204\n",
      "Epoch 221/400\n",
      "13056/13776 [===========================>..] - ETA: 0s - loss: 0.2183 - acc: 0.9259Epoch 00221: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2174 - acc: 0.9257 - val_loss: 0.2575 - val_acc: 0.9233\n",
      "Epoch 222/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2190 - acc: 0.9257Epoch 00222: val_loss improved from 0.25543 to 0.25495, saving model to weights.hdf5\n",
      "13776/13776 [==============================] - 1s 101us/step - loss: 0.2201 - acc: 0.9247 - val_loss: 0.2550 - val_acc: 0.9212\n",
      "Epoch 223/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2150 - acc: 0.9253Epoch 00223: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2131 - acc: 0.9257 - val_loss: 0.2651 - val_acc: 0.9197\n",
      "Epoch 224/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2155 - acc: 0.9277Epoch 00224: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2165 - acc: 0.9276 - val_loss: 0.2661 - val_acc: 0.9216\n",
      "Epoch 225/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2117 - acc: 0.9271- ETA: 0s - loss: 0.2104 - acc: 0.927Epoch 00225: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2126 - acc: 0.9273 - val_loss: 0.2620 - val_acc: 0.9224\n",
      "Epoch 226/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2086 - acc: 0.9289Epoch 00226: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2089 - acc: 0.9288 - val_loss: 0.2624 - val_acc: 0.9207\n",
      "Epoch 227/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2027 - acc: 0.9308Epoch 00227: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2047 - acc: 0.9301 - val_loss: 0.2635 - val_acc: 0.9199\n",
      "Epoch 228/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2110 - acc: 0.9291Epoch 00228: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2120 - acc: 0.9280 - val_loss: 0.2678 - val_acc: 0.9204\n",
      "Epoch 229/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2150 - acc: 0.9270Epoch 00229: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2144 - acc: 0.9273 - val_loss: 0.2621 - val_acc: 0.9224\n",
      "Epoch 230/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2046 - acc: 0.9280Epoch 00230: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2066 - acc: 0.9276 - val_loss: 0.2729 - val_acc: 0.9179\n",
      "Epoch 231/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2080 - acc: 0.9282Epoch 00231: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2088 - acc: 0.9282 - val_loss: 0.2674 - val_acc: 0.9187\n",
      "Epoch 232/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2085 - acc: 0.9277Epoch 00232: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2096 - acc: 0.9275 - val_loss: 0.2584 - val_acc: 0.9226\n",
      "Epoch 233/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2059 - acc: 0.9292Epoch 00233: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2065 - acc: 0.9287 - val_loss: 0.2672 - val_acc: 0.9172\n",
      "Epoch 234/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2007 - acc: 0.9332Epoch 00234: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2022 - acc: 0.9328 - val_loss: 0.2673 - val_acc: 0.9195\n",
      "Epoch 235/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2045 - acc: 0.9319Epoch 00235: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2043 - acc: 0.9318 - val_loss: 0.2711 - val_acc: 0.9172\n",
      "Epoch 236/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2024 - acc: 0.9308Epoch 00236: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2003 - acc: 0.9316 - val_loss: 0.2631 - val_acc: 0.9209\n",
      "Epoch 237/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2097 - acc: 0.9274Epoch 00237: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 96us/step - loss: 0.2097 - acc: 0.9271 - val_loss: 0.2647 - val_acc: 0.9187\n",
      "Epoch 238/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2026 - acc: 0.9325Epoch 00238: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2034 - acc: 0.9323 - val_loss: 0.2654 - val_acc: 0.9194\n",
      "Epoch 239/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1965 - acc: 0.9328Epoch 00239: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2001 - acc: 0.9313 - val_loss: 0.2680 - val_acc: 0.9184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2085 - acc: 0.9289Epoch 00240: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2077 - acc: 0.9289 - val_loss: 0.2652 - val_acc: 0.9221\n",
      "Epoch 241/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2071 - acc: 0.9287Epoch 00241: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2074 - acc: 0.9281 - val_loss: 0.2664 - val_acc: 0.9187\n",
      "Epoch 242/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1994 - acc: 0.9334Epoch 00242: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2004 - acc: 0.9333 - val_loss: 0.2638 - val_acc: 0.9207\n",
      "Epoch 243/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1979 - acc: 0.9332Epoch 00243: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.1978 - acc: 0.9331 - val_loss: 0.2745 - val_acc: 0.9187\n",
      "Epoch 244/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1983 - acc: 0.9322Epoch 00244: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1979 - acc: 0.9326 - val_loss: 0.2690 - val_acc: 0.9187\n",
      "Epoch 245/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1998 - acc: 0.9324Epoch 00245: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2010 - acc: 0.9321 - val_loss: 0.2635 - val_acc: 0.9189\n",
      "Epoch 246/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2071 - acc: 0.9283Epoch 00246: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2068 - acc: 0.9282 - val_loss: 0.2633 - val_acc: 0.9189\n",
      "Epoch 247/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2054 - acc: 0.9307Epoch 00247: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2065 - acc: 0.9302 - val_loss: 0.2666 - val_acc: 0.9195\n",
      "Epoch 248/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1952 - acc: 0.9348Epoch 00248: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1943 - acc: 0.9353 - val_loss: 0.2682 - val_acc: 0.9179\n",
      "Epoch 249/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2051 - acc: 0.9277Epoch 00249: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2065 - acc: 0.9276 - val_loss: 0.2667 - val_acc: 0.9172\n",
      "Epoch 250/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2009 - acc: 0.9309Epoch 00250: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.2012 - acc: 0.9311 - val_loss: 0.2711 - val_acc: 0.9194\n",
      "Epoch 251/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1930 - acc: 0.9349Epoch 00251: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1941 - acc: 0.9345 - val_loss: 0.2686 - val_acc: 0.9192\n",
      "Epoch 252/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1979 - acc: 0.9311Epoch 00252: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1958 - acc: 0.9321 - val_loss: 0.2688 - val_acc: 0.9194\n",
      "Epoch 253/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1924 - acc: 0.9342Epoch 00253: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1946 - acc: 0.9332 - val_loss: 0.2648 - val_acc: 0.9187\n",
      "Epoch 254/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2021 - acc: 0.9307Epoch 00254: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2013 - acc: 0.9312 - val_loss: 0.2646 - val_acc: 0.9241\n",
      "Epoch 255/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2017 - acc: 0.9304Epoch 00255: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2005 - acc: 0.9305 - val_loss: 0.2737 - val_acc: 0.9168\n",
      "Epoch 256/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2027 - acc: 0.9319Epoch 00256: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.2035 - acc: 0.9320 - val_loss: 0.2643 - val_acc: 0.9221\n",
      "Epoch 257/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2016 - acc: 0.9327Epoch 00257: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2021 - acc: 0.9321 - val_loss: 0.2768 - val_acc: 0.9189\n",
      "Epoch 258/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1947 - acc: 0.9361Epoch 00258: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1962 - acc: 0.9356 - val_loss: 0.2680 - val_acc: 0.9180\n",
      "Epoch 259/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1995 - acc: 0.9319Epoch 00259: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1983 - acc: 0.9325 - val_loss: 0.2690 - val_acc: 0.9180\n",
      "Epoch 260/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2048 - acc: 0.9307Epoch 00260: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2049 - acc: 0.9304 - val_loss: 0.2728 - val_acc: 0.9150\n",
      "Epoch 261/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1957 - acc: 0.9337Epoch 00261: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1960 - acc: 0.9335 - val_loss: 0.2659 - val_acc: 0.9173\n",
      "Epoch 262/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1957 - acc: 0.9321Epoch 00262: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.1955 - acc: 0.9326 - val_loss: 0.2697 - val_acc: 0.9195\n",
      "Epoch 263/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2010 - acc: 0.9335Epoch 00263: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.2001 - acc: 0.9335 - val_loss: 0.2625 - val_acc: 0.9195\n",
      "Epoch 264/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1968 - acc: 0.9316Epoch 00264: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1977 - acc: 0.9313 - val_loss: 0.2585 - val_acc: 0.9214\n",
      "Epoch 265/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1969 - acc: 0.9331Epoch 00265: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.1979 - acc: 0.9323 - val_loss: 0.2672 - val_acc: 0.9179\n",
      "Epoch 266/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1928 - acc: 0.9340Epoch 00266: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1940 - acc: 0.9336 - val_loss: 0.2620 - val_acc: 0.9197\n",
      "Epoch 267/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1958 - acc: 0.9326Epoch 00267: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1949 - acc: 0.9331 - val_loss: 0.2637 - val_acc: 0.9195\n",
      "Epoch 268/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1934 - acc: 0.9323Epoch 00268: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1938 - acc: 0.9322 - val_loss: 0.2651 - val_acc: 0.9192\n",
      "Epoch 269/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1927 - acc: 0.9331Epoch 00269: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1944 - acc: 0.9329 - val_loss: 0.2623 - val_acc: 0.9180\n",
      "Epoch 270/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1935 - acc: 0.9361Epoch 00270: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1933 - acc: 0.9358 - val_loss: 0.2611 - val_acc: 0.9204\n",
      "Epoch 271/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1937 - acc: 0.9329Epoch 00271: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1939 - acc: 0.9329 - val_loss: 0.2635 - val_acc: 0.9192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 272/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1992 - acc: 0.9320Epoch 00272: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1987 - acc: 0.9326 - val_loss: 0.2662 - val_acc: 0.9180\n",
      "Epoch 273/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.2007 - acc: 0.9319Epoch 00273: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.2018 - acc: 0.9324 - val_loss: 0.2734 - val_acc: 0.9165\n",
      "Epoch 274/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1917 - acc: 0.9377Epoch 00274: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1910 - acc: 0.9376 - val_loss: 0.2642 - val_acc: 0.9194\n",
      "Epoch 275/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1785 - acc: 0.9361Epoch 00275: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1799 - acc: 0.9358 - val_loss: 0.2664 - val_acc: 0.9207\n",
      "Epoch 276/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1896 - acc: 0.9338Epoch 00276: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1902 - acc: 0.9337 - val_loss: 0.2608 - val_acc: 0.9212\n",
      "Epoch 277/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1934 - acc: 0.9347Epoch 00277: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1927 - acc: 0.9347 - val_loss: 0.2670 - val_acc: 0.9245\n",
      "Epoch 278/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1843 - acc: 0.9393Epoch 00278: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1849 - acc: 0.9391 - val_loss: 0.2702 - val_acc: 0.9204\n",
      "Epoch 279/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1831 - acc: 0.9386- ETA: 0s - loss: 0.1852 - accEpoch 00279: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1837 - acc: 0.9383 - val_loss: 0.2704 - val_acc: 0.9228\n",
      "Epoch 280/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1852 - acc: 0.9338Epoch 00280: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1848 - acc: 0.9341 - val_loss: 0.2754 - val_acc: 0.9185\n",
      "Epoch 281/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1938 - acc: 0.9355Epoch 00281: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.1949 - acc: 0.9355 - val_loss: 0.2672 - val_acc: 0.9185\n",
      "Epoch 282/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1852 - acc: 0.9373Epoch 00282: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1845 - acc: 0.9376 - val_loss: 0.2697 - val_acc: 0.9219\n",
      "Epoch 283/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1931 - acc: 0.9355Epoch 00283: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1934 - acc: 0.9352 - val_loss: 0.2680 - val_acc: 0.9221\n",
      "Epoch 284/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1857 - acc: 0.9384Epoch 00284: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1852 - acc: 0.9386 - val_loss: 0.2642 - val_acc: 0.9214\n",
      "Epoch 285/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1939 - acc: 0.9351Epoch 00285: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1934 - acc: 0.9350 - val_loss: 0.2664 - val_acc: 0.9229\n",
      "Epoch 286/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1902 - acc: 0.9372Epoch 00286: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.1910 - acc: 0.9367 - val_loss: 0.2676 - val_acc: 0.9206\n",
      "Epoch 287/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1817 - acc: 0.9373Epoch 00287: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1802 - acc: 0.9381 - val_loss: 0.2679 - val_acc: 0.9204\n",
      "Epoch 288/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1838 - acc: 0.9395Epoch 00288: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1839 - acc: 0.9390 - val_loss: 0.2682 - val_acc: 0.9207\n",
      "Epoch 289/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1848 - acc: 0.9398Epoch 00289: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1831 - acc: 0.9405 - val_loss: 0.2736 - val_acc: 0.9195\n",
      "Epoch 290/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1843 - acc: 0.9375Epoch 00290: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1870 - acc: 0.9365 - val_loss: 0.2712 - val_acc: 0.9206\n",
      "Epoch 291/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1877 - acc: 0.9337Epoch 00291: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1868 - acc: 0.9337 - val_loss: 0.2666 - val_acc: 0.9206\n",
      "Epoch 292/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1802 - acc: 0.9402Epoch 00292: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1813 - acc: 0.9400 - val_loss: 0.2740 - val_acc: 0.9202\n",
      "Epoch 293/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1932 - acc: 0.9361Epoch 00293: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1927 - acc: 0.9358 - val_loss: 0.2729 - val_acc: 0.9197\n",
      "Epoch 294/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1726 - acc: 0.9395Epoch 00294: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1752 - acc: 0.9382 - val_loss: 0.2698 - val_acc: 0.9216\n",
      "Epoch 295/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1876 - acc: 0.9361Epoch 00295: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1879 - acc: 0.9365 - val_loss: 0.2706 - val_acc: 0.9197\n",
      "Epoch 296/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1883 - acc: 0.9334Epoch 00296: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1895 - acc: 0.9335 - val_loss: 0.2696 - val_acc: 0.9219\n",
      "Epoch 297/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1830 - acc: 0.9385Epoch 00297: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1831 - acc: 0.9385 - val_loss: 0.2597 - val_acc: 0.9226\n",
      "Epoch 298/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1717 - acc: 0.9419Epoch 00298: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1730 - acc: 0.9416 - val_loss: 0.2680 - val_acc: 0.9206\n",
      "Epoch 299/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1769 - acc: 0.9401Epoch 00299: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1753 - acc: 0.9406 - val_loss: 0.2752 - val_acc: 0.9201\n",
      "Epoch 300/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1821 - acc: 0.9377Epoch 00300: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1819 - acc: 0.9382 - val_loss: 0.2779 - val_acc: 0.9160\n",
      "Epoch 301/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1840 - acc: 0.9349Epoch 00301: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.1849 - acc: 0.9342 - val_loss: 0.2700 - val_acc: 0.9211\n",
      "Epoch 302/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1900 - acc: 0.9358Epoch 00302: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1903 - acc: 0.9358 - val_loss: 0.2705 - val_acc: 0.9177\n",
      "Epoch 303/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1788 - acc: 0.9396Epoch 00303: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1800 - acc: 0.9387 - val_loss: 0.2724 - val_acc: 0.9206\n",
      "Epoch 304/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1766 - acc: 0.9420Epoch 00304: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 100us/step - loss: 0.1786 - acc: 0.9411 - val_loss: 0.2658 - val_acc: 0.9217\n",
      "Epoch 305/400\n",
      "13056/13776 [===========================>..] - ETA: 0s - loss: 0.1823 - acc: 0.9354Epoch 00305: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1828 - acc: 0.9357 - val_loss: 0.2678 - val_acc: 0.9201\n",
      "Epoch 306/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1839 - acc: 0.9391Epoch 00306: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1841 - acc: 0.9392 - val_loss: 0.2663 - val_acc: 0.9201\n",
      "Epoch 307/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1797 - acc: 0.9397Epoch 00307: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1807 - acc: 0.9393 - val_loss: 0.2675 - val_acc: 0.9221\n",
      "Epoch 308/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1930 - acc: 0.9362Epoch 00308: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1916 - acc: 0.9366 - val_loss: 0.2618 - val_acc: 0.9223\n",
      "Epoch 309/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1756 - acc: 0.9397Epoch 00309: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1742 - acc: 0.9401 - val_loss: 0.2679 - val_acc: 0.9214\n",
      "Epoch 310/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1807 - acc: 0.9373Epoch 00310: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1798 - acc: 0.9376 - val_loss: 0.2595 - val_acc: 0.9226\n",
      "Epoch 311/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1801 - acc: 0.9398Epoch 00311: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1808 - acc: 0.9397 - val_loss: 0.2617 - val_acc: 0.9231\n",
      "Epoch 312/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1759 - acc: 0.9402Epoch 00312: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1747 - acc: 0.9407 - val_loss: 0.2625 - val_acc: 0.9217\n",
      "Epoch 313/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1781 - acc: 0.9387Epoch 00313: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1781 - acc: 0.9390 - val_loss: 0.2613 - val_acc: 0.9207\n",
      "Epoch 314/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1735 - acc: 0.9403Epoch 00314: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1735 - acc: 0.9400 - val_loss: 0.2713 - val_acc: 0.9223\n",
      "Epoch 315/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1753 - acc: 0.9410Epoch 00315: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1770 - acc: 0.9408 - val_loss: 0.2710 - val_acc: 0.9211\n",
      "Epoch 316/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1744 - acc: 0.9422Epoch 00316: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1755 - acc: 0.9419 - val_loss: 0.2660 - val_acc: 0.9212\n",
      "Epoch 317/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1775 - acc: 0.9392Epoch 00317: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1778 - acc: 0.9388 - val_loss: 0.2614 - val_acc: 0.9255\n",
      "Epoch 318/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1743 - acc: 0.9400Epoch 00318: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1752 - acc: 0.9398 - val_loss: 0.2677 - val_acc: 0.9202\n",
      "Epoch 319/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1796 - acc: 0.9383Epoch 00319: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1807 - acc: 0.9379 - val_loss: 0.2681 - val_acc: 0.9194\n",
      "Epoch 320/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1840 - acc: 0.9390Epoch 00320: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1841 - acc: 0.9389 - val_loss: 0.2642 - val_acc: 0.9226\n",
      "Epoch 321/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1721 - acc: 0.9395Epoch 00321: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1728 - acc: 0.9395 - val_loss: 0.2636 - val_acc: 0.9214\n",
      "Epoch 322/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1727 - acc: 0.9392Epoch 00322: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1729 - acc: 0.9393 - val_loss: 0.2651 - val_acc: 0.9236\n",
      "Epoch 323/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1788 - acc: 0.9383Epoch 00323: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1783 - acc: 0.9382 - val_loss: 0.2650 - val_acc: 0.9236\n",
      "Epoch 324/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1759 - acc: 0.9425Epoch 00324: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.1757 - acc: 0.9424 - val_loss: 0.2679 - val_acc: 0.9209\n",
      "Epoch 325/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1720 - acc: 0.9413Epoch 00325: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.1715 - acc: 0.9416 - val_loss: 0.2671 - val_acc: 0.9226\n",
      "Epoch 326/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1810 - acc: 0.9395Epoch 00326: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.1795 - acc: 0.9401 - val_loss: 0.2688 - val_acc: 0.9216\n",
      "Epoch 327/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1750 - acc: 0.9379Epoch 00327: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.1750 - acc: 0.9378 - val_loss: 0.2672 - val_acc: 0.9229\n",
      "Epoch 328/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1808 - acc: 0.9386Epoch 00328: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1804 - acc: 0.9390 - val_loss: 0.2709 - val_acc: 0.9238\n",
      "Epoch 329/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1699 - acc: 0.9420Epoch 00329: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1709 - acc: 0.9415 - val_loss: 0.2625 - val_acc: 0.9234\n",
      "Epoch 330/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1742 - acc: 0.9420Epoch 00330: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1732 - acc: 0.9423 - val_loss: 0.2712 - val_acc: 0.9197\n",
      "Epoch 331/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1680 - acc: 0.9416Epoch 00331: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1683 - acc: 0.9418 - val_loss: 0.2585 - val_acc: 0.9250\n",
      "Epoch 332/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1639 - acc: 0.9455Epoch 00332: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1642 - acc: 0.9451 - val_loss: 0.2637 - val_acc: 0.9231\n",
      "Epoch 333/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1732 - acc: 0.9423Epoch 00333: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1738 - acc: 0.9424 - val_loss: 0.2637 - val_acc: 0.9228\n",
      "Epoch 334/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1762 - acc: 0.9398Epoch 00334: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1765 - acc: 0.9398 - val_loss: 0.2630 - val_acc: 0.9190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1714 - acc: 0.9407Epoch 00335: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.1704 - acc: 0.9413 - val_loss: 0.2670 - val_acc: 0.9202\n",
      "Epoch 336/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1754 - acc: 0.9383Epoch 00336: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1741 - acc: 0.9390 - val_loss: 0.2646 - val_acc: 0.9221\n",
      "Epoch 337/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1705 - acc: 0.9418Epoch 00337: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1704 - acc: 0.9421 - val_loss: 0.2667 - val_acc: 0.9229\n",
      "Epoch 338/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1671 - acc: 0.9446Epoch 00338: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1662 - acc: 0.9447 - val_loss: 0.2702 - val_acc: 0.9223\n",
      "Epoch 339/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1804 - acc: 0.9392Epoch 00339: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.1789 - acc: 0.9393 - val_loss: 0.2748 - val_acc: 0.9189\n",
      "Epoch 340/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1702 - acc: 0.9425Epoch 00340: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1706 - acc: 0.9424 - val_loss: 0.2705 - val_acc: 0.9219\n",
      "Epoch 341/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1620 - acc: 0.9445Epoch 00341: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1609 - acc: 0.9448 - val_loss: 0.2672 - val_acc: 0.9217\n",
      "Epoch 342/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1745 - acc: 0.9399Epoch 00342: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1749 - acc: 0.9400 - val_loss: 0.2658 - val_acc: 0.9246\n",
      "Epoch 343/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1638 - acc: 0.9437Epoch 00343: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1653 - acc: 0.9428 - val_loss: 0.2667 - val_acc: 0.9233\n",
      "Epoch 344/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1679 - acc: 0.9444Epoch 00344: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1671 - acc: 0.9445 - val_loss: 0.2789 - val_acc: 0.9184\n",
      "Epoch 345/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1666 - acc: 0.9461Epoch 00345: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1665 - acc: 0.9457 - val_loss: 0.2661 - val_acc: 0.9234\n",
      "Epoch 346/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1735 - acc: 0.9407Epoch 00346: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1753 - acc: 0.9401 - val_loss: 0.2694 - val_acc: 0.9217\n",
      "Epoch 347/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1703 - acc: 0.9414Epoch 00347: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1714 - acc: 0.9413 - val_loss: 0.2630 - val_acc: 0.9228\n",
      "Epoch 348/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1746 - acc: 0.9401Epoch 00348: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1748 - acc: 0.9399 - val_loss: 0.2621 - val_acc: 0.9229\n",
      "Epoch 349/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1711 - acc: 0.9407Epoch 00349: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1704 - acc: 0.9411 - val_loss: 0.2641 - val_acc: 0.9202\n",
      "Epoch 350/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1648 - acc: 0.9458Epoch 00350: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1664 - acc: 0.9451 - val_loss: 0.2682 - val_acc: 0.9236\n",
      "Epoch 351/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1669 - acc: 0.9437Epoch 00351: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1684 - acc: 0.9435 - val_loss: 0.2710 - val_acc: 0.9221\n",
      "Epoch 352/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1577 - acc: 0.9461Epoch 00352: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1583 - acc: 0.9460 - val_loss: 0.2734 - val_acc: 0.9207\n",
      "Epoch 353/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1645 - acc: 0.9435Epoch 00353: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1639 - acc: 0.9438 - val_loss: 0.2676 - val_acc: 0.9211\n",
      "Epoch 354/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1590 - acc: 0.9466Epoch 00354: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1610 - acc: 0.9460 - val_loss: 0.2669 - val_acc: 0.9212\n",
      "Epoch 355/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1703 - acc: 0.9437Epoch 00355: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1698 - acc: 0.9437 - val_loss: 0.2681 - val_acc: 0.9209\n",
      "Epoch 356/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1738 - acc: 0.9415Epoch 00356: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1738 - acc: 0.9412 - val_loss: 0.2687 - val_acc: 0.9195\n",
      "Epoch 357/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1638 - acc: 0.9458Epoch 00357: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1634 - acc: 0.9456 - val_loss: 0.2736 - val_acc: 0.9233\n",
      "Epoch 358/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1597 - acc: 0.9479Epoch 00358: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1599 - acc: 0.9474 - val_loss: 0.2625 - val_acc: 0.9251\n",
      "Epoch 359/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1696 - acc: 0.9423Epoch 00359: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1684 - acc: 0.9428 - val_loss: 0.2742 - val_acc: 0.9221\n",
      "Epoch 360/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1616 - acc: 0.9432Epoch 00360: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.1625 - acc: 0.9429 - val_loss: 0.2756 - val_acc: 0.9184\n",
      "Epoch 361/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1663 - acc: 0.9450Epoch 00361: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1664 - acc: 0.9450 - val_loss: 0.2614 - val_acc: 0.9245\n",
      "Epoch 362/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1753 - acc: 0.9412Epoch 00362: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1750 - acc: 0.9413 - val_loss: 0.2616 - val_acc: 0.9226\n",
      "Epoch 363/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1662 - acc: 0.9423Epoch 00363: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1644 - acc: 0.9431 - val_loss: 0.2626 - val_acc: 0.9236\n",
      "Epoch 364/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1673 - acc: 0.9443Epoch 00364: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1672 - acc: 0.9443 - val_loss: 0.2715 - val_acc: 0.9238\n",
      "Epoch 365/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1711 - acc: 0.9422Epoch 00365: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1690 - acc: 0.9426 - val_loss: 0.2679 - val_acc: 0.9224\n",
      "Epoch 366/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1656 - acc: 0.9446Epoch 00366: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1647 - acc: 0.9450 - val_loss: 0.2718 - val_acc: 0.9236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 367/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1619 - acc: 0.9457Epoch 00367: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1626 - acc: 0.9450 - val_loss: 0.2760 - val_acc: 0.9192\n",
      "Epoch 368/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1620 - acc: 0.9448Epoch 00368: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1623 - acc: 0.9452 - val_loss: 0.2688 - val_acc: 0.9207\n",
      "Epoch 369/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1643 - acc: 0.9463Epoch 00369: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1649 - acc: 0.9462 - val_loss: 0.2790 - val_acc: 0.9204\n",
      "Epoch 370/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1660 - acc: 0.9424Epoch 00370: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1664 - acc: 0.9422 - val_loss: 0.2644 - val_acc: 0.9201\n",
      "Epoch 371/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1629 - acc: 0.9427Epoch 00371: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1638 - acc: 0.9424 - val_loss: 0.2709 - val_acc: 0.9221\n",
      "Epoch 372/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1575 - acc: 0.9446Epoch 00372: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1584 - acc: 0.9443 - val_loss: 0.2759 - val_acc: 0.9214\n",
      "Epoch 373/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1623 - acc: 0.9447Epoch 00373: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1631 - acc: 0.9442 - val_loss: 0.2677 - val_acc: 0.9224\n",
      "Epoch 374/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1545 - acc: 0.9464Epoch 00374: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1535 - acc: 0.9467 - val_loss: 0.2745 - val_acc: 0.9221\n",
      "Epoch 375/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1698 - acc: 0.9425Epoch 00375: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1692 - acc: 0.9424 - val_loss: 0.2693 - val_acc: 0.9224\n",
      "Epoch 376/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1584 - acc: 0.9452Epoch 00376: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1588 - acc: 0.9453 - val_loss: 0.2669 - val_acc: 0.9243\n",
      "Epoch 377/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1601 - acc: 0.9455Epoch 00377: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1602 - acc: 0.9456 - val_loss: 0.2635 - val_acc: 0.9260\n",
      "Epoch 378/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1679 - acc: 0.9433Epoch 00378: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.1679 - acc: 0.9432 - val_loss: 0.2685 - val_acc: 0.9217\n",
      "Epoch 379/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1619 - acc: 0.9473Epoch 00379: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1613 - acc: 0.9478 - val_loss: 0.2677 - val_acc: 0.9211\n",
      "Epoch 380/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1702 - acc: 0.9422Epoch 00380: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1702 - acc: 0.9421 - val_loss: 0.2710 - val_acc: 0.9211\n",
      "Epoch 381/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1626 - acc: 0.9447Epoch 00381: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1633 - acc: 0.9441 - val_loss: 0.2699 - val_acc: 0.9214\n",
      "Epoch 382/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1624 - acc: 0.9471Epoch 00382: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1638 - acc: 0.9471 - val_loss: 0.2696 - val_acc: 0.9234\n",
      "Epoch 383/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1654 - acc: 0.9425Epoch 00383: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1638 - acc: 0.9429 - val_loss: 0.2667 - val_acc: 0.9226\n",
      "Epoch 384/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1624 - acc: 0.9441Epoch 00384: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1636 - acc: 0.9440 - val_loss: 0.2646 - val_acc: 0.9211\n",
      "Epoch 385/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1516 - acc: 0.9475Epoch 00385: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1518 - acc: 0.9473 - val_loss: 0.2696 - val_acc: 0.9224\n",
      "Epoch 386/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1585 - acc: 0.9485Epoch 00386: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1596 - acc: 0.9480 - val_loss: 0.2692 - val_acc: 0.9239\n",
      "Epoch 387/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1542 - acc: 0.9470Epoch 00387: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1528 - acc: 0.9474 - val_loss: 0.2694 - val_acc: 0.9239\n",
      "Epoch 388/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1642 - acc: 0.9440Epoch 00388: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1649 - acc: 0.9436 - val_loss: 0.2760 - val_acc: 0.9199\n",
      "Epoch 389/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1621 - acc: 0.944 - ETA: 0s - loss: 0.1620 - acc: 0.9443Epoch 00389: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1640 - acc: 0.9436 - val_loss: 0.2665 - val_acc: 0.9211\n",
      "Epoch 390/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1534 - acc: 0.9482Epoch 00390: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1530 - acc: 0.9485 - val_loss: 0.2686 - val_acc: 0.9202\n",
      "Epoch 391/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1469 - acc: 0.9504Epoch 00391: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1472 - acc: 0.9503 - val_loss: 0.2742 - val_acc: 0.9206\n",
      "Epoch 392/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1553 - acc: 0.9491Epoch 00392: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1532 - acc: 0.9496 - val_loss: 0.2814 - val_acc: 0.9177\n",
      "Epoch 393/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1569 - acc: 0.9471Epoch 00393: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1573 - acc: 0.9467 - val_loss: 0.2786 - val_acc: 0.9223\n",
      "Epoch 394/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1592 - acc: 0.9462Epoch 00394: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 97us/step - loss: 0.1595 - acc: 0.9461 - val_loss: 0.2790 - val_acc: 0.9207\n",
      "Epoch 395/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1583 - acc: 0.9465Epoch 00395: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1577 - acc: 0.9469 - val_loss: 0.2786 - val_acc: 0.9221\n",
      "Epoch 396/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1545 - acc: 0.9468Epoch 00396: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1547 - acc: 0.9464 - val_loss: 0.2794 - val_acc: 0.9212\n",
      "Epoch 397/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1581 - acc: 0.9457- ETA: 0s - loss: 0.1441 Epoch 00397: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1570 - acc: 0.9461 - val_loss: 0.2703 - val_acc: 0.9231\n",
      "Epoch 398/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1664 - acc: 0.9440Epoch 00398: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 99us/step - loss: 0.1667 - acc: 0.9438 - val_loss: 0.2755 - val_acc: 0.9221\n",
      "Epoch 399/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1708 - acc: 0.9429Epoch 00399: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1692 - acc: 0.9432 - val_loss: 0.2759 - val_acc: 0.9201\n",
      "Epoch 400/400\n",
      "13312/13776 [===========================>..] - ETA: 0s - loss: 0.1619 - acc: 0.9467Epoch 00400: val_loss did not improve\n",
      "13776/13776 [==============================] - 1s 98us/step - loss: 0.1613 - acc: 0.9466 - val_loss: 0.2776 - val_acc: 0.9211\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5',verbose=1,save_best_only=True)\n",
    "history=model.fit(X_train.reshape(X_train.shape[0],20,40,1), to_categorical(y_train),\n",
    "          batch_size=256, epochs=400, verbose=1,validation_split=0.3,callbacks=[checkpointer] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff6d86d5208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3Xd4VVXWwOHfSu8hlQQCJPReQxNBiijYsKBg745tHMdRR52xl/FTR9GxouKooyJiQ0VRFMRCR3oNNSFAQkgjIX1/f+ybSxKSEEJu6nqfh+fec+4pKzfhrLPL2VuMMSillFIAbg0dgFJKqcZDk4JSSiknTQpKKaWcNCkopZRy0qSglFLKSZOCUkopJ00KStWQiPxXRJ6o4ba7ROT0kz2OUvVNk4JSSiknTQpKKaWcNCmoZsVRbXOPiKwVkRwReVtEWovItyKSLSLzRSSkzPbnicgGEckQkYUi0qPMZwNEZJVjv48BnwrnOkdEVjv2/V1E+tYy5htFJEFEDonIHBFp41gvIvKCiKSISKbjZ+rt+OwsEdnoiG2viNxdqy9MqQo0Kajm6CJgPNAVOBf4FngACMf+zd8BICJdgY+AO4EIYC7wlYh4iYgX8AXwPhAKfOI4Lo59BwIzgD8BYcAbwBwR8T6RQEVkLPAv4BIgGtgNzHR8fAYwyvFztAKmAGmOz94G/mSMCQR6Az+dyHmVqoomBdUc/ccYc8AYsxf4BVhqjPnDGJMPfA4McGw3BfjGGPODMaYQeA7wBU4BhgGewDRjTKExZjawvMw5bgTeMMYsNcYUG2PeBfId+52Iy4EZxphVjvjuB4aLSCxQCAQC3QExxmwyxuxz7FcI9BSRIGNMujFm1QmeV6lKaVJQzdGBMu+PVLIc4HjfBntnDoAxpgRIBNo6Pttryo8YubvM+w7A3xxVRxkikgG0c+x3IirGcBhbGmhrjPkJeBl4BTggItNFJMix6UXAWcBuEflZRIaf4HmVqpQmBdWSJWMv7oCtw8de2PcC+4C2jnWl2pd5nwg8aYxpVeafnzHmo5OMwR9bHbUXwBjzkjFmENALW410j2P9cmPMJCASW8016wTPq1SlNCmolmwWcLaIjBMRT+Bv2Cqg34HFQBFwh4h4iMiFwJAy+74J3CwiQx0Nwv4icraIBJ5gDB8C14pIf0d7xFPY6q5dIjLYcXxPIAfIA4odbR6Xi0iwo9orCyg+ie9BKSdNCqrFMsZsAa4A/gMcxDZKn2uMKTDGFAAXAtcA6dj2h8/K7LsC267wsuPzBMe2JxrDj8CDwKfY0kknYKrj4yBs8knHVjGlYds9AK4EdolIFnCz4+dQ6qSJTrKjlFKqlJYUlFJKOWlSUEop5aRJQSmllJMmBaWUUk4eDR3AiQoPDzexsbENHYZSSjUpK1euPGiMiTjedk0uKcTGxrJixYqGDkMppZoUEdl9/K20+kgppVQZmhSUUko5aVJQSinl1OTaFCpTWFhIUlISeXl5DR1Ks+Dj40NMTAyenp4NHYpSqp41i6SQlJREYGAgsbGxlB/UUp0oYwxpaWkkJSURFxfX0OEopepZs6g+ysvLIywsTBNCHRARwsLCtNSlVAvVLJICoAmhDul3qVTL5dKkICITRGSLY1Ly+yr5vIOI/OiYkHyhiMS4Kpac/CL2ZR5BR4VVSqmquSwpiIg7dhrBiUBP4FIR6Vlhs+eA94wxfYHHsBOYu0RuQTGp2fkUl9R9UsjIyODVV1894f3OOussMjIy6jwepZSqLVeWFIYACcaYHY4JS2YCkyps0xP40fF+QSWf1xlPd1slUliPSaG4uPrJsObOnUurVq3qPB6llKotVyaFtth5bEslOdaVtQY7ATnABUCgiIRVPJCI3CQiK0RkRWpqaq2C8XS3P2pRcUmt9q/Offfdx/bt2+nfvz+DBw9mzJgxXHbZZfTp0weA888/n0GDBtGrVy+mT5/u3C82NpaDBw+ya9cuevTowY033kivXr0444wzOHLkSJ3HqZRSx+PKLqmVtVZWvE2/G3hZRK4BFmEnKy86ZidjpgPTAeLj46u91X/0qw1sTM469sTGkFtQjLenOx5uJ9aQ2rNNEA+f26vKz59++mnWr1/P6tWrWbhwIWeffTbr1693dumcMWMGoaGhHDlyhMGDB3PRRRcRFlY+923bto2PPvqIN998k0suuYRPP/2UK67QGRaVUvXLlUkhCWhXZjkGSC67gTEmGTsPLiISAFxkjMl0RTClPWpsQ7Nre9cMGTKkXB//l156ic8//xyAxMREtm3bdkxSiIuLo3///gAMGjSIXbt2uTRGpZSqjCuTwnKgi4jEYUsAU4HLym4gIuHAIWNMCXA/MONkT1rdHf3G5EyCfT1pG+J3sqeplr+/v/P9woULmT9/PosXL8bPz4/Ro0dX+gyAt7e38727u7tWHymlGoTL2hSMMUXA7cA8YBMwyxizQUQeE5HzHJuNBraIyFagNfCkq+IB8HB3o7C47huaAwMDyc7OrvSzzMxMQkJC8PPzY/PmzSxZsqTOz6+UUnXFpcNcGGPmAnMrrHuozPvZwGxXxlCWp7sbhSV139AcFhbGiBEj6N27N76+vrRu3dr52YQJE3j99dfp27cv3bp1Y9iwYXV+fqWUqivS1B7mio+PNxUn2dm0aRM9evQ47r5Jh3LJzi+iR3SQq8JrNmr6nSqlmgYRWWmMiT/eds1mmIua8HB3o6i4RJ9qVkqpKrSopODpLhigyAUPsCmlVHPQwpKC/XELXfAAm1JKNQctKil4lA514YIeSEop1Ry0qKTgyqEulFKqOWhRScHDTRC0pKCUUlVpUUlBRBwPsDVsSSEgIACA5ORkJk+eXOk2o0ePpmLX24qmTZtGbm6uc1mH4lZKnawWlRTA9kBqLL2P2rRpw+zZtX92r2JS0KG4lVInq8UlBQ+3ui8p/P3vfy83n8IjjzzCo48+yrhx4xg4cCB9+vThyy+/PGa/Xbt20bt3bwCOHDnC1KlT6du3L1OmTCk39tEtt9xCfHw8vXr14uGHHwbsIHvJycmMGTOGMWPGAEeH4gZ4/vnn6d27N71792batGnO8+kQ3Uqp6rh0mIsG8e19sH9dlR9HFxXbkoLXCfzoUX1g4tNVfjx16lTuvPNObr31VgBmzZrFd999x1//+leCgoI4ePAgw4YN47zzzqty/uPXXnsNPz8/1q5dy9q1axk4cKDzsyeffJLQ0FCKi4sZN24ca9eu5Y477uD5559nwYIFhIeHlzvWypUreeedd1i6dCnGGIYOHcppp51GSEiIDtGtlKpWiyspiAjGgDlmaofaGzBgACkpKSQnJ7NmzRpCQkKIjo7mgQceoG/fvpx++uns3buXAwcOVHmMRYsWOS/Offv2pW/fvs7PZs2axcCBAxkwYAAbNmxg48aN1cbz66+/csEFF+Dv709AQAAXXnghv/zyC6BDdCulqtf8SgrV3NEDHM4pICk9l47h/gT4eNbZaSdPnszs2bPZv38/U6dO5YMPPiA1NZWVK1fi6elJbGxspUNml1VZKWLnzp0899xzLF++nJCQEK655prjHqe6YTx0iG6lVHVaXEmhdK7mHQdz6vR5halTpzJz5kxmz57N5MmTyczMJDIyEk9PTxYsWMDu3bur3X/UqFF88MEHAKxfv561a9cCkJWVhb+/P8HBwRw4cIBvv/3WuU9VQ3aPGjWKL774gtzcXHJycvj8888ZOXJknf2sSqnmq/mVFI7Dy+NoHiwsNni4181xe/XqRXZ2Nm3btiU6OprLL7+cc889l/j4ePr370/37t2r3f+WW27h2muvpW/fvvTv358hQ4YA0K9fPwYMGECvXr3o2LEjI0aMcO5z0003MXHiRKKjo1mwYIFz/cCBA7nmmmucx7jhhhsYMGCAVhUppY6rRQ2dXSojt4A9h3KJC/cnsA6rkJoTHTpbqeZFh86uhq+nLR4U6ZPNSilVjkuTgohMEJEtIpIgIvdV8nl7EVkgIn+IyFoROcuV8ZQqHRivyAWzsCmlVFPmsqQgIu7AK8BEoCdwqYj0rLDZP7FzNw8ApgKvUksnUg3mJoKbiJYUqtDUqhSVUnXHlSWFIUCCMWaHMaYAmAlMqrCNAUrnxgwGkmtzIh8fH9LS0mp8MRMRPNwaz3AXjYkxhrS0NHx8fBo6FKVUA3Bl76O2QGKZ5SRgaIVtHgG+F5E/A/7A6ZUdSERuAm4CaN++/TGfx8TEkJSURGpqao2DS8nOx03g8AHv42/cwvj4+BATE9PQYSilGoArk0Jl4zlUvDW/FPivMebfIjIceF9EehtjylX2G2OmA9PB9j6qeFBPT0/i4uJOKLh/v7uCpPRcvrtz1Antp5RSzZkrq4+SgHZllmM4tnroemAWgDFmMeADhFMPOkcGsD31MDn5RfVxOqWUahJcmRSWA11EJE5EvLANyXMqbLMHGAcgIj2wSaHmdUAnYWSXcAqLDUt3ptXH6ZRSqklwWVIwxhQBtwPzgE3YXkYbROQxETnPsdnfgBtFZA3wEXCNqaeuL4M6hODj6cav2zQpKKVUKZcOc2GMmQvMrbDuoTLvNwIjKu5XH3w83enVJpj1yZkNcXqllGqUWuQTzaW6RwWyeV+W9stXSimHlp0UooPIyitiX2b1Q1ErpVRL0bKTQlQgAJv3ZzVwJEop1Ti06KTQIzoIN4E1idquoJRS0MKTQoC3Bz2ig1ix+1BDh6KUUo1Ci04KAINjQ/ljTwaFdTgLm1JKNVUtPin0axdMbkExu9NyGjoUpZRqcC0+KUQF+QJ2gDyllGrpWnxSiAi0o6SmalJQSilNCpoUlFLqqBafFIJ8PPDycCP1sCYFpZRq8UlBRIgI8NaSglJKoUkBsFVImhSUUkqTAgDhWlJQSilAkwIA7UP92J56mN8SDjZ0KEop1aA0KQB3jOtMVLAP//lpW0OHopRSDUqTAtDKz4sx3SJZl5RJcYnOraCUarlcmhREZIKIbBGRBBG5r5LPXxCR1Y5/W0Ukw5XxVKd/u1bkFBSzLSW7oUJQSqkG57LpOEXEHXgFGA8kActFZI5jCk4AjDF/LbP9n4EBrornePq3awXAmsQMukcFNVQYSinVoFxZUhgCJBhjdhhjCoCZwKRqtr8U+MiF8VQrNswfH083th443FAhKKVUg3NlUmgLJJZZTnKsO4aIdADigJ+q+PwmEVkhIitSU1PrPFAANzchLjyAHamaFJRSLZcrk4JUsq6qVtypwGxjTHFlHxpjphtj4o0x8REREXUWYEWdIvzZnqpDaCulWi5XJoUkoF2Z5RgguYptp9KAVUelOkYEkJSeS15hpblJKaWaPVcmheVAFxGJExEv7IV/TsWNRKQbEAIsdmEsNdIpwp8SA1sPaA8kpVTL5LKkYIwpAm4H5gGbgFnGmA0i8piInFdm00uBmcaYBn9A4NTO4Xi5u/HZqr0NHYpSSjUIl3VJBTDGzAXmVlj3UIXlR1wZw4kIC/BmYp8oPl2ZxL0TuuHn5dKvRymlGh19ormCK4Z1IDu/iK/WVNX8oZRSzZcmhQriO4TQKcKfr9bsa+hQlFKq3mlSqEBEGNQhhM37sxo6FKWUqneaFCrRtXUgBw8XcFCn6FRKtTCaFCpROvbR1v3aNVUp1bJoUqhE16gAADbu0yokpVTLokmhEhEB3nSM8OeHjQcaOhSllKpXmhQqISKc168Ny3YdYsGWlIYORyml6o0mhSpMHdyeAC8P/vTeSh0LSSnVYmhSqEJUsA+PnNeLguIS9mYcaehwlFKqXmhSqEa7UD8A9qZrUlBKtQyaFKoRE+ILQJImBaVUC9FyksKKd+CFPlBcVONdWgf54OEmJKXnujAwpZRqPFpOUnD3gsw9kL6r5ru4Cb5e7ry6cDt/7El3XWxKKdVItJykENHNvh7cckK79W4TDMDbv+6s64iUUqrRaTlJIbyLfT249YR2mza1P7Fhfuw8qHM3K6Wav5aTFHyCISAKUk8sKbQO8mF8z9ZsSM7SORaUUs2eS5OCiEwQkS0ikiAi91WxzSUislFENojIh66Mh4iuJ1xSAOjSOhCAP3/0hz6zoJRq1lyWFETEHXgFmAj0BC4VkZ4VtukC3A+MMMb0Au50VTwAhHaCQztOeLeB7UOc7zfszazLiJRSqlFxZUlhCJBgjNlhjCkAZgKTKmxzI/CKMSYdwBjj2oGGQuPgyCE4knFCu3WODGDJ/eMQgQ3JOnKqUqr5cmVSaAsklllOcqwrqyvQVUR+E5ElIjKhsgOJyE0iskJEVqSmptY+opA4+5p+4j2JooJ96Bjuz0+bUygsLql9DEop1Yi5MilIJetMhWUPoAswGrgUeEtEWh2zkzHTjTHxxpj4iIiI2kcU2tG+1qIKCWBcj9as25vJf35KqH0MSinViLkyKSQB7cosxwAVu+8kAV8aYwqNMTuBLdgk4Rohsfb1UO2eObhvQnfah/qxJvHEqp+UUqqpcGVSWA50EZE4EfECpgJzKmzzBTAGQETCsdVJtbuNrwnvAAhqCykba7W7m5vQp20wew7psBdKqebJZUnBGFME3A7MAzYBs4wxG0TkMRE5z7HZPCBNRDYCC4B7jDFprooJgLaDIGlFrXdvF2ofZDv9+Z85nF/zcZSUUqop8HDlwY0xc4G5FdY9VOa9Ae5y/KsfMYNh0xw4nAIBkSe8e4cwO5x2Qsphlu5IY1TXCDzdW84zgEqp5q3lXc3aDbGvG76o1e5tWvk631//7gpu+d/KuohKKaUahZaXFGKGQMfR8MODkHfizxz0aRuMr6e7c3n+phRsgUcppZq+lpcU3Nwg/jooyoOM3Se8e6i/F5seL/84xa40bXhWSjUPLS8pAARG29fs/bU+xMuXDeDMXq0BWL7zUF1EpZRSDa6FJoUo+5pV+1FPz+nbhlcvH4Snu7BDh9VWSjUTLTMpBDiSwkmUFMDOzNYuxI89hzQpKKWah5aZFDy8wC8csk9+foT2YX7s1jYFpVQzUaOkICJ/EZEgsd4WkVUicoarg3OpwOiTLikAdAj1Y09arvZAUko1CzUtKVxnjMkCzgAigGuBp10WVX0Iioa07VB8ck8ltw/zJzu/iPTcwjoKTCmlGk5Nk0LpiKdnAe8YY9ZQ+SioTUfXCZC2DRY9c1KH6dUmCIBXFySwI/VwXUSmlFINpqZJYaWIfI9NCvNEJBBo2pMKDL4e2gyAxGUndZghsaF4ubvx1q87ueSNJXUUnFJKNYyaJoXrgfuAwcaYXMATW4XUtIV2rNWEO2W5uQlPXdgHgIOH83UOZ6VUk1bTpDAc2GKMyRCRK4B/Ak1/suKQOMhIPOl2hcmDYlh492gAXluYoI3OSqkmq6ZJ4TUgV0T6AfcCu4H3XBZVfQmJBVMMmYnH3fR4YsP9uf7UOP63ZA/L9AlnpVQTVdOkUOQY5noS8KIx5kUg0HVh1ZNQx5zNtZyes6K7xnfF28ONuev21cnxlFKqvtU0KWSLyP3AlcA3IuKObVdo2iK6g4cvfH4zZCad9OH8vT0Y3S2CeRsOaBWSUqpJqmlSmALkY59X2A+0BZ51WVT1xT8crp0L+Vnw42N1csjhHcPYn5XHSz8msFPHRFJKNTE1SgqORPABECwi5wB5xpjjtimIyAQR2SIiCSJyXyWfXyMiqSKy2vHvhhP+CU5W24HQ/3LY+CWUFJ/04bpF2ecWXpi/lcve1C6qSqmmpabDXFwCLAMuBi4BlorI5OPs4w68AkwEegKXikjPSjb92BjT3/HvrROKvq60HWjnV0jfddKH6to6wPl+X2YeJSVajaSUajpqWn30D+wzClcbY64ChgAPHmefIUCCMWaHMaYAmIltqG58InrY15SNJ32osADvcsvvLt510sdUSqn6UtOk4GaMSSmznFaDfdsCZft6JjnWVXSRiKwVkdki0q6yA4nITSKyQkRWpKam1jDkExDRzb6mbK6Tw43v2ZphHUMZ3jGMR7/ayDdrtTeSUqpp8Kjhdt+JyDzgI8fyFGDucfapbGykinUpXwEfGWPyReRm4F1g7DE7GTMdmA4QHx9f9/Ux3gHQqj0cWF8nh3vzqngACopKuODV37jtw1Vk5fXh0iHt6+T4SinlKjVtaL4He1HuC/QDphtj/n6c3ZKAsnf+MUC5CQyMMWnGmHzH4pvAoJrE4xLR/WHf6jo9pJeHGx/eOIwwfy/mbTj5YbqVUsrVajzJjjHmU2PMXcaYvxpjPq/BLsuBLiISJyJewFRgTtkNRCS6zOJ5wKaaxlPn2g6yDc1ZdVvVE+zryWndIliXlElCio6iqpRq3KpNCiKSLSJZlfzLFpGs6vY1xhQBtwPzsBf7WcaYDSLymIic59jsDhHZICJrgDuAa07+R6qltgPt6/Pd4cCGOj107zbBpOUUcPrzP7M/M69Oj62UUnWp2jYFY8xJDWVhjJlLhbYHY8xDZd7fD9x/MueoM20Ggncw5GfCmo/gjCfq7NClcy4ALN2ZxqT+lbW3K6VUw2uZczRXxjsA7tttJ9/Z8EWdHnpwbCjPTO4LwD2z17LtQHadHl8ppeqKJoWyRKDDKXbU1Ly6GxnczU24JL4dp/eIpKCohCe+abimE6WUqo4mhYpCYu1rHTzdXNGTF/ShV5sgft6ayoNfrKegqGlPXqeUan40KVQU4hhO2wVJoXWQDzeMtMd/f8luVu5OByAjt6DOz6WUUrWhSaGi0pLCoZObprMqIzqFO9//kZjO7JVJ9H/sB7ZqO4NSqhHQpFCRj6On0PyHYd3sOj98ZJAP2586i2BfT575bgt3f7IGwFlqUEqphqRJoTIxQ+zrtu9dcnh3N2FMt4hy6zYkN/0pr5VSTZ8mhcpcOhPCukC264ameHRSbxbePZrHJ/Wic2QAa5M0KSilGl5NB8RrWfzDoM0A2OO6SXKCfT0J9vUkNtyf1Ox8Xl6QQEJKNp0jm/7U10qppktLClUJ7QiZe2B/3YycWp2rTonF19Od059fxKzlicffQSmlXESTQlVCHV1TXx8B6btdeqrwAG9ev9IOEPvBUteeSymlqqNJoSrthoC7Yxa1pW9AUUGdD5RX1sguETxwVnfWJGWy82AOKVl5FBbrw21KqfqlSaEqoR3hwRTodxksfR1eGQKvnQI5aS475bn92iACY55byJCnfuSlH7e57FxKKVUZTQrHM/FpCIiEdMfDbKmuG7coOtiX1oE+zuUvVu/FmLqfaE4ppaqiSeF4fIKhz8VHl1NcO5jdvy7qw9l9onni/N4kHjrChuRqp61QSqk6pUmhJkb8BbqfY9+nbnbpqcZ0i+SVywdydp9o3N2EK99eyvRF2116TqWUKqVJoSb8w2HqB/ZJZxeXFEqF+HvROSKA9NxCnpq7mZRsnbFNKeV6Lk0KIjJBRLaISIKI3FfNdpNFxIhIvCvjOWkdhtsH2jL31svp/jq+q/P9kCd/5Ju1+/hp8wE279cqJaWUa7gsKYiIO/AKMBHoCVwqIj0r2S4QOz/zUlfFUmfirwNTAsum18vpJvSOYs3DZziX3/51B9f9dwUTpv1SL+dXSrU8riwpDAESjDE7jDEFwExgUiXbPQ48AzT++pGQWOh9oX1uIXUL/P4f2DbfpacM9vXkqQv6MLxjGKv2ZDjXF5dorySlVN1zZVJoC5QdsyHJsc5JRAYA7YwxX1d3IBG5SURWiMiK1NTUuo/0RIx9EDy84LUR8P0/4dPrXH7Ky4a254Up/cut6/TAXPZnNv48qpRqWlyZFKSSdc7bWxFxA14A/na8Axljphtj4o0x8REREcfb3LVC4+C2ZbbEAPZJ5xLXP3kcFezD+9cP4fz+bZzrFm1L5eoZy1i83XUP1CmlWhZXJoUkoF2Z5RggucxyINAbWCgiu4BhwJxG39gMEBgFF06HSa9A0RE4VD9dRkd2ieCJC/rQOTIAgIe+XM/PW1N5eI7rB+1TSrUMrkwKy4EuIhInIl7AVGBO6YfGmExjTLgxJtYYEwssAc4zxqxwYUx1q80A+7p3Zb2dMsDbg/l3nUbrIG/yCm0JZc+hXHILiuotBqVU8+WypGCMKQJuB+YBm4BZxpgNIvKYiJznqvPWq4ju4NMKdtV/b6DzB9jmmYfP7UleYQkrd6fzzHebWb7rUL3HopRqPqSpja0THx9vVqxoRIWJj6+EzV/DXzdCUHS9nba4xJBfVEx2XhFDn/qRMH8v0nIKAPjxb6fRKSKg3mJRSjV+IrLSGHPc6nl9ovlkdRxtn114vjskuLZ7alnuboKflweRgd4E+niQllNA77ZBBPp48Mgc1w3xrZRq3nQ6zpPV/3L7uuQ1+OgyGPcQePrC4Ovr5fQi4px34dpT4kjOOMK/f9hKSlYekUE+x9lbKaXK05LCyfL0sQng+u8hpAN8/w/45q56bXwO87eTAY3sGs7wTmEAPDtvC2e9+AsLNqfUWxxKqaZPk0Jd8QuFS2dCj3Pt8owJMK0PHHT9RDn/vXYwT1/Yh8hAH3q1CQbgk5VJbNyXxZ/+t5KElGwAlu08pHNAK6WqpUmhLoV1gin/g6kfQbuhkLEH1n/m8tN2aR3I1CHtAfD1cneun3vHSPy83Hngs/V8tiqJS95YzL2frtUhMpRSVdI2BVfofpb99/pIWPgUFObA+Mfq7fT/u34oOQVF9GwTxJ3juvDIVxtZVqarakLKYbpFBdZbPEqppkNLCq7U6wL7+tuLkJ9db6c9tUs4Z/aKAmDqkPa0CbYNzl4e9td95rRFfLh0DweydOwkpVR5+pyCK5UUw/pP4bMbwTsIblxgG6BD46DdkHoL40hBMT6ebhQWG7r+81vnehFY8/AZ+Hq6k55bQGSg9lZSqrmq6XMKWn3kSm7u0OtCmP8oZCXB/y6w7QwAf1kDadshIBKi+rg0jNJ2Bi8P4ZL4GHan5bJ05yGMgUkv/0aAtwfr9may6bEJ5doklFItjyYFV3P3gD+vtCWE98pMJ/HhFDvfs6c//CO56v3r2DOT+wFQUmK44b0V/FSmy+riHQcZ2711vcWilGp8tE2hPnj6QOwImPohTHoVht5iEwLYRuh9a+o9JDc3YcY1g3nonKOT4b3x8w5+3HSAG95dQV5hcb3HpJRqeNrVvUq5AAAgAElEQVSm0BDyMmHdbOgwAt4+AzAQMxjyMmD0/dBlPBzJAK8AW9JwIWMMCSmH+XDZHt75bZdz/b0TunHr6M6kZOWRlVfkHK5bKdU01bRNQZNCQzuYAL/8GzZ9BQWOHkqBbSA72c4Jfc4Ldl3hEUhcBh1Pc1koN7+/ku827Hcux4X7s/NgDgAJT07Ew10Llko1VTogXlMR3hkueA3u2gCj7rHrsh1tDCtm2B5MAHPvhvfOs43TLnLvhG50jgzg9SsGATgTAsCSHTokt1ItgSaFxsInGE65A069q/z6N8dARiLs+tUup+90WQgdIwKYf9dpTOgdxae3nML71w/h+lPj8PZw481fdlCiT0Ir1exp76PGxCcITn/YzuhWUghZ++DnZ+w4SllJdpu0HdAZKC5yaXvDoA4hgJ0CtH2oHw/P2cDnf+zlokExLjunUqrhaVJojHqWmZgubqSjMdohLQG2fAsfTYXbltvkEd7NpQniquEdePf3XfztkzXsOZTL7WM78/XaZDzd3fjvb7sY1jEMH083bhvTGRFxWRxKKddzaVIQkQnAi4A78JYx5ukKn98M3AYUA4eBm4wxG10ZU5MT3Q/Oehbm/NkuH9oOyX/Y968Mtq+j7oWx/wBj7GPKFVW1voZEhIsGxfDsvC28+OM2PlmRSHLm0SEyVuxOB6BzZCATekfV+jxKqYbnst5HIuIObAXGA0nAcuDSshd9EQkyxmQ53p8H3GqMmVDdcZtd76OaSt0Ki56FdbMq/3zwjbDhcxh9Hwy5sfxnb58BviFw8bv2mYlaKCgq4eetqTwyZwN7M44QHezDvszyYydFBfkwtGMot4zuRPeooFqdRynlGo1hmIshQIIxZocjoJnAJMCZFEoTgoM/oC2ZVYnoamd127/WNkpf+CZ4eNuxleY9AMvftNv99DgMvBo8vOBwCmz+BhKX2s9+m2aTRi14ebgxvmdrBnUIIe1wPgE+Hgz/108AjOoaQZ+2QbyyYDtfrk5m0dZUljwwDm8PHTJDqabGlUmhLVB2RpckYGjFjUTkNuAuwAsYW9mBROQm4CaA9u3b13mgTUardnDrkvJVQQOvgpIiO7nPvrXwydU2QbRqb5NF6VhLQTHw+8t2nodOY2odQqi/F6H+XpQtYb533RAycgt4ZYHtLpueW8jv29MI8fOib9tg3Ny0nUGppsKV1UcXA2caY25wLF8JDDHG/LmK7S9zbH91dcdtsdVHNVFSAi/1O5oI/MIgN82+v32lbZzO3m+fiUhcZqubovrAsFvsPsWFsHcV9JxkSxpgZ47LPQTtj8nnvL9kN/5e7lw40PZISkg5TKi/FwMf/8G5zaT+bbh8aAfCA7z4NeEgl8S3w8dTSxBK1bfGUH2UBLQrsxwDVDfy20zgNRfG0/y5ucGEp+38DT0n2dJDwo+Qvc8+JHfOC/DuOfB0hdJWn4vh9VPt8Btgh9sYciMUFdiB+/Iy4O4Ee/wyrhzW4ehCUT6dQzzA04vz+rXhm3X7cBP4cnUyX64++mv/dt1+PrxxqPZSUqqRcuXDa8uBLiISJyJewFRgTtkNRKRLmcWzAddPaNzcdT8brv8eht9mq5Dir4UxD9jPOoyAsC52ZNaJz8IVnwECb4w6mhDAtlMArHjb9nbKTYOUDUc/zz0ES9+Aonz44WFIWgkzL4f/DIKifJ6Z3JcNj57JonvHcMGAts7drj81jsU70ti4L4tPViTy0JfrnVOD7ss8QuKhXBd/OUqp43FZScEYUyQitwPzsF1SZxhjNojIY8AKY8wc4HYROR0oBNKBaquO1Elyc4ObFoCbB3j62nVnPgXz7gd3b7juW9j+E/z0BHxxK2yZC1F9beP2wqdh8jtwJB1mXgZ7V9gqp8Uv2wbsUotfxicoBjL3ED3ybl6Y0p8hcaF0CPOje1QQ7/6+iwte/Z2CohIAOkUEcPUpsdz1/q/k5BXw5Z/ikUAdvlvVk6J8EDdw9zz5YxU6euPVsocfYG+45t4DfSZD616QcxCi+9v/uyXFdo4WF9MB8RQc2gnFBRDRzf7h/fiorYLyDrKljjl3QNIyiB0JqVsgJ6Xy4/iGQn6Wbfgu1f4U8A6As5+H9F38e2sE2w/mML5na2avTGL1ngzevmYwse8OIkrs8w5rBj7OV26n8+dxXQj2LfOf1RgwJZX/x6juM1VecSH8Og0GXw9+obU/Tv5hKMiB2iTxdbPhj/dtN2nfVrWP4URV/Dt5trMtPV/3bfX7lco5CEV5EFzmyf7iQtvD7/Nb7EOkN/8KKZsg5rjV91ZJCexYAB3HwK/P2x6EZU181v6eFj0HE56CTpX2xzkuHSVV1Z4xtgqpdS+I7GH/aL/5K6z8L3j6wdVfQ8ZumH2t3f7yT2HXImgbD7OutOtC4iofp6nPxbakEhJHVlYmw1aMpFVBCr/73FFus1Pzp5HrF8PwTmG8MDgTr+TlsOId+5/52rkQ3K58L6xProH96+D2FTZ+t2Y0rNfW7+3P0/l0u1xcaL/Diu0yH06F8C5wRpmLijE24Xt42+UV78DqDyBpOfSdCl3PtDcDrXtVfX5jYMGT0Hn80Q4HxtjqwkPb4aFDxybjxa/aecmj+8GvL9gRf/tNsZ+lbIZXHceZ+IztENFlvO1qXZmifFuFGdTm+N9VSYljnyPg5X/s53PugKQV9mbHFB9tX3s44+j3aQwsf8uWIAZfb5e//6f9zn+bZkcxvmujvYEqOGyfH1r88tFzdDgVdv9qb5KG3QKn3Vv+Zyn9XZT9rubdD+e9DD//n/0eht0Kxfnw9V+PbteqPZz3H+g4+vjfQyU0Kai6VVwEOxdCaCc7x7Qx8PWdtqvraY7RXY2BjV/axNH1DNjynf1jP+MJ+8dexWRCuZ4h+BWmH7P+S+9z+S67I695vQhAoXcInvl2uyKPANzDOsClHyN+YfBUtN2pzUDITIRr5tpnO46n9GnvrH22hNOqne2ZdXAb9J0CadtsYqypmjw9nrrV3ml6+R3/WJmJMM0xXeu9OyF9F7x/PpzyZzuqbupWCIiwnQL+3RXEHW5ban9PiUth5Tt2WJT4a2HQtfD2eMhJPfZcg2+A8Y/ZC1/Fi1bicnj7dHuxum+Pvah/NMXGAvbid+ZT0G+qXZe8CmZfd+w53L2h94X2Qcpl0+1raSz9r7BP5bt72arNLEfnCIBZV9m/q+G3w9gHIXGJHR+sNInsXwdZyTbRLXvTljzSd8E/DtiqnNJr3OZv4OPL7fu+U6D3RfDhJXa51wX2guvpD9/e40gK7nDFp7DzZ5vYyrpznW1XK5sMqnLL77badcU7sOEzaDsI+l9mf0cr34GNc2yCKnXZJ/b/D8AXt8Hq/9nf99gHj/3dnABNCqrxMQYyk8DDB5a8aovKAMHt7QXh23tJ6nABsw7GcVfaw+V2fafoTJa3u45Xzwplw+/fkLH+B0a4b6jkJA7u3vbuM2k5XP8DhHQo/7kx8Pmf7Ai0I/8GH0y2F6lrvz16F+vhY6sKrp9vL1Db5tuLh7vH0Yt/7iF7R7p/HXx7r31WZMK/7B1yykZb1M9ItNtEdIPfXoKFT9kL3Gl/t4Mglo1p0XO2qm7SK7DyXVjwxNHPQzvZO/NSI/5ij1f2mU9xhw6n2Pe7fqnZ76XU2AftAIyj/27vhgNb27vemZdBgqObcXR/yNprSytDbrR3yZXxj7AJets8GPtPW2rYs9Re0N08bQll0LX2Irllrr1oVnTvTpsg/i/O3vkjEN3X3lxEdIdT/2rbwNZ+XHkMl35sb2B+fAwS5tu79Db9bYmrsrjHPugoFT1hSzarP3KctxKT3zlaUo7sCf0vh+//cfRzn1a2117ZbuEAAVE2mR3ccnRdWGfIy7LVsnGnwVVfHr2xyEqGVe/bn7W0m3gtaVJQjd8jjju90qJ7xh7wC7d30HuWgncg63/6kE279nJv1mS8Pdz45d6xPDV3E5//kcQCr7uIczvgPNyk/MfwCgjlk6t7wBe3wMGtR8/l08peLL0C7EUtabmtVgF7d1r6vpRfOOQetO87nGp7X5VeuAJa295ak16Bb/9u7/KOpDsmR9pH1Q/mi/3M099Owwr2wmmK4fzXbXvMjDMr3zUoxo6U6xVgk8GCJ4/GUlJsY/UOhqF/gkXP2M/GP2aTRPx1ttQw83J7p3nkkL0zH/OATYTf/8PO3XFMuG72opaTau+sy158S+9m962xMX17L+z6zZYaO5xqE6Cbh73jHnKjTYp5WfBMRzuI49QPbU85sHOE/PAQBESWj8PD9+hF+eJ34bv7HN2ru9qBIY2jqihuFPS5BEJiYd0nsOrdyr/D9sPtcUrPs2Wu/bn2rbHtG6U98HpPhoveslPm7lxk/26y9sGHF0O3s2Hrd0fv7M990VaJevnb0vTjYfa7Hf+Yrf4xJfBiP1sKnfoRdHGUAObdD6veg6vm2Cq59F12oq34649fgqwlTQqq8UvfZS88rY7/lPr6vZlc9NrvBPp4kpVXyNhukbT2zOXz1fu4aFBb2hQn8+Rq+5/pT6M64pebyE1bbrJ3mvmH8e11lq0GyD9sL4SZe8qfYNS99knvHx6GEXfYZzyKC21bxeav7R3yvtV227Autlqp9CJf6qK37YXpx0dt9UbWPtj4ha0+C2htLz7R/Wxby/xHyp/fw9c2Jmbvh4n/Z6t82g2F1j1hzxJb3fTdfbaaZujN9uIXOxKCHV1+j2TY+u2AKHvOgNZ2hN2yigpsvMUF9nxlR9bd9BV8fIV933OSrVrZ8p290z/zX7Y94HCqrVrJTLKlr7LVZMWFtjRwvIbr986HA+vhrk2V9/hJ2w7f3AU7FtrlNgPthfW6ebbk8+s0mPo/W0I7tNPORFj2OHmZtrT1+0t2eeyDNqYBV1beDlMqY4+t4gxsA6PuPraaprjIli7ir4X5j9o2g07j7O+q7Lb5h231adk2rez99vcT2b3CMQvrptdTDWlSUM3O0h1pTJm+BIBZfxpOUXEJl7211Pl5rzZBpOcUlBvBVShBgFUPnUkrX8+jjdBz7rB3lLf8but6T7vX3kFWlJlkq4a6nGnvmPetsXfIGXvgjdNs0rn0I1vnPfr+Y/+TF+Qee+eXvhvem3S0IX7is/ZiC3YY9NuXHRtH7iHbK6X0IucKi1+xHQS6n1X1NiUlgKl9L6/MvTZ5RXSrfruD22x1T1Tv2p1nwxe2Z9SAy2u3fzOkSUE1S7OWJ+Ln7c45fduQnlPAAMeQGmO7R/LM5L7k5Bdx2rMLj9nvjSsHcWavKGb8upP8ohJuHtkBycsE/zDnNoXFJbiL1HyspkM77N3e8S5wVdm/zt5Zdhhu+7ivnQmt+0DMoNodT6lqNIZhLpSqc5cMPjpySoi/F3+f0J2RXcLp3da2T4T5H22MC/HzJD23EIDF29M4s1cUj31tB+ldtjONrLwiLhzYlsuH2kboCdMWMSQulH9d2LdmwYR2PLkfJqrP0feePjDompM7nlJ1QJOCatJuGd2p3LKIEOTjQVZeEVMGt+f1n7cTHuDNe4t3kZZztDF5wRbbFXL93kxO7RyOj6c721Nz2J6aw/n92zK0YxhKtURafaSanZTsPLKOFBEb5seqPRnsSsvh3tlrnZ//8+werNydznn92nDLB6sAuGxoez5cugdfT3diQnyZ2Ceaw3lF3DqmE+EBtiFxR+phVu5O5+L4dpWeV6nGrKbVR83osU+lrMhAHzpHBuDh7saQuFDO6NmaQO+jheKx3SN57YpBTOwTzT1n2vaAD5fuwd1NePbivmxLOcxLP25jxm87GfPcQh77aiNFxSW8unA798xey6GcgqpOzaGcAkpKmtaNllJlaVJQzV4rPy9WP3wGfWNsu0P70KO9gW4b05k7xtonZ3u3Deacvm04p280sWF+vHVVPOEB3sz4bSd//3QdvyfY5xZ+336QLfuzeea7zWTlFTqPtTsth0FP/MCU6YvJyS+isNj2o8/ILSDbsd3Xa5P54o+99fJzK1UbWn2kWozD+UXsz8yjc2RAufV5hcWsScygR5sggnw8McaQX1TinAxo2vytTJtf9ajuFw2M4dnJfZnx206e+GaTc/3wjmFcd2ocN763gnHdI3nr6nji7p8LwObHJ+hkQ6peae8jpSoI8PY4JiEA+Hi6l2tYFpFyF+w7T++Kp7sbz87bQsdwf3YctE8jj+0eSVGJ4dNVSfRv34r3Fu8m2NeTzCOOHk870thyIBuAHzensCbp6JwV3R/8jpFdwgF4dnI/ooJPYrhlpeqQlhSUqqGc/CJEwE2ElbvTGdYxDGMMAx7/gew8O1z4Uxf04YHP15Xb75y+0Xy9dh+ndArj9+1pRAf7sK/MA3YAj57Xi7VJmQzrGKoN2coltKFZqTrm7+2Bn5cHPp7ujOgcjrub4OHuRlSQvcu/+4yuXDb06JAdb14Vzx3juvDkBX3w83Ln9+1pdGsdyOL7xzGoQ0i5Yz88ZwOfrkrintlrmfTyr+xJs7PQZeQWsM1R2lCqPmhSUOok/eX0Lni6C1MG24Tw6S3D+deFfRjfszV3je9KsK8nwx3VU6d0tq8ju4TjJjDvzlE8O7kvXh5uXD3cPkS3JimTP3+0it1pOQx4/AfGv7CI9JwC8gqPDq9sjCm3rFRdcWn1kYhMAF7ETsf5ljHm6Qqf3wXcABQBqcB1xpjd1R1Tq49UY2SMQaqZR+HL1Xv5y8zVvHvdEE7rGkFuQRFb9mczoL0tMeQXFePp5sacNclk5xfx4BfriQj0JjU733kMN7HVU1OH2GcqHvh8Hb/cO4ZZKxJpHeTDZUPa4+YmpGbns3L3IeJjQzmUU0DX1oEu//lV49fgYx+JiDuwFRgPJAHLgUuNMRvLbDMGWGqMyRWRW4DRxpgp1R1Xk4JqqnYdzCE2vJLZwCrx0JfreW/xbib0iiIlO4/N+7OJDPRmV1ouA9q34o89GQDl2ifeuWYwY7pHcvWMZfy8NZVAbw+y84t4+kKbSEp9vTaZMH9vhnUM5d7Za4mPDXGWcqpSUmIQodrEpxq3xtD7aAiQYIzZ4QhoJjAJcCYFY8yCMtsvAa5wYTxKNaiaJgSAB87qga+nOxfHtyMq2Ad3EQpLShj//M/OhAA4E0J4gBfTF+2goLiEn7faITyy823j9wvztzJ5UAw/bDzAvbPXOtdfc0osn6xM4pOVSXSODGB1YiZXDGuPt0f5rrIJKdlMeWMJfzm9C1cNjz2Zr0A1Aa4sKUwGJhhjbnAsXwkMNcbcXsX2LwP7jTFPVPLZTcBNAO3btx+0e3e1NUxKNVtJ6blsSznM9J938I+ze/Cfn7Yxrkdr8gqLeehLOxNd96hArhsRx72fHh3aY0KvKL7bsB+w8038tDmFbSmHKz3HraM7ccGAtkQEeuPj6c7ZL/3C9tQcOob789Pdo8ttm5Kdh5+XBwHe2ru9sWsMJYXKypmVZiARuQKIB06r7HNjzHRgOtjqo7oKUKmmJibEj5gQP8Z0s3M/vHHl0f/j0cG+7E7L4cKBMRQWlyBiL/DvL97tTAjPXdyPyYNiGNE5nGfnbeH2sZ15+tvN7M04ws2ndWL+xgO8unA7ry7cXu68fdoGs3FfFqnZ+by/ZDendg5ncGwIF7zyO4E+Nil0igjA29ONf5zdo1xpY+66fRwpKOaiQTHH/DwV22JKSgy/JBxkVJdwrapqIK5MCklA2Q7XMUByxY1E5HTgH8Bpxpj8ip8rpWpmfM/W5ZZn3jiMnm2CmNg7mtTD+fh7eTA41jZsj+oawaiuEQCc0bO18wnum0Z1dA7DkZVXSOKhIwxs34p2oX5Mnb6ECdMWkZZTwNdrk3lp6gD2Zhydw3jFbjtdaUyILzeN6kTa4XwKiku41THoYGJ6Lqv2ZPD6FQPx8/KgqLiEiS/+wqiuETx4Tk8A51Phr18xiAm9o1z7halKuTIpLAe6iEgcsBeYClxWdgMRGQC8ga1mSnFhLEq1OKVPaZfONVGVsk9wB3h7cMWwDsdsU1BUgpeHm3P48R2pOdxTZuTZUt4ebny8PJHDeUW8/rNt4yhVOlTIrOWJXDMijg+W7mFbymG2pRzm/ond+XlrKm//amejS0rPrTLekhLDV2uTGds9kkCf+pvOsqVwWVIwxhSJyO3APGyX1BnGmA0i8hiwwhgzB3gWCAA+cRQV9xhjznNVTEqp2vHycMPHw42CohIeP783//1tJ5v2ZTGhVxS70nI4tXM47u5CYZFhxm87eemnBFr5eVKQW1LuOF0iA3hv8W56tgl2TngEMPHFX8q1cfy8NZWsvCLO6hNF96igcsf4v+8288aiHdw1vit3jOsC2PGr0nMLWL0ng0EdQogMqnzYEGMMJQbcazq7Xgukw1wopWrktYXb+b/vNrP6ofEE+XhyKLfAOddEqVV70pk6fQl/GdeFCwa0ZXdaLs99v4WVjqqlR8/rxcNzNtA+1I+8wmI+uXk4459fREFxCef2a8OeQ7msScwod8xTO4czJC6UfZlHWLrjkHPsqZFdwgn29cTH052i4hK+WG1rp8d1j+TtawYDcCArjz/2pHNmryhEhPs/W8vcdftZ/dD4cm0WmUcKCfKx98g1acsoKTE1n7a1kWjw5xRcRZOCUg3DGENuQTH+x+lpVFxiyt2JFxWX0Pkf39I5MoAZVw9m1LO2J/rdZ3Tl9rFd2HUwhyU70rhoUAye7m5c8sZilu08xGldI5zdaysqHU+qKv84qweTB8Uw5Kn5FBYbooJ8+MvpXbj/Mzsu1d/Gd+XfP2xl4d2j2ZtxhMvfWgrYdpk3rhhU7oL/0bI9pGbnO0slBw/nE//EfGejfVPRGHofKaWaERE5bkKAY6tmPNzdmHP7CGJC/Aj192JCryiCfT25YaSd4zo23L/cMxyPT+rNwi0pXDsijq/WJJOccYSsvEIm9okmzN+LI4XFpOcUMn/TAfrFtCI2zJ+PVyTy0Dk9yThSyEs/buPJuZv4aXMKhcX2pnd/Vp4zIQD8+4etAHyzbh8JZaqtfth4gH//sIUze0XRN6YVaxIznPtdNbwDm/dnO6u5Xv5pG/5e7kQGeTOoQ+gx30NxiWHGrzu5ZHA7CotLSEo/wovztzJtygCC/WreFmKM4dNVe+kcGUD/dq1qvF9taUlBKdWkFRWX8EdiBvEdQigqMXy2Kon//r6bTfuyiAnx5ZmL+rI99TDTf9lB4qEjzjm8yzqjZ2tuG9OZ//yUwPxNBwCYf9coXl24nc9W2d5YPaKD2LQv65jze7gJC+8ZTXJGHi/+uJVx3Vtz7YhYftl2kKtmLGNS/zZ8ufpox8trTonl6lNiCfb1JNTfi+ISw9drkxnQLoT2YX7HHP+VBQk8O28LrYO8WfrA6bX+nrT6SCnVYu08mMPEFxdxxdAO/NPR3XVDcibv/b6b3m2DeNDxoF/vtkH0aRvM9afG0TkykEM5Bdz2wSoW70hzHuvsvtH8tCmFI2UGIDy7TzTfrNvHhQPb8t36/ZQYQ17h0Ub1yhJPRaUDIm5PzeHm/60EYP2jZxLg7YExBmNABEY/t5DdjlFz594xkp5tgqo7bJU0KSilWrTkjCOE+nsdM8NdSlYeQ576kWlT+nP+gLbH7GeMcc6QB/DxTcP4IzGDp7/dzGldIxjeKYw/jepIdn4RQT6ebDuQzcsLEvD1dOf+iT0Y9/xCDh4+dh7viEBvbh/TGTc3wV3kmHk3AN6+Op4+McHc+r9VHMjO45bTOvPA5+u4+4yuvLwggX+e3bPSLsM1oUlBKaWqULExvKIPlu4mPaeAW0fbi7gxhj8SM+gX0+q43VnXJWXy8Jz1rHKMUfWn0zpySXw7OkWUn/Uv9r5vnO9D/Dw5UljMlPh2bN6fzbJdh/D1dCe3oBgPN+HXv4/F18udYN/aP5ehDc1KKVWF413YLx9a/m5cRBjYPqSKrcvrExPMp7ecwtCnfuTKYR34s6PXUkW92gSxIdm2UVwS346tB7L5cNkeCosNz1zUlzHdI3nmu81cMrhdvU7XqiUFpZRqAPsyj7B5fza92wTTys+T5TsPcdlbS+nWOpBv/zKyzp+D0JKCUko1YtHBvkQH+zqXh3cK467xXRnROaxBH4zTpKCUUo2AiDgfkGtIOkezUkopJ00KSimlnDQpKKWUctKkoJRSykmTglJKKSdNCkoppZw0KSillHLSpKCUUsqpyQ1zISKpwO5a7h4OHKzDcOpKY40LGm9sGteJ0bhOTHOMq4MxJuJ4GzW5pHAyRGRFTcb+qG+NNS5ovLFpXCdG4zoxLTkurT5SSinlpElBKaWUU0tLCtMbOoAqNNa4oPHGpnGdGI3rxLTYuFpUm4JSSqnqtbSSglJKqWpoUlBKKeXUYpKCiEwQkS0ikiAi9zVwLLtEZJ2IrBaRFY51oSLyg4hsc7zWbELYk4tjhoikiMj6MusqjUOslxzf31oRGVjPcT0iInsd39lqETmrzGf3O+LaIiJnujCudiKyQEQ2icgGEfmLY32DfmfVxNWg35mI+IjIMhFZ44jrUcf6OBFZ6vi+PhYRL8d6b8dyguPzWFfEdZzY/isiO8t8Z/0d6+vz799dRP4Qka8dy/X7fRljmv0/wB3YDnQEvIA1QM8GjGcXEF5h3TPAfY739wH/Vw9xjAIGAuuPFwdwFvAtIMAwYGk9x/UIcHcl2/Z0/D69gTjH79ndRXFFAwMd7wOBrY7zN+h3Vk1cDfqdOX7uAMd7T2Cp43uYBUx1rH8duMXx/lbgdcf7qcDHLvwbqyq2/wKTK9m+Pv/+7wI+BL52LNfr99VSSgpDgARjzA5jTAEwE5jUwDFVNAl41/H+XeB8V5/QGLMIOFTDOCYB7xlrCdBKRKLrMa6qTAJmGmPyjTE7gQTs79sVce0zxqxyvM8GNgFtaeDvrJq4qlIv35nj5z7sWPR0/DPAWGC2Y33F76v0e5wNjBMRl0xWXE1sVamX36WIxPH3+00AAATMSURBVABnA285loV6/r5aSlJoCySWWU6i+v80rmaA70VkpYjc5FjX2hizD+x/ciCygWKrKo7G8B3e7ii6zyhTvdYgcTmK6gOwd5iN5jurEBc08HfmqApZDaQAP2BLJRnGmKJKzu2My/F5JhDmirgqi80YU/qdPen4zl4QEe+KsVUSd12aBtwLlDiWw6jn76ulJIXKsmdD9sUdYYwZCEwEbhORUQ0YS0019Hf4GtAJ6A/sA/7tWF/vcYlIAPApcKcxJqu6TStZ57LYKomrwb8zY0yxMaY/EIMtjfSo5tz1+n1VjE1EegP3A92BwUAo8Pf6ik1EzgFSjDEry66u5rwuiamlJIUkoF2Z5Rgg+f/bu5vQOOowjuPfn7TW2kpDoYIotEZ7EKEW2oP4AkVFrAdRiFSsNYjHXrxJ6YvgXW8Fi4hUG0QqDe3ZRAM9SIox1qj1BfFQhPaihQoWaR8P/2cm6zZJQ+jOLOT3gWV3Z2dnn32S2f/OM7PPtBQLEfFHXl8ERikry4VqczSvL7YU3nxxtJrDiLiQK/E14H1myx2NxiVpJeWDdyQiTuTk1nM2V1z9krOM5S/gS0o9fkDSijleu44rH1/H4suINyO2Z7IUFxFxBfiQZnP2KPCcpN8pJe4nKFsOjeZruQwKZ4DNuRf/VspOmVNtBCJpjaQ7qtvA08BMxjOcsw0DJ9uIb4E4TgGv5lEYDwOXqpJJE7rqty9QclbF9VIeiXEvsBmY7FEMAj4AfoyIdzseajVn88XVds4kbZA0kLdXA09R9nd8AQzlbN35qvI4BIxH7kVtKLZzHYO7KLX7zpz19G8ZEfsi4p6I2ET5jBqPiN00na+btce83y+Uowd+ptQ097cYxyDlyI9vge+rWCi1wDHgl7xe30Asn1DKCv9SvnW8Pl8clE3Vw5m/74DtDcf1cb7u2VwZ7uqYf3/G9ROws4dxPUbZPD8LTOfl2bZztkBcreYM2AJ8k68/AxzqWAcmKTu4jwOrcvptef/XfHywh3/L+WIbz5zNAMeYPUKpsf//fL0dzB591Gi+3ObCzMxqy6V8ZGZmi+BBwczMah4UzMys5kHBzMxqHhTMzKzmQcGsQZJ2VN0vzfqRBwUzM6t5UDCbg6RXst/+tKQj2TztsqR3JE1JGpO0IefdKumrbKI2qtnzKdwv6XOVnv1Tku7Lxa+V9Jmkc5JGetUJ1GwpPCiYdZH0ALCL0rhwK3AV2A2sAaaiNDOcAN7Kp3wEvBkRWyi/dq2mjwCHI+Ih4BHKr7ShdDF9g3Jeg0FKzxuzvrDixrOYLTtPAtuAM/klfjWlyd014NOc5xhwQtI6YCAiJnL6UeB49re6OyJGASLiH4Bc3mREnM/708Am4HTv35bZjXlQMLuegKMRse9/E6WDXfMt1CNmoZLQlY7bV/F6aH3E5SOz640BQ5LuhPoczBsp60vVrfJl4HREXAL+lPR4Tt8DTEQ5n8F5Sc/nMlZJur3Rd2G2BP6GYtYlIn6QdIBydrxbKN1a9wJ/Aw9K+ppylqtd+ZRh4L380P8NeC2n7wGOSHo7l/Fig2/DbEncJdVskSRdjoi1bcdh1ksuH5mZWc1bCmZmVvOWgpmZ1TwomJlZzYOCmZnVPCiYmVnNg4KZmdX+AwcoXdK7R/FJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff6f6e0c208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accurac : 91.9621%\n"
     ]
    }
   ],
   "source": [
    "y_predictions = [np.argmax(model.predict(np.expand_dims(example,axis=0))) for example in X_test.reshape(X_test.shape[0],20,40,1)]\n",
    "\n",
    "accuracy =100*np.sum(np.array(y_predictions)==np.argmax( to_categorical(y_test),axis=1))/len(y_predictions)\n",
    "\n",
    "print ('test accurac : %.4f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test (path):\n",
    "    #path=r\"C:\\Users\\abdal_000\\Downloads\\data-set\\test\\audio\\clip_0a77ea19a.wav\"\n",
    "    mfcc_feature=convert_wave_to_mfcc(path)\n",
    "    #print(mfcc_feature.shape)\n",
    "    \n",
    "    pre=model.predict(mfcc_feature.reshape(1,20,40,1))\n",
    "    return np.argmax(pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "audfiles=os.listdir(r\"C:\\Users\\abdal_000\\Downloads\\data-set\\test\\audio\")\n",
    "print (audfiles[0])\n",
    "with open('sample_submission.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames=['fname','label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    for audio in audfiles:\n",
    "        pred = test(r\"C:\\Users\\abdal_000\\Downloads\\data-set\\test\\audio\\\\\"+audio)\n",
    "        writer.writerow({'fname':audio,'label':labels[pred]})\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
