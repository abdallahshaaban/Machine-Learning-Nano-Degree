<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>speech recognition </title>

<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script>
(function() {
  function addWidgetsRenderer() {
    var mimeElement = document.querySelector('script[type="application/vnd.jupyter.widget-view+json"]');
    var scriptElement = document.createElement('script');
    var widgetRendererSrc = '@jupyter-widgets/html-manager@*/dist/embed-amd.js';
    var widgetState;

    // Fallback for older version:
    try {
      widgetState = mimeElement && JSON.parse(mimeElement.innerHTML);

      if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) {
        widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';
      }
    } catch(e) {}

    scriptElement.src = widgetRendererSrc;
    document.body.appendChild(scriptElement);
  }

  document.addEventListener('DOMContentLoaded', addWidgetsRenderer);
}());
</script>

<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.7.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.7.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.7.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff2?v=4.7.0') format('woff2'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.7.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.7.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.7.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.fa-pull-left {
  float: left;
}
.fa-pull-right {
  float: right;
}
.fa.fa-pull-left {
  margin-right: .3em;
}
.fa.fa-pull-right {
  margin-left: .3em;
}
/* Deprecated as of 4.4.0 */
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
.fa-pulse {
  -webkit-animation: fa-spin 1s infinite steps(8);
  animation: fa-spin 1s infinite steps(8);
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=1)";
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2)";
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=3)";
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1)";
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1)";
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook-f:before,
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-feed:before,
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before,
.fa-gratipay:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper-pp:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-resistance:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-y-combinator-square:before,
.fa-yc-square:before,
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
.fa-buysellads:before {
  content: "\f20d";
}
.fa-connectdevelop:before {
  content: "\f20e";
}
.fa-dashcube:before {
  content: "\f210";
}
.fa-forumbee:before {
  content: "\f211";
}
.fa-leanpub:before {
  content: "\f212";
}
.fa-sellsy:before {
  content: "\f213";
}
.fa-shirtsinbulk:before {
  content: "\f214";
}
.fa-simplybuilt:before {
  content: "\f215";
}
.fa-skyatlas:before {
  content: "\f216";
}
.fa-cart-plus:before {
  content: "\f217";
}
.fa-cart-arrow-down:before {
  content: "\f218";
}
.fa-diamond:before {
  content: "\f219";
}
.fa-ship:before {
  content: "\f21a";
}
.fa-user-secret:before {
  content: "\f21b";
}
.fa-motorcycle:before {
  content: "\f21c";
}
.fa-street-view:before {
  content: "\f21d";
}
.fa-heartbeat:before {
  content: "\f21e";
}
.fa-venus:before {
  content: "\f221";
}
.fa-mars:before {
  content: "\f222";
}
.fa-mercury:before {
  content: "\f223";
}
.fa-intersex:before,
.fa-transgender:before {
  content: "\f224";
}
.fa-transgender-alt:before {
  content: "\f225";
}
.fa-venus-double:before {
  content: "\f226";
}
.fa-mars-double:before {
  content: "\f227";
}
.fa-venus-mars:before {
  content: "\f228";
}
.fa-mars-stroke:before {
  content: "\f229";
}
.fa-mars-stroke-v:before {
  content: "\f22a";
}
.fa-mars-stroke-h:before {
  content: "\f22b";
}
.fa-neuter:before {
  content: "\f22c";
}
.fa-genderless:before {
  content: "\f22d";
}
.fa-facebook-official:before {
  content: "\f230";
}
.fa-pinterest-p:before {
  content: "\f231";
}
.fa-whatsapp:before {
  content: "\f232";
}
.fa-server:before {
  content: "\f233";
}
.fa-user-plus:before {
  content: "\f234";
}
.fa-user-times:before {
  content: "\f235";
}
.fa-hotel:before,
.fa-bed:before {
  content: "\f236";
}
.fa-viacoin:before {
  content: "\f237";
}
.fa-train:before {
  content: "\f238";
}
.fa-subway:before {
  content: "\f239";
}
.fa-medium:before {
  content: "\f23a";
}
.fa-yc:before,
.fa-y-combinator:before {
  content: "\f23b";
}
.fa-optin-monster:before {
  content: "\f23c";
}
.fa-opencart:before {
  content: "\f23d";
}
.fa-expeditedssl:before {
  content: "\f23e";
}
.fa-battery-4:before,
.fa-battery:before,
.fa-battery-full:before {
  content: "\f240";
}
.fa-battery-3:before,
.fa-battery-three-quarters:before {
  content: "\f241";
}
.fa-battery-2:before,
.fa-battery-half:before {
  content: "\f242";
}
.fa-battery-1:before,
.fa-battery-quarter:before {
  content: "\f243";
}
.fa-battery-0:before,
.fa-battery-empty:before {
  content: "\f244";
}
.fa-mouse-pointer:before {
  content: "\f245";
}
.fa-i-cursor:before {
  content: "\f246";
}
.fa-object-group:before {
  content: "\f247";
}
.fa-object-ungroup:before {
  content: "\f248";
}
.fa-sticky-note:before {
  content: "\f249";
}
.fa-sticky-note-o:before {
  content: "\f24a";
}
.fa-cc-jcb:before {
  content: "\f24b";
}
.fa-cc-diners-club:before {
  content: "\f24c";
}
.fa-clone:before {
  content: "\f24d";
}
.fa-balance-scale:before {
  content: "\f24e";
}
.fa-hourglass-o:before {
  content: "\f250";
}
.fa-hourglass-1:before,
.fa-hourglass-start:before {
  content: "\f251";
}
.fa-hourglass-2:before,
.fa-hourglass-half:before {
  content: "\f252";
}
.fa-hourglass-3:before,
.fa-hourglass-end:before {
  content: "\f253";
}
.fa-hourglass:before {
  content: "\f254";
}
.fa-hand-grab-o:before,
.fa-hand-rock-o:before {
  content: "\f255";
}
.fa-hand-stop-o:before,
.fa-hand-paper-o:before {
  content: "\f256";
}
.fa-hand-scissors-o:before {
  content: "\f257";
}
.fa-hand-lizard-o:before {
  content: "\f258";
}
.fa-hand-spock-o:before {
  content: "\f259";
}
.fa-hand-pointer-o:before {
  content: "\f25a";
}
.fa-hand-peace-o:before {
  content: "\f25b";
}
.fa-trademark:before {
  content: "\f25c";
}
.fa-registered:before {
  content: "\f25d";
}
.fa-creative-commons:before {
  content: "\f25e";
}
.fa-gg:before {
  content: "\f260";
}
.fa-gg-circle:before {
  content: "\f261";
}
.fa-tripadvisor:before {
  content: "\f262";
}
.fa-odnoklassniki:before {
  content: "\f263";
}
.fa-odnoklassniki-square:before {
  content: "\f264";
}
.fa-get-pocket:before {
  content: "\f265";
}
.fa-wikipedia-w:before {
  content: "\f266";
}
.fa-safari:before {
  content: "\f267";
}
.fa-chrome:before {
  content: "\f268";
}
.fa-firefox:before {
  content: "\f269";
}
.fa-opera:before {
  content: "\f26a";
}
.fa-internet-explorer:before {
  content: "\f26b";
}
.fa-tv:before,
.fa-television:before {
  content: "\f26c";
}
.fa-contao:before {
  content: "\f26d";
}
.fa-500px:before {
  content: "\f26e";
}
.fa-amazon:before {
  content: "\f270";
}
.fa-calendar-plus-o:before {
  content: "\f271";
}
.fa-calendar-minus-o:before {
  content: "\f272";
}
.fa-calendar-times-o:before {
  content: "\f273";
}
.fa-calendar-check-o:before {
  content: "\f274";
}
.fa-industry:before {
  content: "\f275";
}
.fa-map-pin:before {
  content: "\f276";
}
.fa-map-signs:before {
  content: "\f277";
}
.fa-map-o:before {
  content: "\f278";
}
.fa-map:before {
  content: "\f279";
}
.fa-commenting:before {
  content: "\f27a";
}
.fa-commenting-o:before {
  content: "\f27b";
}
.fa-houzz:before {
  content: "\f27c";
}
.fa-vimeo:before {
  content: "\f27d";
}
.fa-black-tie:before {
  content: "\f27e";
}
.fa-fonticons:before {
  content: "\f280";
}
.fa-reddit-alien:before {
  content: "\f281";
}
.fa-edge:before {
  content: "\f282";
}
.fa-credit-card-alt:before {
  content: "\f283";
}
.fa-codiepie:before {
  content: "\f284";
}
.fa-modx:before {
  content: "\f285";
}
.fa-fort-awesome:before {
  content: "\f286";
}
.fa-usb:before {
  content: "\f287";
}
.fa-product-hunt:before {
  content: "\f288";
}
.fa-mixcloud:before {
  content: "\f289";
}
.fa-scribd:before {
  content: "\f28a";
}
.fa-pause-circle:before {
  content: "\f28b";
}
.fa-pause-circle-o:before {
  content: "\f28c";
}
.fa-stop-circle:before {
  content: "\f28d";
}
.fa-stop-circle-o:before {
  content: "\f28e";
}
.fa-shopping-bag:before {
  content: "\f290";
}
.fa-shopping-basket:before {
  content: "\f291";
}
.fa-hashtag:before {
  content: "\f292";
}
.fa-bluetooth:before {
  content: "\f293";
}
.fa-bluetooth-b:before {
  content: "\f294";
}
.fa-percent:before {
  content: "\f295";
}
.fa-gitlab:before {
  content: "\f296";
}
.fa-wpbeginner:before {
  content: "\f297";
}
.fa-wpforms:before {
  content: "\f298";
}
.fa-envira:before {
  content: "\f299";
}
.fa-universal-access:before {
  content: "\f29a";
}
.fa-wheelchair-alt:before {
  content: "\f29b";
}
.fa-question-circle-o:before {
  content: "\f29c";
}
.fa-blind:before {
  content: "\f29d";
}
.fa-audio-description:before {
  content: "\f29e";
}
.fa-volume-control-phone:before {
  content: "\f2a0";
}
.fa-braille:before {
  content: "\f2a1";
}
.fa-assistive-listening-systems:before {
  content: "\f2a2";
}
.fa-asl-interpreting:before,
.fa-american-sign-language-interpreting:before {
  content: "\f2a3";
}
.fa-deafness:before,
.fa-hard-of-hearing:before,
.fa-deaf:before {
  content: "\f2a4";
}
.fa-glide:before {
  content: "\f2a5";
}
.fa-glide-g:before {
  content: "\f2a6";
}
.fa-signing:before,
.fa-sign-language:before {
  content: "\f2a7";
}
.fa-low-vision:before {
  content: "\f2a8";
}
.fa-viadeo:before {
  content: "\f2a9";
}
.fa-viadeo-square:before {
  content: "\f2aa";
}
.fa-snapchat:before {
  content: "\f2ab";
}
.fa-snapchat-ghost:before {
  content: "\f2ac";
}
.fa-snapchat-square:before {
  content: "\f2ad";
}
.fa-pied-piper:before {
  content: "\f2ae";
}
.fa-first-order:before {
  content: "\f2b0";
}
.fa-yoast:before {
  content: "\f2b1";
}
.fa-themeisle:before {
  content: "\f2b2";
}
.fa-google-plus-circle:before,
.fa-google-plus-official:before {
  content: "\f2b3";
}
.fa-fa:before,
.fa-font-awesome:before {
  content: "\f2b4";
}
.fa-handshake-o:before {
  content: "\f2b5";
}
.fa-envelope-open:before {
  content: "\f2b6";
}
.fa-envelope-open-o:before {
  content: "\f2b7";
}
.fa-linode:before {
  content: "\f2b8";
}
.fa-address-book:before {
  content: "\f2b9";
}
.fa-address-book-o:before {
  content: "\f2ba";
}
.fa-vcard:before,
.fa-address-card:before {
  content: "\f2bb";
}
.fa-vcard-o:before,
.fa-address-card-o:before {
  content: "\f2bc";
}
.fa-user-circle:before {
  content: "\f2bd";
}
.fa-user-circle-o:before {
  content: "\f2be";
}
.fa-user-o:before {
  content: "\f2c0";
}
.fa-id-badge:before {
  content: "\f2c1";
}
.fa-drivers-license:before,
.fa-id-card:before {
  content: "\f2c2";
}
.fa-drivers-license-o:before,
.fa-id-card-o:before {
  content: "\f2c3";
}
.fa-quora:before {
  content: "\f2c4";
}
.fa-free-code-camp:before {
  content: "\f2c5";
}
.fa-telegram:before {
  content: "\f2c6";
}
.fa-thermometer-4:before,
.fa-thermometer:before,
.fa-thermometer-full:before {
  content: "\f2c7";
}
.fa-thermometer-3:before,
.fa-thermometer-three-quarters:before {
  content: "\f2c8";
}
.fa-thermometer-2:before,
.fa-thermometer-half:before {
  content: "\f2c9";
}
.fa-thermometer-1:before,
.fa-thermometer-quarter:before {
  content: "\f2ca";
}
.fa-thermometer-0:before,
.fa-thermometer-empty:before {
  content: "\f2cb";
}
.fa-shower:before {
  content: "\f2cc";
}
.fa-bathtub:before,
.fa-s15:before,
.fa-bath:before {
  content: "\f2cd";
}
.fa-podcast:before {
  content: "\f2ce";
}
.fa-window-maximize:before {
  content: "\f2d0";
}
.fa-window-minimize:before {
  content: "\f2d1";
}
.fa-window-restore:before {
  content: "\f2d2";
}
.fa-times-rectangle:before,
.fa-window-close:before {
  content: "\f2d3";
}
.fa-times-rectangle-o:before,
.fa-window-close-o:before {
  content: "\f2d4";
}
.fa-bandcamp:before {
  content: "\f2d5";
}
.fa-grav:before {
  content: "\f2d6";
}
.fa-etsy:before {
  content: "\f2d7";
}
.fa-imdb:before {
  content: "\f2d8";
}
.fa-ravelry:before {
  content: "\f2d9";
}
.fa-eercast:before {
  content: "\f2da";
}
.fa-microchip:before {
  content: "\f2db";
}
.fa-snowflake-o:before {
  content: "\f2dc";
}
.fa-superpowers:before {
  content: "\f2dd";
}
.fa-wpexplorer:before {
  content: "\f2de";
}
.fa-meetup:before {
  content: "\f2e0";
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
div.traceback-wrapper pre.traceback {
  max-height: 600px;
  overflow: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  display: flex;
  flex-direction: row;
  justify-content: space-between;
  padding: 5px;
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
[dir="rtl"] #ipython_notebook {
  margin-right: 10px;
  margin-left: 0;
}
[dir="rtl"] #ipython_notebook.pull-left {
  float: right !important;
  float: right;
}
.flex-spacer {
  flex: 1;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#kernel_logo_widget {
  margin: 0 10px;
}
span#login_widget {
  float: right;
}
[dir="rtl"] span#login_widget {
  float: left;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
.modal-header {
  cursor: move;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
[dir="rtl"] .center-nav form.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] .center-nav .navbar-text {
  float: right;
}
[dir="rtl"] .navbar-inner {
  text-align: right;
}
[dir="rtl"] div.text-left {
  text-align: right;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  position: absolute;
  display: block;
  width: 100%;
  height: 100%;
  overflow: hidden;
  cursor: pointer;
  opacity: 0;
  z-index: 2;
}
.alternate_upload .btn-xs > input.fileinput {
  margin: -1px -5px;
}
.alternate_upload .btn-upload {
  position: relative;
  height: 22px;
}
::-webkit-file-upload-button {
  cursor: pointer;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
ul#tabs {
  margin-bottom: 4px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
[dir="rtl"] ul#tabs.nav-tabs > li {
  float: right;
}
[dir="rtl"] ul#tabs.nav.nav-tabs {
  padding-right: 0;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons .pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .list_toolbar .col-sm-4,
[dir="rtl"] .list_toolbar .col-sm-8 {
  float: right;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: text-bottom;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
[dir="rtl"] .list_item > div input {
  margin-right: 0;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_modified {
  margin-right: 7px;
  margin-left: 7px;
}
[dir="rtl"] .item_modified.pull-right {
  float: left !important;
  float: left;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
[dir="rtl"] .item_buttons.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .item_buttons .kernel-name {
  margin-left: 7px;
  float: right;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
.sort_button {
  display: inline-block;
  padding-left: 7px;
}
[dir="rtl"] .sort_button.pull-right {
  float: left !important;
  float: left;
}
#tree-selector {
  padding-right: 0px;
}
#button-select-all {
  min-width: 50px;
}
[dir="rtl"] #button-select-all.btn {
  float: right ;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
  margin-top: 2px;
  height: 16px;
}
[dir="rtl"] #select-all.pull-left {
  float: right !important;
  float: right;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.fa-pull-left {
  margin-right: .3em;
}
.folder_icon:before.fa-pull-right {
  margin-left: .3em;
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.fa-pull-left {
  margin-right: .3em;
}
.file_icon:before.fa-pull-right {
  margin-left: .3em;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
#new-menu .dropdown-header {
  font-size: 10px;
  border-bottom: 1px solid #e5e5e5;
  padding: 0 0 3px;
  margin: -3px 20px 0;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.move-button {
  display: none;
}
.download-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
.CodeMirror-dialog {
  background-color: #fff;
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph > img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}
.rendered_html ul {
  list-style: disc;
}
.rendered_html ul ul {
  list-style: square;
  margin-top: 0;
}
.rendered_html ul ul ul {
  list-style: circle;
}
.rendered_html ol {
  list-style: decimal;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin-top: 0;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
  padding: 0px;
  background-color: #fff;
}
.rendered_html code {
  background-color: #eff0f1;
}
.rendered_html p code {
  padding: 1px 5px;
}
.rendered_html pre code {
  background-color: #fff;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  color: #000;
  font-size: 100%;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
.rendered_html .alert {
  margin-bottom: initial;
}
.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] .rendered_html p {
  text-align: right;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered .rendered_html td {
  max-width: none;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
.jupyter-keybindings {
  padding: 1px;
  line-height: 24px;
  border-bottom: 1px solid gray;
}
.jupyter-keybindings input {
  margin: 0;
  padding: 0;
  border: none;
}
.jupyter-keybindings i {
  padding: 6px;
}
.well code {
  background-color: #ffffff;
  border-color: #ababab;
  border-width: 1px;
  border-style: solid;
  padding: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.tags_button_container {
  width: 100%;
  display: flex;
}
.tag-container {
  display: flex;
  flex-direction: row;
  flex-grow: 1;
  overflow: hidden;
  position: relative;
}
.tag-container > * {
  margin: 0 4px;
}
.remove-tag-btn {
  margin-left: 4px;
}
.tags-input {
  display: flex;
}
.cell-tag:last-child:after {
  content: "";
  position: absolute;
  right: 0;
  width: 40px;
  height: 100%;
  /* Fade to background color of cell toolbar */
  background: linear-gradient(to right, rgba(0, 0, 0, 0), #EEE);
}
.tags-input > * {
  margin-left: 4px;
}
.cell-tag,
.tags-input input,
.tags-input button {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  box-shadow: none;
  width: inherit;
  font-size: inherit;
  height: 22px;
  line-height: 22px;
  padding: 0px 4px;
  display: inline-block;
}
.cell-tag:focus,
.tags-input input:focus,
.tags-input button:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.cell-tag::-moz-placeholder,
.tags-input input::-moz-placeholder,
.tags-input button::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.cell-tag:-ms-input-placeholder,
.tags-input input:-ms-input-placeholder,
.tags-input button:-ms-input-placeholder {
  color: #999;
}
.cell-tag::-webkit-input-placeholder,
.tags-input input::-webkit-input-placeholder,
.tags-input button::-webkit-input-placeholder {
  color: #999;
}
.cell-tag::-ms-expand,
.tags-input input::-ms-expand,
.tags-input button::-ms-expand {
  border: 0;
  background-color: transparent;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
.cell-tag[readonly],
.tags-input input[readonly],
.tags-input button[readonly],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  background-color: #eeeeee;
  opacity: 1;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  cursor: not-allowed;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button {
  height: auto;
}
select.cell-tag,
select.tags-input input,
select.tags-input button {
  height: 30px;
  line-height: 30px;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button,
select[multiple].cell-tag,
select[multiple].tags-input input,
select[multiple].tags-input button {
  height: auto;
}
.cell-tag,
.tags-input button {
  padding: 0px 4px;
}
.cell-tag {
  background-color: #fff;
  white-space: nowrap;
}
.tags-input input[type=text]:focus {
  outline: none;
  box-shadow: none;
  border-color: #ccc;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
[dir="rtl"] #kernel_logo_widget {
  float: left !important;
  float: left;
}
.modal .modal-body .move-path {
  display: flex;
  flex-direction: row;
  justify-content: space;
  align-items: center;
}
.modal .modal-body .move-path .server-root {
  padding-right: 20px;
}
.modal .modal-body .move-path .path-input {
  flex: 1;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
[dir="rtl"] #menubar .navbar-toggle {
  float: right;
}
[dir="rtl"] #menubar .navbar-collapse {
  clear: right;
}
[dir="rtl"] #menubar .navbar-nav {
  float: right;
}
[dir="rtl"] #menubar .nav {
  padding-right: 0px;
}
[dir="rtl"] #menubar .navbar-nav > li {
  float: right;
}
[dir="rtl"] #menubar .navbar-right {
  float: left !important;
}
[dir="rtl"] ul.dropdown-menu {
  text-align: right;
  left: auto;
}
[dir="rtl"] ul#new-menu.dropdown-menu {
  right: auto;
  left: 0;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
[dir="rtl"] i.menu-icon.pull-right {
  float: left !important;
  float: left;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
[dir="rtl"] ul#help_menu li a {
  padding-left: 2.2em;
}
[dir="rtl"] ul#help_menu li a i {
  margin-right: 0;
  margin-left: -1.2em;
}
[dir="rtl"] ul#help_menu li a i.pull-right {
  float: left !important;
  float: left;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
[dir="rtl"] .dropdown-submenu > .dropdown-menu {
  right: 100%;
  margin-right: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.fa-pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.fa-pull-right {
  margin-left: .3em;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
[dir="rtl"] .dropdown-submenu > a:after {
  float: left;
  content: "\f0d9";
  margin-right: 0;
  margin-left: -10px;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
[dir="rtl"] #notification_area {
  float: left !important;
  float: left;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] .indicator_area {
  float: left !important;
  float: left;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
[dir="rtl"] #kernel_indicator {
  float: left !important;
  float: left;
  border-left: 0;
  border-right: 1px solid;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] #modal_indicator {
  float: left !important;
  float: left;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  height: 30px;
  margin-top: 4px;
  display: flex;
  justify-content: flex-start;
  align-items: baseline;
  width: 50%;
  flex: 1;
}
span.save_widget span.filename {
  height: 100%;
  line-height: 1em;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
[dir="rtl"] span.save_widget.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] span.save_widget span.filename {
  margin-left: 0;
  margin-right: 16px;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
  white-space: nowrap;
  padding: 0 5px;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
    padding: 0 0 0 5px;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
.toolbar-btn-label {
  margin-left: 6px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
[dir="rtl"] .btn-group > .btn,
.btn-group-vertical > .btn {
  float: right;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
[dir="rtl"] ul.typeahead-list i {
  margin-left: 0;
  margin-right: -10px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
ul.typeahead-list  > li > a.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .typeahead-list {
  text-align: right;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  min-width: 20px;
  color: transparent;
}
[dir="rtl"] .no-shortcut.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .command-shortcut.pull-right {
  float: left !important;
  float: left;
}
.command-shortcut:before {
  content: "(command mode)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
[dir="rtl"] .edit-shortcut.pull-right {
  float: left !important;
  float: left;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
[dir="ltr"] #find-and-replace .input-group-btn + .form-control {
  border-left: none;
}
[dir="rtl"] #find-and-replace .input-group-btn + .form-control {
  border-right: none;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Import-Necessary-Python-Libraries">Import Necessary Python Libraries<a class="anchor-link" href="#Import-Necessary-Python-Libraries">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">librosa</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="k">import</span> <span class="n">signal</span>
<span class="kn">from</span> <span class="nn">scipy.io</span> <span class="k">import</span> <span class="n">wavfile</span>
<span class="kn">import</span> <span class="nn">wave</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="k">import</span> <span class="n">Sequential</span> 
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span>  <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">MaxPooling2D</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="k">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="k">import</span> <span class="n">ModelCheckpoint</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">StratifiedKFold</span>
<span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;down&#39;</span><span class="p">,</span> <span class="s1">&#39;go&#39;</span><span class="p">,</span> <span class="s1">&#39;unknown&#39;</span><span class="p">,</span> <span class="s1">&#39;up&#39;</span><span class="p">,</span> <span class="s1">&#39;on&#39;</span><span class="p">,</span> <span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span> <span class="s1">&#39;off&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="s1">&#39;silence&#39;</span><span class="p">,</span> <span class="s1">&#39;stop&#39;</span><span class="p">,</span> <span class="s1">&#39;yes&#39;</span><span class="p">]</span>
<span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;audio&quot;</span>
<span class="n">max_pad_length</span><span class="o">=</span><span class="mi">40</span>
<span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Audio-visualizations">Audio visualizations<a class="anchor-link" href="#Audio-visualizations">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">spf</span> <span class="o">=</span> <span class="n">wave</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;left/00176480_nohash_0.wav&#39;</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>

<span class="c1">#Extract Raw Audio from Wav File</span>
<span class="n">signal</span> <span class="o">=</span> <span class="n">spf</span><span class="o">.</span><span class="n">readframes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">signal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromstring</span><span class="p">(</span><span class="n">signal</span><span class="p">,</span> <span class="s1">&#39;Int16&#39;</span><span class="p">)</span>
<span class="n">fs</span> <span class="o">=</span> <span class="n">spf</span><span class="o">.</span><span class="n">getframerate</span><span class="p">()</span>

<span class="c1">#If Stereo</span>
<span class="k">if</span> <span class="n">spf</span><span class="o">.</span><span class="n">getnchannels</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Just mono files&#39;</span><span class="p">)</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>


<span class="n">Time</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">signal</span><span class="p">)</span><span class="o">/</span><span class="n">fs</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">signal</span><span class="p">))</span>

<span class="n">p</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Signal Wave of left</span><span class="se">\\</span><span class="s1">00176480_nohash_0.wav&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Time</span><span class="p">,</span><span class="n">signal</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">p</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;signal.png&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">samplingFrequency</span><span class="p">,</span> <span class="n">signalData</span> <span class="o">=</span> <span class="n">wavfile</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="s1">&#39;left/00176480_nohash_0.wav&#39;</span><span class="p">)</span>

<span class="c1"># Plot the signal read from wav file</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Spectrogram of left</span><span class="se">\\</span><span class="s1">00176480_nohash_0.wav&#39;</span><span class="p">)</span>

<span class="n">p1</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">specgram</span><span class="p">(</span><span class="n">signalData</span><span class="p">,</span><span class="n">Fs</span><span class="o">=</span><span class="n">samplingFrequency</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">p1</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;signal1.png&#39;</span><span class="p">)</span>

<span class="n">spf</span> <span class="o">=</span> <span class="n">wave</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;happy/0f250098_nohash_0.wav&#39;</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>

<span class="c1">#Extract Raw Audio from Wav File</span>
<span class="n">signal</span> <span class="o">=</span> <span class="n">spf</span><span class="o">.</span><span class="n">readframes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">signal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromstring</span><span class="p">(</span><span class="n">signal</span><span class="p">,</span> <span class="s1">&#39;Int16&#39;</span><span class="p">)</span>
<span class="n">fs</span> <span class="o">=</span> <span class="n">spf</span><span class="o">.</span><span class="n">getframerate</span><span class="p">()</span>

<span class="c1">#If Stereo</span>
<span class="k">if</span> <span class="n">spf</span><span class="o">.</span><span class="n">getnchannels</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Just mono files&#39;</span><span class="p">)</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>


<span class="n">Time</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">signal</span><span class="p">)</span><span class="o">/</span><span class="n">fs</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">signal</span><span class="p">))</span>

<span class="n">p</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Signal Wave of happy</span><span class="se">\\</span><span class="s1">0f250098_nohash_0.wav&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Time</span><span class="p">,</span><span class="n">signal</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">p</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;signal2.png&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Numeric-style type codes are deprecated and will result in an error in the future.
  
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: Numeric-style type codes are deprecated and will result in an error in the future.
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Process-audio-files">Process audio files<a class="anchor-link" href="#Process-audio-files">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">test_wavfile</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s2">&quot;audio/go/0d53e045_nohash_1.wav&quot;</span><span class="p">):</span>
    
    <span class="n">sample_rate</span><span class="p">,</span> <span class="n">samples</span> <span class="o">=</span> <span class="n">wavfile</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">frequencies</span><span class="p">,</span> <span class="n">times</span><span class="p">,</span> <span class="n">spectrogram</span> <span class="o">=</span> <span class="n">signal</span><span class="o">.</span><span class="n">spectrogram</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">sample_rate</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">times</span><span class="p">,</span> <span class="n">frequencies</span><span class="p">,</span> <span class="n">spectrogram</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">spectrogram</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Frequecy [HZ]&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time [sec]&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">convert_wave_to_mfcc</span><span class="p">(</span><span class="n">filePath</span><span class="p">):</span>
    <span class="n">wav</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span><span class="n">librosa</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filePath</span><span class="p">,</span> <span class="n">mono</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">mfcc</span> <span class="o">=</span> <span class="n">librosa</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">mfcc</span><span class="p">(</span><span class="n">wav</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">)</span>
    <span class="c1">##padding</span>
    <span class="n">mfcc_feature</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">mfcc</span><span class="p">,</span> <span class="n">pad_width</span><span class="o">=</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_pad_length</span> <span class="o">-</span> <span class="n">mfcc</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mfcc_feature</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_labels</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">file_path</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">save_mfcc_files</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">file_path</span><span class="p">):</span>
    <span class="c1">#labels = get_labels(path)</span>
    <span class="c1">#print(labels)</span>
    <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">:</span>
        <span class="n">mfcc_features</span><span class="o">=</span><span class="p">[]</span>
        <span class="n">audfiles</span><span class="o">=</span><span class="p">[</span><span class="n">path</span><span class="o">+</span><span class="s1">&#39;/&#39;</span><span class="o">+</span><span class="n">label</span><span class="o">+</span><span class="s1">&#39;/&#39;</span><span class="o">+</span><span class="n">aud</span> <span class="k">for</span> <span class="n">aud</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">path</span><span class="o">+</span><span class="s1">&#39;/&#39;</span><span class="o">+</span><span class="n">label</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">audio</span> <span class="ow">in</span> <span class="n">audfiles</span><span class="p">:</span>
            <span class="n">mfcc_features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">convert_wave_to_mfcc</span><span class="p">(</span><span class="n">audio</span><span class="p">))</span>
        <span class="c1">#print(len(mfcc_vectors))</span>
        <span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">label</span><span class="o">+</span><span class="s1">&#39;.npy&#39;</span><span class="p">,</span> <span class="n">mfcc_features</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#get_labels()</span>
<span class="c1">#print(len(get_labels()))</span>
<span class="c1">#save_mfcc_files()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Generate--Training-&amp;-Testing-sets">Generate  Training &amp; Testing sets<a class="anchor-link" href="#Generate--Training-&amp;-Testing-sets">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">generate_training_testing_set</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">file_path</span><span class="p">,</span><span class="n">test_size</span><span class="o">=.</span><span class="mi">3</span><span class="p">):</span>
    <span class="c1">#labels = get_labels(path)</span>
    <span class="c1">#features=[]</span>
    <span class="n">features</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;.npy&#39;</span><span class="p">)</span>
    <span class="n">classes</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1">#for i,label in zip(range(0,len(labels)-1),labels):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)):</span>
        <span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;.npy&#39;</span><span class="p">)</span>
        <span class="n">features</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">features</span><span class="p">,</span><span class="n">x</span><span class="p">))</span>
        <span class="n">classes</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">classes</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">fill_value</span><span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span> <span class="n">test_size</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">generate_training_testing_set1</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">file_path</span><span class="p">,</span><span class="n">test_size</span><span class="o">=.</span><span class="mi">3</span><span class="p">):</span>
    <span class="c1">#labels=get_labels(path)</span>
    <span class="n">features</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;.npy&#39;</span><span class="p">)</span>
    <span class="n">classes</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">classes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">feature_train</span><span class="p">,</span> <span class="n">feature_test</span><span class="p">,</span> <span class="n">labels_train</span><span class="p">,</span> <span class="n">labels_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span>
                                        <span class="n">test_size</span><span class="o">=</span> <span class="n">test_size</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)):</span>
        <span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;.npy&#39;</span><span class="p">)</span>
        <span class="c1">#features=np.vstack((features,x))</span>
        <span class="n">y</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">fill_value</span><span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="c1">#print(labels[i])</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="o">=</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                    <span class="n">test_size</span><span class="o">=</span> <span class="n">test_size</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
     
        
        <span class="n">feature_train</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">feature_train</span><span class="p">,</span><span class="n">X_train</span><span class="p">))</span>
        <span class="n">feature_test</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">feature_test</span><span class="p">,</span><span class="n">X_test</span><span class="p">))</span>
       
        <span class="n">labels_train</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">labels_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
        <span class="n">labels_test</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">labels_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">feature_train</span><span class="p">,</span> <span class="n">feature_test</span><span class="p">,</span> <span class="n">labels_train</span><span class="p">,</span> <span class="n">labels_test</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Initial-Model-Architecture">Initial Model Architecture<a class="anchor-link" href="#Initial-Model-Architecture">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">generate_training_testing_set</span><span class="p">()</span>

<span class="n">intial_model</span> <span class="o">=</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">intial_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">intial_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
<span class="n">intial_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="o">.</span><span class="mi">25</span><span class="p">))</span>
<span class="n">intial_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">))</span>
<span class="n">intial_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
<span class="n">intial_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="o">.</span><span class="mi">25</span><span class="p">))</span>
<span class="n">intial_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">intial_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">))</span>
<span class="n">intial_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="o">.</span><span class="mi">2</span><span class="p">))</span>
<span class="n">intial_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
<span class="n">intial_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">categorical_crossentropy</span><span class="p">,</span><span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">rmsprop</span><span class="p">(),</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Initial-Model-Training">Initial Model Training<a class="anchor-link" href="#Initial-Model-Training">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">checkpointer</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span><span class="o">=</span><span class="s1">&#39;initial_weights.hdf5&#39;</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">history</span><span class="o">=</span><span class="n">intial_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">20</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">),</span>
          <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpointer</span><span class="p">]</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Train on 13776 samples, validate on 5904 samples
Epoch 1/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 2.3038 - acc: 0.2601Epoch 00001: val_loss improved from inf to 1.81328, saving model to initial_weights.hdf5
13776/13776 [==============================] - 2s 129us/step - loss: 2.2901 - acc: 0.2634 - val_loss: 1.8133 - val_acc: 0.4090
Epoch 2/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.7394 - acc: 0.4195Epoch 00002: val_loss improved from 1.81328 to 1.59190, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 96us/step - loss: 1.7379 - acc: 0.4196 - val_loss: 1.5919 - val_acc: 0.4782
Epoch 3/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.5567 - acc: 0.4805Epoch 00003: val_loss improved from 1.59190 to 1.51252, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 96us/step - loss: 1.5583 - acc: 0.4803 - val_loss: 1.5125 - val_acc: 0.5151
Epoch 4/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.4252 - acc: 0.5240Epoch 00004: val_loss improved from 1.51252 to 1.32893, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 96us/step - loss: 1.4237 - acc: 0.5238 - val_loss: 1.3289 - val_acc: 0.5708
Epoch 5/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.3529 - acc: 0.5494Epoch 00005: val_loss did not improve
13776/13776 [==============================] - 1s 94us/step - loss: 1.3532 - acc: 0.5488 - val_loss: 1.3499 - val_acc: 0.5544
Epoch 6/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.2622 - acc: 0.5766Epoch 00006: val_loss improved from 1.32893 to 1.30564, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 95us/step - loss: 1.2643 - acc: 0.5760 - val_loss: 1.3056 - val_acc: 0.5818
Epoch 7/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.1961 - acc: 0.6019Epoch 00007: val_loss improved from 1.30564 to 1.18408, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 95us/step - loss: 1.1945 - acc: 0.6022 - val_loss: 1.1841 - val_acc: 0.6114
Epoch 8/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.1246 - acc: 0.6275Epoch 00008: val_loss improved from 1.18408 to 1.05593, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 94us/step - loss: 1.1272 - acc: 0.6271 - val_loss: 1.0559 - val_acc: 0.6553
Epoch 9/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.0449 - acc: 0.6530Epoch 00009: val_loss improved from 1.05593 to 1.03135, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 94us/step - loss: 1.0449 - acc: 0.6529 - val_loss: 1.0314 - val_acc: 0.6604
Epoch 10/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.0094 - acc: 0.6627Epoch 00010: val_loss improved from 1.03135 to 0.98197, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 95us/step - loss: 1.0070 - acc: 0.6634 - val_loss: 0.9820 - val_acc: 0.6736
Epoch 11/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.9669 - acc: 0.6747Epoch 00011: val_loss improved from 0.98197 to 0.94655, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 95us/step - loss: 0.9658 - acc: 0.6755 - val_loss: 0.9465 - val_acc: 0.6904
Epoch 12/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.9153 - acc: 0.6958Epoch 00012: val_loss improved from 0.94655 to 0.94089, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 95us/step - loss: 0.9128 - acc: 0.6961 - val_loss: 0.9409 - val_acc: 0.6875
Epoch 13/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.8759 - acc: 0.7107Epoch 00013: val_loss improved from 0.94089 to 0.89251, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 95us/step - loss: 0.8765 - acc: 0.7103 - val_loss: 0.8925 - val_acc: 0.7053
Epoch 14/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.8323 - acc: 0.7213Epoch 00014: val_loss improved from 0.89251 to 0.84618, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 95us/step - loss: 0.8323 - acc: 0.7214 - val_loss: 0.8462 - val_acc: 0.7210
Epoch 15/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.7909 - acc: 0.7345Epoch 00015: val_loss improved from 0.84618 to 0.80648, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 95us/step - loss: 0.7909 - acc: 0.7353 - val_loss: 0.8065 - val_acc: 0.7373
Epoch 16/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.7801 - acc: 0.7393Epoch 00016: val_loss did not improve
13776/13776 [==============================] - 1s 93us/step - loss: 0.7820 - acc: 0.7379 - val_loss: 0.8173 - val_acc: 0.7312
Epoch 17/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.7490 - acc: 0.7519Epoch 00017: val_loss improved from 0.80648 to 0.76176, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 97us/step - loss: 0.7508 - acc: 0.7515 - val_loss: 0.7618 - val_acc: 0.7519
Epoch 18/50
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.7220 - acc: 0.7571Epoch 00018: val_loss did not improve
13776/13776 [==============================] - 1s 100us/step - loss: 0.7229 - acc: 0.7562 - val_loss: 0.7711 - val_acc: 0.7485
Epoch 19/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.6927 - acc: 0.7667Epoch 00019: val_loss improved from 0.76176 to 0.74670, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 100us/step - loss: 0.6938 - acc: 0.7666 - val_loss: 0.7467 - val_acc: 0.7564
Epoch 20/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.6757 - acc: 0.7705Epoch 00020: val_loss did not improve
13776/13776 [==============================] - 1s 96us/step - loss: 0.6761 - acc: 0.7708 - val_loss: 0.8157 - val_acc: 0.7314
Epoch 21/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.6592 - acc: 0.7728Epoch 00021: val_loss did not improve
13776/13776 [==============================] - 1s 97us/step - loss: 0.6599 - acc: 0.7723 - val_loss: 0.7528 - val_acc: 0.7481
Epoch 22/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.6377 - acc: 0.7860Epoch 00022: val_loss did not improve
13776/13776 [==============================] - 1s 97us/step - loss: 0.6401 - acc: 0.7847 - val_loss: 0.7551 - val_acc: 0.7583
Epoch 23/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.6164 - acc: 0.7899Epoch 00023: val_loss improved from 0.74670 to 0.70362, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 99us/step - loss: 0.6147 - acc: 0.7909 - val_loss: 0.7036 - val_acc: 0.7639
Epoch 24/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5897 - acc: 0.8031Epoch 00024: val_loss did not improve
13776/13776 [==============================] - 1s 97us/step - loss: 0.5892 - acc: 0.8029 - val_loss: 0.7190 - val_acc: 0.7669
Epoch 25/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5751 - acc: 0.8066Epoch 00025: val_loss did not improve
13776/13776 [==============================] - 1s 96us/step - loss: 0.5783 - acc: 0.8053 - val_loss: 0.7194 - val_acc: 0.7625
Epoch 26/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5601 - acc: 0.8124Epoch 00026: val_loss improved from 0.70362 to 0.70060, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 96us/step - loss: 0.5618 - acc: 0.8117 - val_loss: 0.7006 - val_acc: 0.7695
Epoch 27/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5474 - acc: 0.8158Epoch 00027: val_loss improved from 0.70060 to 0.69903, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 96us/step - loss: 0.5472 - acc: 0.8157 - val_loss: 0.6990 - val_acc: 0.7739
Epoch 28/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5262 - acc: 0.8226Epoch 00028: val_loss improved from 0.69903 to 0.69114, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 96us/step - loss: 0.5270 - acc: 0.8222 - val_loss: 0.6911 - val_acc: 0.7718
Epoch 29/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5173 - acc: 0.8259Epoch 00029: val_loss did not improve
13776/13776 [==============================] - 1s 91us/step - loss: 0.5171 - acc: 0.8266 - val_loss: 0.7387 - val_acc: 0.7575
Epoch 30/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5036 - acc: 0.8286Epoch 00030: val_loss improved from 0.69114 to 0.66622, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 92us/step - loss: 0.5016 - acc: 0.8297 - val_loss: 0.6662 - val_acc: 0.7859
Epoch 31/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4891 - acc: 0.8332Epoch 00031: val_loss improved from 0.66622 to 0.66140, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 93us/step - loss: 0.4899 - acc: 0.8329 - val_loss: 0.6614 - val_acc: 0.7866
Epoch 32/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4745 - acc: 0.8396Epoch 00032: val_loss did not improve
13776/13776 [==============================] - 1s 90us/step - loss: 0.4768 - acc: 0.8383 - val_loss: 0.6937 - val_acc: 0.7734
Epoch 33/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4678 - acc: 0.8419Epoch 00033: val_loss did not improve
13776/13776 [==============================] - 1s 90us/step - loss: 0.4664 - acc: 0.8425 - val_loss: 0.7177 - val_acc: 0.7671
Epoch 34/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4541 - acc: 0.8462Epoch 00034: val_loss did not improve
13776/13776 [==============================] - 1s 91us/step - loss: 0.4543 - acc: 0.8460 - val_loss: 0.6804 - val_acc: 0.7818
Epoch 35/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4415 - acc: 0.8516Epoch 00035: val_loss did not improve
13776/13776 [==============================] - 1s 91us/step - loss: 0.4435 - acc: 0.8507 - val_loss: 0.6749 - val_acc: 0.7781
Epoch 36/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4375 - acc: 0.8499Epoch 00036: val_loss improved from 0.66140 to 0.64127, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 92us/step - loss: 0.4376 - acc: 0.8496 - val_loss: 0.6413 - val_acc: 0.7954
Epoch 37/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4245 - acc: 0.8570Epoch 00037: val_loss improved from 0.64127 to 0.62661, saving model to initial_weights.hdf5
13776/13776 [==============================] - 1s 92us/step - loss: 0.4235 - acc: 0.8567 - val_loss: 0.6266 - val_acc: 0.7989
Epoch 38/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4216 - acc: 0.8575Epoch 00038: val_loss did not improve
13776/13776 [==============================] - 1s 90us/step - loss: 0.4243 - acc: 0.8563 - val_loss: 0.6777 - val_acc: 0.7874
Epoch 39/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4147 - acc: 0.8594Epoch 00039: val_loss did not improve
13776/13776 [==============================] - 1s 90us/step - loss: 0.4141 - acc: 0.8594 - val_loss: 0.6803 - val_acc: 0.7840
Epoch 40/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4111 - acc: 0.8560Epoch 00040: val_loss did not improve
13776/13776 [==============================] - 1s 90us/step - loss: 0.4098 - acc: 0.8569 - val_loss: 0.6425 - val_acc: 0.7935
Epoch 41/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3892 - acc: 0.8698Epoch 00041: val_loss did not improve
13776/13776 [==============================] - 1s 90us/step - loss: 0.3937 - acc: 0.8685 - val_loss: 0.6540 - val_acc: 0.7923
Epoch 42/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3906 - acc: 0.8670Epoch 00042: val_loss did not improve
13776/13776 [==============================] - 1s 91us/step - loss: 0.3908 - acc: 0.8665 - val_loss: 0.6467 - val_acc: 0.7939
Epoch 43/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3825 - acc: 0.8704Epoch 00043: val_loss did not improve
13776/13776 [==============================] - 1s 91us/step - loss: 0.3835 - acc: 0.8697 - val_loss: 0.6476 - val_acc: 0.7952
Epoch 44/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3767 - acc: 0.8705Epoch 00044: val_loss did not improve
13776/13776 [==============================] - 1s 90us/step - loss: 0.3778 - acc: 0.8709 - val_loss: 0.6653 - val_acc: 0.7873
Epoch 45/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3599 - acc: 0.8758Epoch 00045: val_loss did not improve
13776/13776 [==============================] - 1s 90us/step - loss: 0.3608 - acc: 0.8756 - val_loss: 0.6401 - val_acc: 0.7983
Epoch 46/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3617 - acc: 0.8741Epoch 00046: val_loss did not improve
13776/13776 [==============================] - 1s 90us/step - loss: 0.3623 - acc: 0.8741 - val_loss: 0.6489 - val_acc: 0.7949
Epoch 47/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3570 - acc: 0.8776Epoch 00047: val_loss did not improve
13776/13776 [==============================] - 1s 89us/step - loss: 0.3563 - acc: 0.8781 - val_loss: 0.6612 - val_acc: 0.7959
Epoch 48/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3482 - acc: 0.8820Epoch 00048: val_loss did not improve
13776/13776 [==============================] - 1s 90us/step - loss: 0.3469 - acc: 0.8822 - val_loss: 0.6613 - val_acc: 0.7947
Epoch 49/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3507 - acc: 0.8781Epoch 00049: val_loss did not improve
13776/13776 [==============================] - 1s 90us/step - loss: 0.3509 - acc: 0.8780 - val_loss: 0.6419 - val_acc: 0.8006
Epoch 50/50
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3503 - acc: 0.8791Epoch 00050: val_loss did not improve
13776/13776 [==============================] - 1s 90us/step - loss: 0.3496 - acc: 0.8791 - val_loss: 0.6643 - val_acc: 0.7925
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Test-the-model">Test the model<a class="anchor-link" href="#Test-the-model">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[28]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">intial_model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">&#39;initial_weights.hdf5&#39;</span><span class="p">)</span>
<span class="n">y_predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">intial_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">example</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span> <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">20</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">1</span><span class="p">)]</span>

<span class="n">accuracy</span> <span class="o">=</span><span class="mi">100</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_predictions</span><span class="p">)</span><span class="o">==</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y_predictions</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;test accurac : </span><span class="si">%.4f%%</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>test accurac : 80.1541%
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Final-Model-Architecture">Final Model Architecture<a class="anchor-link" href="#Final-Model-Architecture">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">generate_training_testing_set</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">_model</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="o">.</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="o">.</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="o">.</span><span class="mi">25</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="o">.</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">categorical_crossentropy</span><span class="p">,</span><span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">adadelta</span><span class="p">(),</span>
                  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">=</span><span class="n">_model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_31 (Conv2D)           (None, 19, 39, 32)        160       
_________________________________________________________________
max_pooling2d_31 (MaxPooling (None, 9, 19, 32)         0         
_________________________________________________________________
dropout_41 (Dropout)         (None, 9, 19, 32)         0         
_________________________________________________________________
conv2d_32 (Conv2D)           (None, 8, 18, 64)         8256      
_________________________________________________________________
max_pooling2d_32 (MaxPooling (None, 4, 9, 64)          0         
_________________________________________________________________
dropout_42 (Dropout)         (None, 4, 9, 64)          0         
_________________________________________________________________
conv2d_33 (Conv2D)           (None, 3, 8, 128)         32896     
_________________________________________________________________
max_pooling2d_33 (MaxPooling (None, 1, 4, 128)         0         
_________________________________________________________________
dropout_43 (Dropout)         (None, 1, 4, 128)         0         
_________________________________________________________________
flatten_11 (Flatten)         (None, 512)               0         
_________________________________________________________________
dense_21 (Dense)             (None, 128)               65664     
_________________________________________________________________
dropout_44 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_22 (Dense)             (None, 12)                1548      
=================================================================
Total params: 108,524
Trainable params: 108,524
Non-trainable params: 0
_________________________________________________________________
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span> <span class="p">(</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>19680 19680
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Final-Model-Training">Final Model Training<a class="anchor-link" href="#Final-Model-Training">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">checkpointer</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span><span class="o">=</span><span class="s1">&#39;weights.hdf5&#39;</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">history</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">),</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> 
                  <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=.</span><span class="mi">3</span><span class="p">,</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpointer</span><span class="p">]</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Train on 13776 samples, validate on 5904 samples
Epoch 1/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 2.4243 - acc: 0.1095Epoch 00001: val_loss improved from inf to 2.41974, saving model to weights.hdf5
13776/13776 [==============================] - 2s 120us/step - loss: 2.4236 - acc: 0.1095 - val_loss: 2.4197 - val_acc: 0.1128
Epoch 2/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 2.4245 - acc: 0.1087Epoch 00002: val_loss improved from 2.41974 to 2.41965, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 2.4238 - acc: 0.1095 - val_loss: 2.4196 - val_acc: 0.1130
Epoch 3/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 2.4248 - acc: 0.1076Epoch 00003: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 2.4246 - acc: 0.1078 - val_loss: 2.4197 - val_acc: 0.1130
Epoch 4/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 2.4238 - acc: 0.1079Epoch 00004: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 2.4237 - acc: 0.1081 - val_loss: 2.4197 - val_acc: 0.1130
Epoch 5/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 2.4228 - acc: 0.1078Epoch 00005: val_loss improved from 2.41965 to 2.41964, saving model to weights.hdf5
13776/13776 [==============================] - 1s 107us/step - loss: 2.4233 - acc: 0.1076 - val_loss: 2.4196 - val_acc: 0.1130
Epoch 6/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 2.4251 - acc: 0.1073Epoch 00006: val_loss did not improve
13776/13776 [==============================] - 1s 105us/step - loss: 2.4240 - acc: 0.1082 - val_loss: 2.4196 - val_acc: 0.1130
Epoch 7/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 2.4228 - acc: 0.1085Epoch 00007: val_loss improved from 2.41964 to 2.41947, saving model to weights.hdf5
13776/13776 [==============================] - 1s 106us/step - loss: 2.4236 - acc: 0.1082 - val_loss: 2.4195 - val_acc: 0.1130
Epoch 8/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 2.4216 - acc: 0.1097Epoch 00008: val_loss improved from 2.41947 to 2.40271, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 2.4218 - acc: 0.1095 - val_loss: 2.4027 - val_acc: 0.1194
Epoch 9/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 2.3921 - acc: 0.1332Epoch 00009: val_loss improved from 2.40271 to 2.33751, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 2.3912 - acc: 0.1333 - val_loss: 2.3375 - val_acc: 0.1636
Epoch 10/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 2.3345 - acc: 0.1653Epoch 00010: val_loss improved from 2.33751 to 2.30133, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 2.3328 - acc: 0.1663 - val_loss: 2.3013 - val_acc: 0.2195
Epoch 11/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 2.2504 - acc: 0.2034Epoch 00011: val_loss improved from 2.30133 to 2.13749, saving model to weights.hdf5
13776/13776 [==============================] - 1s 103us/step - loss: 2.2485 - acc: 0.2041 - val_loss: 2.1375 - val_acc: 0.2586
Epoch 12/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 2.1360 - acc: 0.2385Epoch 00012: val_loss improved from 2.13749 to 1.94613, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 2.1363 - acc: 0.2395 - val_loss: 1.9461 - val_acc: 0.3318
Epoch 13/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 2.0114 - acc: 0.2794Epoch 00013: val_loss improved from 1.94613 to 1.74867, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 2.0070 - acc: 0.2805 - val_loss: 1.7487 - val_acc: 0.4001
Epoch 14/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.8759 - acc: 0.3236Epoch 00014: val_loss improved from 1.74867 to 1.61123, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 1.8737 - acc: 0.3242 - val_loss: 1.6112 - val_acc: 0.4565
Epoch 15/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.7746 - acc: 0.3664Epoch 00015: val_loss improved from 1.61123 to 1.51651, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 1.7737 - acc: 0.3672 - val_loss: 1.5165 - val_acc: 0.4949
Epoch 16/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.6517 - acc: 0.4102Epoch 00016: val_loss improved from 1.51651 to 1.36460, saving model to weights.hdf5
13776/13776 [==============================] - 1s 103us/step - loss: 1.6495 - acc: 0.4117 - val_loss: 1.3646 - val_acc: 0.5452
Epoch 17/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.5565 - acc: 0.4445Epoch 00017: val_loss improved from 1.36460 to 1.30007, saving model to weights.hdf5
13776/13776 [==============================] - 1s 103us/step - loss: 1.5545 - acc: 0.4456 - val_loss: 1.3001 - val_acc: 0.5683
Epoch 18/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.4797 - acc: 0.4783Epoch 00018: val_loss improved from 1.30007 to 1.18515, saving model to weights.hdf5
13776/13776 [==============================] - 1s 103us/step - loss: 1.4746 - acc: 0.4795 - val_loss: 1.1852 - val_acc: 0.6123
Epoch 19/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.3958 - acc: 0.5002Epoch 00019: val_loss improved from 1.18515 to 1.11225, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 1.3963 - acc: 0.5001 - val_loss: 1.1122 - val_acc: 0.6453
Epoch 20/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.3256 - acc: 0.5281Epoch 00020: val_loss improved from 1.11225 to 1.02974, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 1.3233 - acc: 0.5288 - val_loss: 1.0297 - val_acc: 0.6668
Epoch 21/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.2775 - acc: 0.5466Epoch 00021: val_loss improved from 1.02974 to 0.95923, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 1.2731 - acc: 0.5470 - val_loss: 0.9592 - val_acc: 0.6867
Epoch 22/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.2075 - acc: 0.5704Epoch 00022: val_loss improved from 0.95923 to 0.88259, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 1.2055 - acc: 0.5703 - val_loss: 0.8826 - val_acc: 0.7180
Epoch 23/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.1487 - acc: 0.5981Epoch 00023: val_loss improved from 0.88259 to 0.85409, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 1.1503 - acc: 0.5984 - val_loss: 0.8541 - val_acc: 0.7309
Epoch 24/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 1.1145 - acc: 0.6091Epoch 00024: val_loss improved from 0.85409 to 0.80323, saving model to weights.hdf5
13776/13776 [==============================] - 1s 107us/step - loss: 1.1135 - acc: 0.6098 - val_loss: 0.8032 - val_acc: 0.7431
Epoch 25/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 1.0601 - acc: 0.6279Epoch 00025: val_loss improved from 0.80323 to 0.77651, saving model to weights.hdf5
13776/13776 [==============================] - 1s 107us/step - loss: 1.0608 - acc: 0.6272 - val_loss: 0.7765 - val_acc: 0.7527
Epoch 26/400
13568/13776 [============================&gt;.] - ETA: 0s - loss: 1.0316 - acc: 0.6464Epoch 00026: val_loss improved from 0.77651 to 0.72911, saving model to weights.hdf5
13776/13776 [==============================] - 1s 107us/step - loss: 1.0308 - acc: 0.6465 - val_loss: 0.7291 - val_acc: 0.7634
Epoch 27/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.9886 - acc: 0.6539Epoch 00027: val_loss improved from 0.72911 to 0.72710, saving model to weights.hdf5
13776/13776 [==============================] - 1s 107us/step - loss: 0.9896 - acc: 0.6531 - val_loss: 0.7271 - val_acc: 0.7658
Epoch 28/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.9574 - acc: 0.6720Epoch 00028: val_loss improved from 0.72710 to 0.67650, saving model to weights.hdf5
13776/13776 [==============================] - 1s 107us/step - loss: 0.9579 - acc: 0.6723 - val_loss: 0.6765 - val_acc: 0.7761
Epoch 29/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.9177 - acc: 0.6825Epoch 00029: val_loss improved from 0.67650 to 0.67393, saving model to weights.hdf5
13776/13776 [==============================] - 1s 108us/step - loss: 0.9166 - acc: 0.6829 - val_loss: 0.6739 - val_acc: 0.7781
Epoch 30/400
13568/13776 [============================&gt;.] - ETA: 0s - loss: 0.8971 - acc: 0.6904Epoch 00030: val_loss improved from 0.67393 to 0.63477, saving model to weights.hdf5
13776/13776 [==============================] - 2s 112us/step - loss: 0.8971 - acc: 0.6910 - val_loss: 0.6348 - val_acc: 0.7932
Epoch 31/400
13568/13776 [============================&gt;.] - ETA: 0s - loss: 0.8764 - acc: 0.6944Epoch 00031: val_loss improved from 0.63477 to 0.62461, saving model to weights.hdf5
13776/13776 [==============================] - 2s 109us/step - loss: 0.8755 - acc: 0.6946 - val_loss: 0.6246 - val_acc: 0.7984
Epoch 32/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.8511 - acc: 0.7043Epoch 00032: val_loss improved from 0.62461 to 0.60110, saving model to weights.hdf5
13776/13776 [==============================] - 1s 107us/step - loss: 0.8505 - acc: 0.7054 - val_loss: 0.6011 - val_acc: 0.8008
Epoch 33/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.8359 - acc: 0.7095Epoch 00033: val_loss improved from 0.60110 to 0.59428, saving model to weights.hdf5
13776/13776 [==============================] - 1s 106us/step - loss: 0.8352 - acc: 0.7099 - val_loss: 0.5943 - val_acc: 0.8071
Epoch 34/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.8188 - acc: 0.7221Epoch 00034: val_loss improved from 0.59428 to 0.57800, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.8171 - acc: 0.7221 - val_loss: 0.5780 - val_acc: 0.8101
Epoch 35/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.8027 - acc: 0.7251Epoch 00035: val_loss improved from 0.57800 to 0.56165, saving model to weights.hdf5
13776/13776 [==============================] - 1s 106us/step - loss: 0.8033 - acc: 0.7245 - val_loss: 0.5617 - val_acc: 0.8145
Epoch 36/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.7734 - acc: 0.7361Epoch 00036: val_loss improved from 0.56165 to 0.54962, saving model to weights.hdf5
13776/13776 [==============================] - 1s 106us/step - loss: 0.7713 - acc: 0.7366 - val_loss: 0.5496 - val_acc: 0.8186
Epoch 37/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.7479 - acc: 0.7423Epoch 00037: val_loss improved from 0.54962 to 0.53094, saving model to weights.hdf5
13776/13776 [==============================] - 1s 107us/step - loss: 0.7485 - acc: 0.7424 - val_loss: 0.5309 - val_acc: 0.8277
Epoch 38/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.7513 - acc: 0.7448Epoch 00038: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.7531 - acc: 0.7438 - val_loss: 0.5319 - val_acc: 0.8213
Epoch 39/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.7213 - acc: 0.7548Epoch 00039: val_loss improved from 0.53094 to 0.51209, saving model to weights.hdf5
13776/13776 [==============================] - 1s 106us/step - loss: 0.7227 - acc: 0.7538 - val_loss: 0.5121 - val_acc: 0.8325
Epoch 40/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.7200 - acc: 0.7518Epoch 00040: val_loss improved from 0.51209 to 0.50625, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.7201 - acc: 0.7519 - val_loss: 0.5063 - val_acc: 0.8311
Epoch 41/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.6967 - acc: 0.7578Epoch 00041: val_loss improved from 0.50625 to 0.48674, saving model to weights.hdf5
13776/13776 [==============================] - 1s 103us/step - loss: 0.6937 - acc: 0.7587 - val_loss: 0.4867 - val_acc: 0.8382
Epoch 42/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.6872 - acc: 0.7608Epoch 00042: val_loss improved from 0.48674 to 0.47941, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 0.6881 - acc: 0.7609 - val_loss: 0.4794 - val_acc: 0.8460
Epoch 43/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.6822 - acc: 0.7724Epoch 00043: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.6793 - acc: 0.7729 - val_loss: 0.4826 - val_acc: 0.8372
Epoch 44/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.6685 - acc: 0.7709Epoch 00044: val_loss improved from 0.47941 to 0.47040, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.6674 - acc: 0.7713 - val_loss: 0.4704 - val_acc: 0.8455
Epoch 45/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.6516 - acc: 0.7789Epoch 00045: val_loss improved from 0.47040 to 0.45932, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 0.6538 - acc: 0.7776 - val_loss: 0.4593 - val_acc: 0.8535
Epoch 46/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.6425 - acc: 0.7840Epoch 00046: val_loss improved from 0.45932 to 0.45193, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.6434 - acc: 0.7832 - val_loss: 0.4519 - val_acc: 0.8548
Epoch 47/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.6319 - acc: 0.7857Epoch 00047: val_loss improved from 0.45193 to 0.44256, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.6311 - acc: 0.7863 - val_loss: 0.4426 - val_acc: 0.8569
Epoch 48/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.6276 - acc: 0.7862Epoch 00048: val_loss improved from 0.44256 to 0.43504, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.6246 - acc: 0.7872 - val_loss: 0.4350 - val_acc: 0.8579
Epoch 49/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.6221 - acc: 0.7888Epoch 00049: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.6221 - acc: 0.7888 - val_loss: 0.4360 - val_acc: 0.8586
Epoch 50/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.6002 - acc: 0.7948Epoch 00050: val_loss improved from 0.43504 to 0.42668, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.5989 - acc: 0.7951 - val_loss: 0.4267 - val_acc: 0.8598
Epoch 51/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5960 - acc: 0.7976Epoch 00051: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.5966 - acc: 0.7969 - val_loss: 0.4395 - val_acc: 0.8513
Epoch 52/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5866 - acc: 0.7992Epoch 00052: val_loss improved from 0.42668 to 0.41547, saving model to weights.hdf5
13776/13776 [==============================] - 1s 106us/step - loss: 0.5863 - acc: 0.7993 - val_loss: 0.4155 - val_acc: 0.8594
Epoch 53/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5787 - acc: 0.8027Epoch 00053: val_loss improved from 0.41547 to 0.41301, saving model to weights.hdf5
13776/13776 [==============================] - 1s 106us/step - loss: 0.5802 - acc: 0.8022 - val_loss: 0.4130 - val_acc: 0.8665
Epoch 54/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5746 - acc: 0.8042Epoch 00054: val_loss improved from 0.41301 to 0.41002, saving model to weights.hdf5
13776/13776 [==============================] - 1s 106us/step - loss: 0.5754 - acc: 0.8039 - val_loss: 0.4100 - val_acc: 0.8657
Epoch 55/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5688 - acc: 0.8060Epoch 00055: val_loss improved from 0.41002 to 0.40251, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.5704 - acc: 0.8052 - val_loss: 0.4025 - val_acc: 0.8721
Epoch 56/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5548 - acc: 0.8144Epoch 00056: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.5558 - acc: 0.8140 - val_loss: 0.4033 - val_acc: 0.8674
Epoch 57/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5660 - acc: 0.8065Epoch 00057: val_loss improved from 0.40251 to 0.39441, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.5685 - acc: 0.8055 - val_loss: 0.3944 - val_acc: 0.8725
Epoch 58/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5511 - acc: 0.8152Epoch 00058: val_loss improved from 0.39441 to 0.38739, saving model to weights.hdf5
13776/13776 [==============================] - 1s 103us/step - loss: 0.5503 - acc: 0.8157 - val_loss: 0.3874 - val_acc: 0.8740
Epoch 59/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5403 - acc: 0.8181Epoch 00059: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.5431 - acc: 0.8170 - val_loss: 0.3955 - val_acc: 0.8713
Epoch 60/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5332 - acc: 0.8194Epoch 00060: val_loss improved from 0.38739 to 0.38600, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.5312 - acc: 0.8201 - val_loss: 0.3860 - val_acc: 0.8750
Epoch 61/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5280 - acc: 0.8172Epoch 00061: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.5290 - acc: 0.8174 - val_loss: 0.3887 - val_acc: 0.8718
Epoch 62/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5326 - acc: 0.8230Epoch 00062: val_loss improved from 0.38600 to 0.38373, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.5348 - acc: 0.8223 - val_loss: 0.3837 - val_acc: 0.8733
Epoch 63/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5172 - acc: 0.8198Epoch 00063: val_loss improved from 0.38373 to 0.37206, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.5144 - acc: 0.8206 - val_loss: 0.3721 - val_acc: 0.8784
Epoch 64/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5110 - acc: 0.8271Epoch 00064: val_loss improved from 0.37206 to 0.36856, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.5127 - acc: 0.8261 - val_loss: 0.3686 - val_acc: 0.8808
Epoch 65/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.5016 - acc: 0.8302Epoch 00065: val_loss improved from 0.36856 to 0.36140, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.5017 - acc: 0.8301 - val_loss: 0.3614 - val_acc: 0.8801
Epoch 66/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4976 - acc: 0.8359Epoch 00066: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.5003 - acc: 0.8356 - val_loss: 0.3678 - val_acc: 0.8819
Epoch 67/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4965 - acc: 0.8327Epoch 00067: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.4981 - acc: 0.8312 - val_loss: 0.3630 - val_acc: 0.8847
Epoch 68/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4872 - acc: 0.8327Epoch 00068: val_loss improved from 0.36140 to 0.35782, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.4887 - acc: 0.8318 - val_loss: 0.3578 - val_acc: 0.8863
Epoch 69/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4912 - acc: 0.8318Epoch 00069: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.4934 - acc: 0.8303 - val_loss: 0.3591 - val_acc: 0.8838
Epoch 70/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4916 - acc: 0.8356Epoch 00070: val_loss improved from 0.35782 to 0.35608, saving model to weights.hdf5
13776/13776 [==============================] - 1s 106us/step - loss: 0.4888 - acc: 0.8364 - val_loss: 0.3561 - val_acc: 0.8833
Epoch 71/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4757 - acc: 0.8405Epoch 00071: val_loss improved from 0.35608 to 0.35503, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.4747 - acc: 0.8407 - val_loss: 0.3550 - val_acc: 0.8831
Epoch 72/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4720 - acc: 0.8402Epoch 00072: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.4722 - acc: 0.8402 - val_loss: 0.3556 - val_acc: 0.8865
Epoch 73/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4733 - acc: 0.8380Epoch 00073: val_loss improved from 0.35503 to 0.34968, saving model to weights.hdf5
13776/13776 [==============================] - 1s 106us/step - loss: 0.4749 - acc: 0.8377 - val_loss: 0.3497 - val_acc: 0.8880
Epoch 74/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4745 - acc: 0.8394Epoch 00074: val_loss improved from 0.34968 to 0.33988, saving model to weights.hdf5
13776/13776 [==============================] - 1s 106us/step - loss: 0.4742 - acc: 0.8395 - val_loss: 0.3399 - val_acc: 0.8886
Epoch 75/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4649 - acc: 0.8448Epoch 00075: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.4655 - acc: 0.8439 - val_loss: 0.3497 - val_acc: 0.8843
Epoch 76/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4629 - acc: 0.8436Epoch 00076: val_loss improved from 0.33988 to 0.33864, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.4615 - acc: 0.8435 - val_loss: 0.3386 - val_acc: 0.8923
Epoch 77/400
13568/13776 [============================&gt;.] - ETA: 0s - loss: 0.4483 - acc: 0.8481Epoch 00077: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.4493 - acc: 0.8478 - val_loss: 0.3409 - val_acc: 0.8886
Epoch 78/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4546 - acc: 0.8459Epoch 00078: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.4555 - acc: 0.8452 - val_loss: 0.3437 - val_acc: 0.8865
Epoch 79/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4472 - acc: 0.8447Epoch 00079: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.4443 - acc: 0.8461 - val_loss: 0.3506 - val_acc: 0.8877
Epoch 80/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4437 - acc: 0.8498Epoch 00080: val_loss improved from 0.33864 to 0.33840, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.4445 - acc: 0.8488 - val_loss: 0.3384 - val_acc: 0.8906
Epoch 81/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4314 - acc: 0.8551Epoch 00081: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.4322 - acc: 0.8545 - val_loss: 0.3387 - val_acc: 0.8901
Epoch 82/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4453 - acc: 0.8531Epoch 00082: val_loss improved from 0.33840 to 0.32765, saving model to weights.hdf5
13776/13776 [==============================] - 1s 103us/step - loss: 0.4446 - acc: 0.8534 - val_loss: 0.3277 - val_acc: 0.8933
Epoch 83/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4414 - acc: 0.8519Epoch 00083: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.4414 - acc: 0.8518 - val_loss: 0.3349 - val_acc: 0.8933
Epoch 84/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4207 - acc: 0.8549Epoch 00084: val_loss improved from 0.32765 to 0.32305, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 0.4197 - acc: 0.8550 - val_loss: 0.3230 - val_acc: 0.8943
Epoch 85/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4218 - acc: 0.8570Epoch 00085: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.4223 - acc: 0.8566 - val_loss: 0.3342 - val_acc: 0.8897
Epoch 86/400
13568/13776 [============================&gt;.] - ETA: 0s - loss: 0.4192 - acc: 0.8607Epoch 00086: val_loss improved from 0.32305 to 0.32135, saving model to weights.hdf5
13776/13776 [==============================] - 1s 107us/step - loss: 0.4193 - acc: 0.8604 - val_loss: 0.3213 - val_acc: 0.8962
Epoch 87/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4131 - acc: 0.8589Epoch 00087: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.4118 - acc: 0.8590 - val_loss: 0.3224 - val_acc: 0.8952
Epoch 88/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4157 - acc: 0.8579Epoch 00088: val_loss improved from 0.32135 to 0.31998, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 0.4181 - acc: 0.8569 - val_loss: 0.3200 - val_acc: 0.8987
Epoch 89/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4108 - acc: 0.8591Epoch 00089: val_loss improved from 0.31998 to 0.31673, saving model to weights.hdf5
13776/13776 [==============================] - 1s 103us/step - loss: 0.4106 - acc: 0.8595 - val_loss: 0.3167 - val_acc: 0.8958
Epoch 90/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4141 - acc: 0.8621Epoch 00090: val_loss did not improve
13776/13776 [==============================] - 1s 100us/step - loss: 0.4114 - acc: 0.8634 - val_loss: 0.3204 - val_acc: 0.8955
Epoch 91/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4076 - acc: 0.8616Epoch 00091: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.4076 - acc: 0.8616 - val_loss: 0.3183 - val_acc: 0.8963
Epoch 92/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4039 - acc: 0.8643Epoch 00092: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.4039 - acc: 0.8643 - val_loss: 0.3217 - val_acc: 0.8958
Epoch 93/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3984 - acc: 0.8632Epoch 00093: val_loss improved from 0.31673 to 0.31519, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 0.3983 - acc: 0.8634 - val_loss: 0.3152 - val_acc: 0.8967
Epoch 94/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4001 - acc: 0.8634Epoch 00094: val_loss improved from 0.31519 to 0.31233, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 0.4001 - acc: 0.8628 - val_loss: 0.3123 - val_acc: 0.8974
Epoch 95/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3983 - acc: 0.8637Epoch 00095: val_loss improved from 0.31233 to 0.30449, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 0.3966 - acc: 0.8645 - val_loss: 0.3045 - val_acc: 0.9001
Epoch 96/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.4002 - acc: 0.8638Epoch 00096: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.4013 - acc: 0.8640 - val_loss: 0.3111 - val_acc: 0.8994
Epoch 97/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3895 - acc: 0.8660Epoch 00097: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.3887 - acc: 0.8663 - val_loss: 0.3064 - val_acc: 0.8987
Epoch 98/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3822 - acc: 0.8693Epoch 00098: val_loss improved from 0.30449 to 0.30415, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 0.3844 - acc: 0.8687 - val_loss: 0.3042 - val_acc: 0.9031
Epoch 99/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3926 - acc: 0.8679Epoch 00099: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.3934 - acc: 0.8677 - val_loss: 0.3100 - val_acc: 0.9014
Epoch 100/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3886 - acc: 0.8695Epoch 00100: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.3880 - acc: 0.8694 - val_loss: 0.3045 - val_acc: 0.9013
Epoch 101/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3893 - acc: 0.8672Epoch 00101: val_loss improved from 0.30415 to 0.30152, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.3880 - acc: 0.8675 - val_loss: 0.3015 - val_acc: 0.9013
Epoch 102/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3821 - acc: 0.8704Epoch 00102: val_loss improved from 0.30152 to 0.29928, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.3807 - acc: 0.8710 - val_loss: 0.2993 - val_acc: 0.9038
Epoch 103/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3832 - acc: 0.8743Epoch 00103: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.3819 - acc: 0.8749 - val_loss: 0.3068 - val_acc: 0.9031
Epoch 104/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3651 - acc: 0.8752Epoch 00104: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.3665 - acc: 0.8746 - val_loss: 0.3039 - val_acc: 0.9026
Epoch 105/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3738 - acc: 0.8741Epoch 00105: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.3730 - acc: 0.8745 - val_loss: 0.3066 - val_acc: 0.9016
Epoch 106/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3674 - acc: 0.8739Epoch 00106: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.3688 - acc: 0.8740 - val_loss: 0.3030 - val_acc: 0.9018
Epoch 107/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3587 - acc: 0.8777Epoch 00107: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.3570 - acc: 0.8780 - val_loss: 0.3001 - val_acc: 0.9040
Epoch 108/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3640 - acc: 0.8752Epoch 00108: val_loss improved from 0.29928 to 0.29533, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 0.3644 - acc: 0.8753 - val_loss: 0.2953 - val_acc: 0.9048
Epoch 109/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3640 - acc: 0.8761Epoch 00109: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.3636 - acc: 0.8759 - val_loss: 0.2965 - val_acc: 0.9065
Epoch 110/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3675 - acc: 0.8759Epoch 00110: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.3648 - acc: 0.8769 - val_loss: 0.2974 - val_acc: 0.9045
Epoch 111/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3593 - acc: 0.8791Epoch 00111: val_loss improved from 0.29533 to 0.29230, saving model to weights.hdf5
13776/13776 [==============================] - 1s 103us/step - loss: 0.3581 - acc: 0.8795 - val_loss: 0.2923 - val_acc: 0.9045
Epoch 112/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3579 - acc: 0.8741Epoch 00112: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.3583 - acc: 0.8738 - val_loss: 0.2968 - val_acc: 0.9048
Epoch 113/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3487 - acc: 0.8833Epoch 00113: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.3499 - acc: 0.8833 - val_loss: 0.2926 - val_acc: 0.9077
Epoch 114/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3611 - acc: 0.8796Epoch 00114: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.3612 - acc: 0.8789 - val_loss: 0.3015 - val_acc: 0.9029
Epoch 115/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3449 - acc: 0.8822Epoch 00115: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.3429 - acc: 0.8833 - val_loss: 0.2929 - val_acc: 0.9058
Epoch 116/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3382 - acc: 0.8844Epoch 00116: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.3384 - acc: 0.8843 - val_loss: 0.2936 - val_acc: 0.9063
Epoch 117/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3493 - acc: 0.8824Epoch 00117: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.3488 - acc: 0.8825 - val_loss: 0.2969 - val_acc: 0.9048
Epoch 118/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3466 - acc: 0.8824Epoch 00118: val_loss improved from 0.29230 to 0.28893, saving model to weights.hdf5
13776/13776 [==============================] - 1s 103us/step - loss: 0.3473 - acc: 0.8825 - val_loss: 0.2889 - val_acc: 0.9072
Epoch 119/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3383 - acc: 0.8859Epoch 00119: val_loss did not improve
13776/13776 [==============================] - 1s 100us/step - loss: 0.3378 - acc: 0.8858 - val_loss: 0.2901 - val_acc: 0.9067
Epoch 120/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3378 - acc: 0.8831Epoch 00120: val_loss improved from 0.28893 to 0.28665, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 0.3393 - acc: 0.8832 - val_loss: 0.2866 - val_acc: 0.9096
Epoch 121/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3394 - acc: 0.8869Epoch 00121: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.3376 - acc: 0.8875 - val_loss: 0.2895 - val_acc: 0.9063
Epoch 122/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3421 - acc: 0.8824Epoch 00122: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.3420 - acc: 0.8828 - val_loss: 0.2955 - val_acc: 0.9068
Epoch 123/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3323 - acc: 0.8885Epoch 00123: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.3321 - acc: 0.8885 - val_loss: 0.2941 - val_acc: 0.9031
Epoch 124/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3231 - acc: 0.8863Epoch 00124: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.3245 - acc: 0.8863 - val_loss: 0.2920 - val_acc: 0.9097
Epoch 125/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3377 - acc: 0.8842Epoch 00125: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.3367 - acc: 0.8844 - val_loss: 0.2902 - val_acc: 0.9079
Epoch 126/400
13568/13776 [============================&gt;.] - ETA: 0s - loss: 0.3299 - acc: 0.8858Epoch 00126: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.3289 - acc: 0.8863 - val_loss: 0.2892 - val_acc: 0.9084
Epoch 127/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3269 - acc: 0.8904Epoch 00127: val_loss improved from 0.28665 to 0.28587, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.3271 - acc: 0.8899 - val_loss: 0.2859 - val_acc: 0.9097
Epoch 128/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3283 - acc: 0.8864Epoch 00128: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.3270 - acc: 0.8873 - val_loss: 0.2891 - val_acc: 0.9087
Epoch 129/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3305 - acc: 0.8891Epoch 00129: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.3300 - acc: 0.8900 - val_loss: 0.2888 - val_acc: 0.9075
Epoch 130/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3100 - acc: 0.8945Epoch 00130: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.3131 - acc: 0.8933 - val_loss: 0.2902 - val_acc: 0.9094
Epoch 131/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3186 - acc: 0.8915Epoch 00131: val_loss improved from 0.28587 to 0.28553, saving model to weights.hdf5
13776/13776 [==============================] - 1s 106us/step - loss: 0.3173 - acc: 0.8917 - val_loss: 0.2855 - val_acc: 0.9089
Epoch 132/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3176 - acc: 0.8908Epoch 00132: val_loss improved from 0.28553 to 0.28483, saving model to weights.hdf5
13776/13776 [==============================] - 1s 106us/step - loss: 0.3196 - acc: 0.8900 - val_loss: 0.2848 - val_acc: 0.9084
Epoch 133/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3211 - acc: 0.8899Epoch 00133: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.3228 - acc: 0.8896 - val_loss: 0.2951 - val_acc: 0.9046
Epoch 134/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3185 - acc: 0.8921Epoch 00134: val_loss improved from 0.28483 to 0.28293, saving model to weights.hdf5
13776/13776 [==============================] - 1s 103us/step - loss: 0.3194 - acc: 0.8918 - val_loss: 0.2829 - val_acc: 0.9072
Epoch 135/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3138 - acc: 0.8942Epoch 00135: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.3132 - acc: 0.8942 - val_loss: 0.2841 - val_acc: 0.9097
Epoch 136/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3112 - acc: 0.8937Epoch 00136: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.3129 - acc: 0.8929 - val_loss: 0.2850 - val_acc: 0.9104
Epoch 137/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3144 - acc: 0.8912Epoch 00137: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.3146 - acc: 0.8913 - val_loss: 0.2859 - val_acc: 0.9099
Epoch 138/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3025 - acc: 0.8946Epoch 00138: val_loss improved from 0.28293 to 0.28193, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 0.3035 - acc: 0.8942 - val_loss: 0.2819 - val_acc: 0.9109
Epoch 139/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3167 - acc: 0.8902Epoch 00139: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.3135 - acc: 0.8916 - val_loss: 0.2823 - val_acc: 0.9121
Epoch 140/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3161 - acc: 0.8921Epoch 00140: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.3146 - acc: 0.8928 - val_loss: 0.2820 - val_acc: 0.9116
Epoch 141/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3108 - acc: 0.8939Epoch 00141: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.3120 - acc: 0.8935 - val_loss: 0.2868 - val_acc: 0.9094
Epoch 142/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2984 - acc: 0.8984Epoch 00142: val_loss improved from 0.28193 to 0.27835, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 0.2989 - acc: 0.8979 - val_loss: 0.2784 - val_acc: 0.9119
Epoch 143/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3065 - acc: 0.8937Epoch 00143: val_loss improved from 0.27835 to 0.27734, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 0.3059 - acc: 0.8943 - val_loss: 0.2773 - val_acc: 0.9116
Epoch 144/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2953 - acc: 0.8971Epoch 00144: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2930 - acc: 0.8980 - val_loss: 0.2829 - val_acc: 0.9129
Epoch 145/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2992 - acc: 0.8971Epoch 00145: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.3016 - acc: 0.8961 - val_loss: 0.2820 - val_acc: 0.9106
Epoch 146/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2983 - acc: 0.8978Epoch 00146: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2999 - acc: 0.8969 - val_loss: 0.2786 - val_acc: 0.9106
Epoch 147/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2982 - acc: 0.8977Epoch 00147: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2993 - acc: 0.8971 - val_loss: 0.2813 - val_acc: 0.9138
Epoch 148/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.3039 - acc: 0.8988Epoch 00148: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.3041 - acc: 0.8987 - val_loss: 0.2780 - val_acc: 0.9129
Epoch 149/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2933 - acc: 0.9003Epoch 00149: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2911 - acc: 0.9008 - val_loss: 0.2774 - val_acc: 0.9133
Epoch 150/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2900 - acc: 0.9029Epoch 00150: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2916 - acc: 0.9024 - val_loss: 0.2776 - val_acc: 0.9155
Epoch 151/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2887 - acc: 0.9008Epoch 00151: val_loss improved from 0.27734 to 0.27693, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 0.2898 - acc: 0.9006 - val_loss: 0.2769 - val_acc: 0.9148
Epoch 152/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2895 - acc: 0.9006Epoch 00152: val_loss improved from 0.27693 to 0.27437, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 0.2915 - acc: 0.9001 - val_loss: 0.2744 - val_acc: 0.9136
Epoch 153/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2859 - acc: 0.9018Epoch 00153: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2863 - acc: 0.9016 - val_loss: 0.2783 - val_acc: 0.9123
Epoch 154/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2829 - acc: 0.9030Epoch 00154: val_loss improved from 0.27437 to 0.27249, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.2859 - acc: 0.9019 - val_loss: 0.2725 - val_acc: 0.9140
Epoch 155/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2941 - acc: 0.9018Epoch 00155: val_loss improved from 0.27249 to 0.27072, saving model to weights.hdf5
13776/13776 [==============================] - 1s 106us/step - loss: 0.2932 - acc: 0.9021 - val_loss: 0.2707 - val_acc: 0.9140
Epoch 156/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2931 - acc: 0.9016Epoch 00156: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2912 - acc: 0.9019 - val_loss: 0.2757 - val_acc: 0.9143
Epoch 157/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2877 - acc: 0.9013Epoch 00157: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2890 - acc: 0.9010 - val_loss: 0.2725 - val_acc: 0.9157
Epoch 158/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2858 - acc: 0.9026Epoch 00158: val_loss improved from 0.27072 to 0.27013, saving model to weights.hdf5
13776/13776 [==============================] - 1s 104us/step - loss: 0.2844 - acc: 0.9032 - val_loss: 0.2701 - val_acc: 0.9146
Epoch 159/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2855 - acc: 0.9054Epoch 00159: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2861 - acc: 0.9056 - val_loss: 0.2730 - val_acc: 0.9126
Epoch 160/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2882 - acc: 0.9016Epoch 00160: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2875 - acc: 0.9021 - val_loss: 0.2759 - val_acc: 0.9133
Epoch 161/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2846 - acc: 0.9051Epoch 00161: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2852 - acc: 0.9049 - val_loss: 0.2717 - val_acc: 0.9163
Epoch 162/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2697 - acc: 0.9055Epoch 00162: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2690 - acc: 0.9060 - val_loss: 0.2736 - val_acc: 0.9150
Epoch 163/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2706 - acc: 0.9080Epoch 00163: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2718 - acc: 0.9073 - val_loss: 0.2779 - val_acc: 0.9123
Epoch 164/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2764 - acc: 0.9056Epoch 00164: val_loss improved from 0.27013 to 0.26674, saving model to weights.hdf5
13776/13776 [==============================] - 1s 103us/step - loss: 0.2764 - acc: 0.9059 - val_loss: 0.2667 - val_acc: 0.9153
Epoch 165/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2682 - acc: 0.9097Epoch 00165: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2665 - acc: 0.9098 - val_loss: 0.2695 - val_acc: 0.9146
Epoch 166/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2865 - acc: 0.9026Epoch 00166: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2860 - acc: 0.9028 - val_loss: 0.2746 - val_acc: 0.9138
Epoch 167/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2785 - acc: 0.9067Epoch 00167: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2796 - acc: 0.9063 - val_loss: 0.2819 - val_acc: 0.9121
Epoch 168/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2758 - acc: 0.9014Epoch 00168: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2743 - acc: 0.9018 - val_loss: 0.2844 - val_acc: 0.9124
Epoch 169/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2658 - acc: 0.9053Epoch 00169: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2662 - acc: 0.9051 - val_loss: 0.2779 - val_acc: 0.9131
Epoch 170/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2680 - acc: 0.9089Epoch 00170: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2691 - acc: 0.9087 - val_loss: 0.2766 - val_acc: 0.9134
Epoch 171/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2725 - acc: 0.9078Epoch 00171: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2729 - acc: 0.9078 - val_loss: 0.2758 - val_acc: 0.9123
Epoch 172/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2696 - acc: 0.9101Epoch 00172: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2697 - acc: 0.9093 - val_loss: 0.2710 - val_acc: 0.9138
Epoch 173/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2704 - acc: 0.9066Epoch 00173: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2701 - acc: 0.9065 - val_loss: 0.2709 - val_acc: 0.9150
Epoch 174/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2746 - acc: 0.9067Epoch 00174: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2726 - acc: 0.9072 - val_loss: 0.2711 - val_acc: 0.9155
Epoch 175/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2634 - acc: 0.9096Epoch 00175: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2642 - acc: 0.9098 - val_loss: 0.2728 - val_acc: 0.9162
Epoch 176/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2728 - acc: 0.9049Epoch 00176: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.2710 - acc: 0.9054 - val_loss: 0.2735 - val_acc: 0.9162
Epoch 177/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2706 - acc: 0.9075Epoch 00177: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.2716 - acc: 0.9072 - val_loss: 0.2753 - val_acc: 0.9134
Epoch 178/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2613 - acc: 0.9104Epoch 00178: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2605 - acc: 0.9105 - val_loss: 0.2702 - val_acc: 0.9150
Epoch 179/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2591 - acc: 0.9103Epoch 00179: val_loss improved from 0.26674 to 0.26639, saving model to weights.hdf5
13776/13776 [==============================] - 1s 105us/step - loss: 0.2620 - acc: 0.9089 - val_loss: 0.2664 - val_acc: 0.9194
Epoch 180/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2631 - acc: 0.9091Epoch 00180: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.2620 - acc: 0.9097 - val_loss: 0.2684 - val_acc: 0.9177
Epoch 181/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2576 - acc: 0.9123Epoch 00181: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2573 - acc: 0.9124 - val_loss: 0.2690 - val_acc: 0.9162
Epoch 182/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2542 - acc: 0.9096Epoch 00182: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2535 - acc: 0.9098 - val_loss: 0.2726 - val_acc: 0.9165
Epoch 183/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2548 - acc: 0.9096Epoch 00183: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.2564 - acc: 0.9093 - val_loss: 0.2708 - val_acc: 0.9170
Epoch 184/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2550 - acc: 0.9138Epoch 00184: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2564 - acc: 0.9131 - val_loss: 0.2684 - val_acc: 0.9167
Epoch 185/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2483 - acc: 0.9147Epoch 00185: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2477 - acc: 0.9149 - val_loss: 0.2750 - val_acc: 0.9175
Epoch 186/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2615 - acc: 0.9104Epoch 00186: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2617 - acc: 0.9101 - val_loss: 0.2754 - val_acc: 0.9136
Epoch 187/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2526 - acc: 0.9137Epoch 00187: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2537 - acc: 0.9132 - val_loss: 0.2673 - val_acc: 0.9167
Epoch 188/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2587 - acc: 0.9102Epoch 00188: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2590 - acc: 0.9098 - val_loss: 0.2721 - val_acc: 0.9150
Epoch 189/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2516 - acc: 0.9140Epoch 00189: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2515 - acc: 0.9142 - val_loss: 0.2714 - val_acc: 0.9150
Epoch 190/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2372 - acc: 0.9205Epoch 00190: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2389 - acc: 0.9201 - val_loss: 0.2738 - val_acc: 0.9160
Epoch 191/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2395 - acc: 0.9174Epoch 00191: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2389 - acc: 0.9179 - val_loss: 0.2681 - val_acc: 0.9175
Epoch 192/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2529 - acc: 0.9129Epoch 00192: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2531 - acc: 0.9127 - val_loss: 0.2737 - val_acc: 0.9157
Epoch 193/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2617 - acc: 0.9086Epoch 00193: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2621 - acc: 0.9083 - val_loss: 0.2707 - val_acc: 0.9172
Epoch 194/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2440 - acc: 0.9155Epoch 00194: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2444 - acc: 0.9155 - val_loss: 0.2701 - val_acc: 0.9172
Epoch 195/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2496 - acc: 0.9146Epoch 00195: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2507 - acc: 0.9147 - val_loss: 0.2688 - val_acc: 0.9148
Epoch 196/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2484 - acc: 0.9157Epoch 00196: val_loss improved from 0.26639 to 0.26309, saving model to weights.hdf5
13776/13776 [==============================] - 1s 103us/step - loss: 0.2479 - acc: 0.9159 - val_loss: 0.2631 - val_acc: 0.9172
Epoch 197/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2346 - acc: 0.9172Epoch 00197: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2336 - acc: 0.9172 - val_loss: 0.2720 - val_acc: 0.9160
Epoch 198/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2476 - acc: 0.9161Epoch 00198: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2482 - acc: 0.9159 - val_loss: 0.2697 - val_acc: 0.9163
Epoch 199/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2325 - acc: 0.9201Epoch 00199: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2349 - acc: 0.9191 - val_loss: 0.2726 - val_acc: 0.9158
Epoch 200/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2495 - acc: 0.9135Epoch 00200: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2484 - acc: 0.9144 - val_loss: 0.2722 - val_acc: 0.9160
Epoch 201/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2396 - acc: 0.9177Epoch 00201: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2385 - acc: 0.9182 - val_loss: 0.2730 - val_acc: 0.9145
Epoch 202/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2430 - acc: 0.9157Epoch 00202: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2432 - acc: 0.9157 - val_loss: 0.2748 - val_acc: 0.9146
Epoch 203/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2383 - acc: 0.9159Epoch 00203: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2400 - acc: 0.9152 - val_loss: 0.2673 - val_acc: 0.9165
Epoch 204/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2401 - acc: 0.9163Epoch 00204: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2408 - acc: 0.9162 - val_loss: 0.2700 - val_acc: 0.9140
Epoch 205/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2269 - acc: 0.9227Epoch 00205: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2289 - acc: 0.9221 - val_loss: 0.2719 - val_acc: 0.9182
Epoch 206/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2348 - acc: 0.9200Epoch 00206: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2372 - acc: 0.9194 - val_loss: 0.2715 - val_acc: 0.9163
Epoch 207/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2342 - acc: 0.9189Epoch 00207: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2361 - acc: 0.9185 - val_loss: 0.2813 - val_acc: 0.9155
Epoch 208/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2325 - acc: 0.9191Epoch 00208: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2331 - acc: 0.9190 - val_loss: 0.2744 - val_acc: 0.9162
Epoch 209/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2422 - acc: 0.9174Epoch 00209: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2415 - acc: 0.9175 - val_loss: 0.2724 - val_acc: 0.9175
Epoch 210/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2379 - acc: 0.9173Epoch 00210: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2358 - acc: 0.9181 - val_loss: 0.2697 - val_acc: 0.9187
Epoch 211/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2341 - acc: 0.9209Epoch 00211: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2351 - acc: 0.9204 - val_loss: 0.2690 - val_acc: 0.9158
Epoch 212/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2350 - acc: 0.9183Epoch 00212: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2379 - acc: 0.9178 - val_loss: 0.2760 - val_acc: 0.9158
Epoch 213/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2344 - acc: 0.9209Epoch 00213: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2338 - acc: 0.9207 - val_loss: 0.2726 - val_acc: 0.9168
Epoch 214/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2333 - acc: 0.9189Epoch 00214: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2331 - acc: 0.9186 - val_loss: 0.2726 - val_acc: 0.9158
Epoch 215/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2260 - acc: 0.9216Epoch 00215: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2255 - acc: 0.9216 - val_loss: 0.2684 - val_acc: 0.9175
Epoch 216/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2341 - acc: 0.9183Epoch 00216: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2326 - acc: 0.9192 - val_loss: 0.2713 - val_acc: 0.9182
Epoch 217/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2232 - acc: 0.9239Epoch 00217: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2223 - acc: 0.9244 - val_loss: 0.2693 - val_acc: 0.9175
Epoch 218/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2332 - acc: 0.9215Epoch 00218: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2324 - acc: 0.9210 - val_loss: 0.2690 - val_acc: 0.9157
Epoch 219/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2320 - acc: 0.9191Epoch 00219: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2343 - acc: 0.9183 - val_loss: 0.2704 - val_acc: 0.9151
Epoch 220/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2266 - acc: 0.9227Epoch 00220: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2277 - acc: 0.9225 - val_loss: 0.2712 - val_acc: 0.9180
Epoch 221/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2299 - acc: 0.9201Epoch 00221: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2281 - acc: 0.9208 - val_loss: 0.2735 - val_acc: 0.9179
Epoch 222/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2310 - acc: 0.9210Epoch 00222: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2311 - acc: 0.9209 - val_loss: 0.2658 - val_acc: 0.9184
Epoch 223/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2230 - acc: 0.9240Epoch 00223: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.2254 - acc: 0.9232 - val_loss: 0.2669 - val_acc: 0.9157
Epoch 224/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2197 - acc: 0.9246Epoch 00224: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.2206 - acc: 0.9245 - val_loss: 0.2665 - val_acc: 0.9163
Epoch 225/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2263 - acc: 0.9220Epoch 00225: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.2264 - acc: 0.9224 - val_loss: 0.2667 - val_acc: 0.9170
Epoch 226/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2143 - acc: 0.9262Epoch 00226: val_loss did not improve
13776/13776 [==============================] - 1s 105us/step - loss: 0.2144 - acc: 0.9260 - val_loss: 0.2706 - val_acc: 0.9165
Epoch 227/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2296 - acc: 0.9219Epoch 00227: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2286 - acc: 0.9225 - val_loss: 0.2692 - val_acc: 0.9173
Epoch 228/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2282 - acc: 0.9227Epoch 00228: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2278 - acc: 0.9233 - val_loss: 0.2657 - val_acc: 0.9209
Epoch 229/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2226 - acc: 0.9238Epoch 00229: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2225 - acc: 0.9239 - val_loss: 0.2738 - val_acc: 0.9172
Epoch 230/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2333 - acc: 0.9226Epoch 00230: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2335 - acc: 0.9223 - val_loss: 0.2657 - val_acc: 0.9180
Epoch 231/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2211 - acc: 0.9232Epoch 00231: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.2202 - acc: 0.9233 - val_loss: 0.2710 - val_acc: 0.9160
Epoch 232/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2157 - acc: 0.9269Epoch 00232: val_loss did not improve
13776/13776 [==============================] - 1s 105us/step - loss: 0.2151 - acc: 0.9271 - val_loss: 0.2734 - val_acc: 0.9175
Epoch 233/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2177 - acc: 0.9243Epoch 00233: val_loss did not improve
13776/13776 [==============================] - 1s 105us/step - loss: 0.2173 - acc: 0.9243 - val_loss: 0.2751 - val_acc: 0.9150
Epoch 234/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2266 - acc: 0.9229Epoch 00234: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2259 - acc: 0.9231 - val_loss: 0.2701 - val_acc: 0.9184
Epoch 235/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2173 - acc: 0.9256Epoch 00235: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2183 - acc: 0.9249 - val_loss: 0.2645 - val_acc: 0.9201
Epoch 236/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2240 - acc: 0.9210Epoch 00236: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.2237 - acc: 0.9215 - val_loss: 0.2723 - val_acc: 0.9146
Epoch 237/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2172 - acc: 0.9262Epoch 00237: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2167 - acc: 0.9265 - val_loss: 0.2666 - val_acc: 0.9189
Epoch 238/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2224 - acc: 0.9225Epoch 00238: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2220 - acc: 0.9225 - val_loss: 0.2713 - val_acc: 0.9168
Epoch 239/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2190 - acc: 0.9262Epoch 00239: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2199 - acc: 0.9255 - val_loss: 0.2697 - val_acc: 0.9175
Epoch 240/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2142 - acc: 0.9253Epoch 00240: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2147 - acc: 0.9257 - val_loss: 0.2717 - val_acc: 0.9170
Epoch 241/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2114 - acc: 0.9269Epoch 00241: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2106 - acc: 0.9268 - val_loss: 0.2711 - val_acc: 0.9185
Epoch 242/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2182 - acc: 0.9257Epoch 00242: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2180 - acc: 0.9257 - val_loss: 0.2688 - val_acc: 0.9192
Epoch 243/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2043 - acc: 0.9295Epoch 00243: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2043 - acc: 0.9298 - val_loss: 0.2738 - val_acc: 0.9182
Epoch 244/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2096 - acc: 0.9278Epoch 00244: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2091 - acc: 0.9276 - val_loss: 0.2701 - val_acc: 0.9165
Epoch 245/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2151 - acc: 0.9273Epoch 00245: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2141 - acc: 0.9276 - val_loss: 0.2666 - val_acc: 0.9207
Epoch 246/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2097 - acc: 0.9283Epoch 00246: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2080 - acc: 0.9291 - val_loss: 0.2735 - val_acc: 0.9170
Epoch 247/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2088 - acc: 0.9292Epoch 00247: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2079 - acc: 0.9293 - val_loss: 0.2713 - val_acc: 0.9201
Epoch 248/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2106 - acc: 0.9300Epoch 00248: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2110 - acc: 0.9300 - val_loss: 0.2745 - val_acc: 0.9163
Epoch 249/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2105 - acc: 0.9275Epoch 00249: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.2108 - acc: 0.9272 - val_loss: 0.2714 - val_acc: 0.9185
Epoch 250/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2066 - acc: 0.9292Epoch 00250: val_loss did not improve
13776/13776 [==============================] - 1s 105us/step - loss: 0.2082 - acc: 0.9288 - val_loss: 0.2702 - val_acc: 0.9202
Epoch 251/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2095 - acc: 0.9274Epoch 00251: val_loss did not improve
13776/13776 [==============================] - 1s 105us/step - loss: 0.2090 - acc: 0.9273 - val_loss: 0.2685 - val_acc: 0.9187
Epoch 252/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2006 - acc: 0.9292Epoch 00252: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.2008 - acc: 0.9292 - val_loss: 0.2759 - val_acc: 0.9184
Epoch 253/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2033 - acc: 0.9306Epoch 00253: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.2039 - acc: 0.9307 - val_loss: 0.2715 - val_acc: 0.9175
Epoch 254/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2008 - acc: 0.9331Epoch 00254: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.2003 - acc: 0.9334 - val_loss: 0.2721 - val_acc: 0.9182
Epoch 255/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2108 - acc: 0.9264Epoch 00255: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2101 - acc: 0.9269 - val_loss: 0.2729 - val_acc: 0.9202
Epoch 256/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2022 - acc: 0.9326Epoch 00256: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2028 - acc: 0.9321 - val_loss: 0.2733 - val_acc: 0.9190
Epoch 257/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1945 - acc: 0.9343Epoch 00257: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1962 - acc: 0.9337 - val_loss: 0.2744 - val_acc: 0.9168
Epoch 258/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2018 - acc: 0.9331Epoch 00258: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.2017 - acc: 0.9327 - val_loss: 0.2689 - val_acc: 0.9207
Epoch 259/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2050 - acc: 0.9303Epoch 00259: val_loss did not improve
13776/13776 [==============================] - 1s 100us/step - loss: 0.2039 - acc: 0.9307 - val_loss: 0.2662 - val_acc: 0.9211
Epoch 260/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2032 - acc: 0.9310Epoch 00260: val_loss did not improve
13776/13776 [==============================] - 1s 100us/step - loss: 0.2021 - acc: 0.9311 - val_loss: 0.2754 - val_acc: 0.9165
Epoch 261/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1942 - acc: 0.9329Epoch 00261: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.1941 - acc: 0.9329 - val_loss: 0.2740 - val_acc: 0.9150
Epoch 262/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2065 - acc: 0.9253Epoch 00262: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2064 - acc: 0.9252 - val_loss: 0.2710 - val_acc: 0.9185
Epoch 263/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2034 - acc: 0.9286Epoch 00263: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.2018 - acc: 0.9291 - val_loss: 0.2702 - val_acc: 0.9199
Epoch 264/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.2052 - acc: 0.9286Epoch 00264: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.2053 - acc: 0.9286 - val_loss: 0.2771 - val_acc: 0.9162
Epoch 265/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1950 - acc: 0.9325Epoch 00265: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.1961 - acc: 0.9321 - val_loss: 0.2707 - val_acc: 0.9194
Epoch 266/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1971 - acc: 0.9347Epoch 00266: val_loss did not improve
13776/13776 [==============================] - 1s 100us/step - loss: 0.1969 - acc: 0.9348 - val_loss: 0.2782 - val_acc: 0.9175
Epoch 267/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1903 - acc: 0.9358Epoch 00267: val_loss did not improve
13776/13776 [==============================] - 1s 100us/step - loss: 0.1902 - acc: 0.9361 - val_loss: 0.2731 - val_acc: 0.9194
Epoch 268/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1917 - acc: 0.9344Epoch 00268: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1922 - acc: 0.9342 - val_loss: 0.2762 - val_acc: 0.9177
Epoch 269/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1876 - acc: 0.9347Epoch 00269: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1900 - acc: 0.9342 - val_loss: 0.2818 - val_acc: 0.9150
Epoch 270/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1965 - acc: 0.9333Epoch 00270: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1970 - acc: 0.9331 - val_loss: 0.2678 - val_acc: 0.9195
Epoch 271/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1923 - acc: 0.9361Epoch 00271: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1919 - acc: 0.9359 - val_loss: 0.2692 - val_acc: 0.9175
Epoch 272/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1986 - acc: 0.9314Epoch 00272: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1988 - acc: 0.9313 - val_loss: 0.2731 - val_acc: 0.9179
Epoch 273/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1954 - acc: 0.9323Epoch 00273: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1945 - acc: 0.9325 - val_loss: 0.2741 - val_acc: 0.9165
Epoch 274/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1951 - acc: 0.9333Epoch 00274: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1961 - acc: 0.9331 - val_loss: 0.2803 - val_acc: 0.9153
Epoch 275/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1957 - acc: 0.9331Epoch 00275: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1955 - acc: 0.9333 - val_loss: 0.2751 - val_acc: 0.9173
Epoch 276/400
13568/13776 [============================&gt;.] - ETA: 0s - loss: 0.1957 - acc: 0.9351Epoch 00276: val_loss did not improve
13776/13776 [==============================] - 1s 105us/step - loss: 0.1952 - acc: 0.9352 - val_loss: 0.2702 - val_acc: 0.9194
Epoch 277/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1971 - acc: 0.9313Epoch 00277: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1985 - acc: 0.9307 - val_loss: 0.2814 - val_acc: 0.9160
Epoch 278/400
13568/13776 [============================&gt;.] - ETA: 0s - loss: 0.1939 - acc: 0.9357Epoch 00278: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1936 - acc: 0.9357 - val_loss: 0.2720 - val_acc: 0.9199
Epoch 279/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1978 - acc: 0.9328Epoch 00279: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1977 - acc: 0.9330 - val_loss: 0.2693 - val_acc: 0.9207
Epoch 280/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1994 - acc: 0.9294Epoch 00280: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1985 - acc: 0.9295 - val_loss: 0.2738 - val_acc: 0.9179
Epoch 281/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1965 - acc: 0.9317Epoch 00281: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1959 - acc: 0.9320 - val_loss: 0.2670 - val_acc: 0.9194
Epoch 282/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1945 - acc: 0.9349Epoch 00282: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1949 - acc: 0.9347 - val_loss: 0.2643 - val_acc: 0.9194
Epoch 283/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1912 - acc: 0.9312Epoch 00283: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1915 - acc: 0.9310 - val_loss: 0.2714 - val_acc: 0.9177
Epoch 284/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1946 - acc: 0.9343Epoch 00284: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1950 - acc: 0.9342 - val_loss: 0.2752 - val_acc: 0.9182
Epoch 285/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1999 - acc: 0.9305Epoch 00285: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.2002 - acc: 0.9304 - val_loss: 0.2658 - val_acc: 0.9195
Epoch 286/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1924 - acc: 0.9331Epoch 00286: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1922 - acc: 0.9332 - val_loss: 0.2689 - val_acc: 0.9195
Epoch 287/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1926 - acc: 0.9336Epoch 00287: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1924 - acc: 0.9334 - val_loss: 0.2696 - val_acc: 0.9189
Epoch 288/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1897 - acc: 0.9342Epoch 00288: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1884 - acc: 0.9345 - val_loss: 0.2679 - val_acc: 0.9217
Epoch 289/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1928 - acc: 0.9324Epoch 00289: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1931 - acc: 0.9319 - val_loss: 0.2666 - val_acc: 0.9194
Epoch 290/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1908 - acc: 0.9350Epoch 00290: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1901 - acc: 0.9350 - val_loss: 0.2767 - val_acc: 0.9190
Epoch 291/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1960 - acc: 0.9316Epoch 00291: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1945 - acc: 0.9319 - val_loss: 0.2723 - val_acc: 0.9202
Epoch 292/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1943 - acc: 0.9355Epoch 00292: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1938 - acc: 0.9354 - val_loss: 0.2762 - val_acc: 0.9192
Epoch 293/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1902 - acc: 0.9361Epoch 00293: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1884 - acc: 0.9366 - val_loss: 0.2709 - val_acc: 0.9204
Epoch 294/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1903 - acc: 0.9331Epoch 00294: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1899 - acc: 0.9331 - val_loss: 0.2685 - val_acc: 0.9217
Epoch 295/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1890 - acc: 0.9367Epoch 00295: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1882 - acc: 0.9372 - val_loss: 0.2717 - val_acc: 0.9202
Epoch 296/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1855 - acc: 0.9349Epoch 00296: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1859 - acc: 0.9350 - val_loss: 0.2750 - val_acc: 0.9206
Epoch 297/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1830 - acc: 0.9370Epoch 00297: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1824 - acc: 0.9375 - val_loss: 0.2724 - val_acc: 0.9224
Epoch 298/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1842 - acc: 0.9380Epoch 00298: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.1832 - acc: 0.9380 - val_loss: 0.2696 - val_acc: 0.9217
Epoch 299/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1887 - acc: 0.9344Epoch 00299: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.1884 - acc: 0.9345 - val_loss: 0.2679 - val_acc: 0.9209
Epoch 300/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1832 - acc: 0.9373Epoch 00300: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.1834 - acc: 0.9374 - val_loss: 0.2687 - val_acc: 0.9221
Epoch 301/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1937 - acc: 0.9324Epoch 00301: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1931 - acc: 0.9324 - val_loss: 0.2682 - val_acc: 0.9219
Epoch 302/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1917 - acc: 0.9327Epoch 00302: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1907 - acc: 0.9333 - val_loss: 0.2678 - val_acc: 0.9197
Epoch 303/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1841 - acc: 0.9377Epoch 00303: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.1848 - acc: 0.9375 - val_loss: 0.2688 - val_acc: 0.9187
Epoch 304/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1887 - acc: 0.9332Epoch 00304: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1868 - acc: 0.9338 - val_loss: 0.2680 - val_acc: 0.9192
Epoch 305/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1778 - acc: 0.9380Epoch 00305: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1778 - acc: 0.9384 - val_loss: 0.2701 - val_acc: 0.9197
Epoch 306/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1771 - acc: 0.9391Epoch 00306: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1780 - acc: 0.9392 - val_loss: 0.2748 - val_acc: 0.9173
Epoch 307/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1883 - acc: 0.9362Epoch 00307: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1891 - acc: 0.9358 - val_loss: 0.2698 - val_acc: 0.9199
Epoch 308/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1832 - acc: 0.9374Epoch 00308: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1834 - acc: 0.9378 - val_loss: 0.2737 - val_acc: 0.9187
Epoch 309/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1761 - acc: 0.9386Epoch 00309: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1746 - acc: 0.9390 - val_loss: 0.2755 - val_acc: 0.9195
Epoch 310/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1721 - acc: 0.9403Epoch 00310: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1730 - acc: 0.9399 - val_loss: 0.2696 - val_acc: 0.9223
Epoch 311/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1826 - acc: 0.9392Epoch 00311: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.1825 - acc: 0.9395 - val_loss: 0.2717 - val_acc: 0.9182
Epoch 312/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1857 - acc: 0.9370Epoch 00312: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.1857 - acc: 0.9374 - val_loss: 0.2685 - val_acc: 0.9209
Epoch 313/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1773 - acc: 0.9419Epoch 00313: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.1754 - acc: 0.9425 - val_loss: 0.2666 - val_acc: 0.9185
Epoch 314/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1661 - acc: 0.9459Epoch 00314: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1671 - acc: 0.9458 - val_loss: 0.2758 - val_acc: 0.9199
Epoch 315/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1832 - acc: 0.9368Epoch 00315: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1831 - acc: 0.9366 - val_loss: 0.2732 - val_acc: 0.9172
Epoch 316/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1746 - acc: 0.9413Epoch 00316: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1764 - acc: 0.9408 - val_loss: 0.2676 - val_acc: 0.9217
Epoch 317/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1765 - acc: 0.9398Epoch 00317: val_loss did not improve
13776/13776 [==============================] - 1s 105us/step - loss: 0.1765 - acc: 0.9399 - val_loss: 0.2680 - val_acc: 0.9216
Epoch 318/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1817 - acc: 0.9383Epoch 00318: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1810 - acc: 0.9384 - val_loss: 0.2679 - val_acc: 0.9206
Epoch 319/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1799 - acc: 0.9370Epoch 00319: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1802 - acc: 0.9370 - val_loss: 0.2752 - val_acc: 0.9185
Epoch 320/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1811 - acc: 0.9395Epoch 00320: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1800 - acc: 0.9396 - val_loss: 0.2696 - val_acc: 0.9209
Epoch 321/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1821 - acc: 0.9381Epoch 00321: val_loss did not improve
13776/13776 [==============================] - 1s 105us/step - loss: 0.1818 - acc: 0.9380 - val_loss: 0.2669 - val_acc: 0.9192
Epoch 322/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1768 - acc: 0.9410Epoch 00322: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1771 - acc: 0.9405 - val_loss: 0.2713 - val_acc: 0.9194
Epoch 323/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1817 - acc: 0.9386Epoch 00323: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1817 - acc: 0.9387 - val_loss: 0.2761 - val_acc: 0.9206
Epoch 324/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1722 - acc: 0.9392Epoch 00324: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1722 - acc: 0.9390 - val_loss: 0.2721 - val_acc: 0.9207
Epoch 325/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1704 - acc: 0.9409Epoch 00325: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1697 - acc: 0.9414 - val_loss: 0.2718 - val_acc: 0.9226
Epoch 326/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1733 - acc: 0.9423Epoch 00326: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1738 - acc: 0.9419 - val_loss: 0.2768 - val_acc: 0.9192
Epoch 327/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1737 - acc: 0.9401Epoch 00327: val_loss did not improve
13776/13776 [==============================] - 1s 105us/step - loss: 0.1717 - acc: 0.9408 - val_loss: 0.2740 - val_acc: 0.9212
Epoch 328/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1823 - acc: 0.9355Epoch 00328: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1809 - acc: 0.9362 - val_loss: 0.2776 - val_acc: 0.9199
Epoch 329/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1750 - acc: 0.9410Epoch 00329: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1737 - acc: 0.9414 - val_loss: 0.2713 - val_acc: 0.9195
Epoch 330/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1731 - acc: 0.9377Epoch 00330: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1729 - acc: 0.9380 - val_loss: 0.2705 - val_acc: 0.9202
Epoch 331/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1751 - acc: 0.9418Epoch 00331: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1757 - acc: 0.9419 - val_loss: 0.2761 - val_acc: 0.9180
Epoch 332/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1719 - acc: 0.9408Epoch 00332: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1716 - acc: 0.9411 - val_loss: 0.2805 - val_acc: 0.9202
Epoch 333/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1713 - acc: 0.9419Epoch 00333: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1726 - acc: 0.9415 - val_loss: 0.2706 - val_acc: 0.9212
Epoch 334/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1776 - acc: 0.9391Epoch 00334: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1768 - acc: 0.9396 - val_loss: 0.2787 - val_acc: 0.9204
Epoch 335/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1723 - acc: 0.9394Epoch 00335: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1725 - acc: 0.9394 - val_loss: 0.2743 - val_acc: 0.9211
Epoch 336/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1772 - acc: 0.9411Epoch 00336: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1780 - acc: 0.9405 - val_loss: 0.2799 - val_acc: 0.9190
Epoch 337/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1680 - acc: 0.9422Epoch 00337: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1685 - acc: 0.9422 - val_loss: 0.2841 - val_acc: 0.9177
Epoch 338/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1688 - acc: 0.9429Epoch 00338: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1681 - acc: 0.9433 - val_loss: 0.2795 - val_acc: 0.9207
Epoch 339/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1740 - acc: 0.9403Epoch 00339: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1744 - acc: 0.9397 - val_loss: 0.2773 - val_acc: 0.9217
Epoch 340/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1691 - acc: 0.9412Epoch 00340: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1700 - acc: 0.9411 - val_loss: 0.2818 - val_acc: 0.9194
Epoch 341/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1668 - acc: 0.9401Epoch 00341: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1694 - acc: 0.9392 - val_loss: 0.2758 - val_acc: 0.9207
Epoch 342/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1768 - acc: 0.9421Epoch 00342: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1761 - acc: 0.9421 - val_loss: 0.2749 - val_acc: 0.9214
Epoch 343/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1726 - acc: 0.9407Epoch 00343: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1725 - acc: 0.9404 - val_loss: 0.2723 - val_acc: 0.9202
Epoch 344/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1685 - acc: 0.9427Epoch 00344: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1675 - acc: 0.9434 - val_loss: 0.2753 - val_acc: 0.9206
Epoch 345/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1732 - acc: 0.9402Epoch 00345: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1728 - acc: 0.9403 - val_loss: 0.2818 - val_acc: 0.9187
Epoch 346/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1726 - acc: 0.9415Epoch 00346: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1733 - acc: 0.9413 - val_loss: 0.2755 - val_acc: 0.9177
Epoch 347/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1671 - acc: 0.9418Epoch 00347: val_loss did not improve
13776/13776 [==============================] - 1s 105us/step - loss: 0.1685 - acc: 0.9413 - val_loss: 0.2762 - val_acc: 0.9194
Epoch 348/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1641 - acc: 0.9435Epoch 00348: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1655 - acc: 0.9432 - val_loss: 0.2729 - val_acc: 0.9214
Epoch 349/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1670 - acc: 0.9409Epoch 00349: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1675 - acc: 0.9410 - val_loss: 0.2736 - val_acc: 0.9211
Epoch 350/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1650 - acc: 0.9430Epoch 00350: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1660 - acc: 0.9427 - val_loss: 0.2776 - val_acc: 0.9199
Epoch 351/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1649 - acc: 0.9419Epoch 00351: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1647 - acc: 0.9419 - val_loss: 0.2736 - val_acc: 0.9214
Epoch 352/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1742 - acc: 0.9396Epoch 00352: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1744 - acc: 0.9398 - val_loss: 0.2719 - val_acc: 0.9223
Epoch 353/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1654 - acc: 0.9435Epoch 00353: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1651 - acc: 0.9436 - val_loss: 0.2800 - val_acc: 0.9201
Epoch 354/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1716 - acc: 0.9401Epoch 00354: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1725 - acc: 0.9396 - val_loss: 0.2802 - val_acc: 0.9201
Epoch 355/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1728 - acc: 0.9419Epoch 00355: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1760 - acc: 0.9411 - val_loss: 0.2680 - val_acc: 0.9211
Epoch 356/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1731 - acc: 0.9422Epoch 00356: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1733 - acc: 0.9421 - val_loss: 0.2755 - val_acc: 0.9207
Epoch 357/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1694 - acc: 0.9415Epoch 00357: val_loss did not improve
13776/13776 [==============================] - 1s 105us/step - loss: 0.1693 - acc: 0.9420 - val_loss: 0.2753 - val_acc: 0.9202
Epoch 358/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1652 - acc: 0.9416Epoch 00358: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1648 - acc: 0.9417 - val_loss: 0.2770 - val_acc: 0.9192
Epoch 359/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1575 - acc: 0.9467Epoch 00359: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1585 - acc: 0.9464 - val_loss: 0.2738 - val_acc: 0.9239
Epoch 360/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1636 - acc: 0.9426Epoch 00360: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1636 - acc: 0.9427 - val_loss: 0.2724 - val_acc: 0.9207
Epoch 361/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1565 - acc: 0.9469Epoch 00361: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1579 - acc: 0.9467 - val_loss: 0.2724 - val_acc: 0.9209
Epoch 362/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1605 - acc: 0.9460Epoch 00362: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1620 - acc: 0.9452 - val_loss: 0.2757 - val_acc: 0.9216
Epoch 363/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1647 - acc: 0.9452Epoch 00363: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1643 - acc: 0.9458 - val_loss: 0.2764 - val_acc: 0.9204
Epoch 364/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1577 - acc: 0.9473Epoch 00364: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1569 - acc: 0.9475 - val_loss: 0.2815 - val_acc: 0.9209
Epoch 365/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1660 - acc: 0.9429Epoch 00365: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1653 - acc: 0.9432 - val_loss: 0.2752 - val_acc: 0.9214
Epoch 366/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1698 - acc: 0.9440Epoch 00366: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1688 - acc: 0.9440 - val_loss: 0.2798 - val_acc: 0.9201
Epoch 367/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1617 - acc: 0.9445Epoch 00367: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1616 - acc: 0.9445 - val_loss: 0.2797 - val_acc: 0.9180
Epoch 368/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1621 - acc: 0.9461Epoch 00368: val_loss did not improve
13776/13776 [==============================] - 1s 106us/step - loss: 0.1615 - acc: 0.9461 - val_loss: 0.2739 - val_acc: 0.9206
Epoch 369/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1662 - acc: 0.9439Epoch 00369: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1653 - acc: 0.9440 - val_loss: 0.2772 - val_acc: 0.9199
Epoch 370/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1653 - acc: 0.9438Epoch 00370: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1647 - acc: 0.9439 - val_loss: 0.2763 - val_acc: 0.9197
Epoch 371/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1517 - acc: 0.9487Epoch 00371: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1548 - acc: 0.9473 - val_loss: 0.2739 - val_acc: 0.9221
Epoch 372/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1630 - acc: 0.9441Epoch 00372: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1650 - acc: 0.9437 - val_loss: 0.2736 - val_acc: 0.9223
Epoch 373/400
13056/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1656 - acc: 0.9419Epoch 00373: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1658 - acc: 0.9420 - val_loss: 0.2716 - val_acc: 0.9226
Epoch 374/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1677 - acc: 0.9425Epoch 00374: val_loss did not improve
13776/13776 [==============================] - 1s 105us/step - loss: 0.1672 - acc: 0.9428 - val_loss: 0.2764 - val_acc: 0.9195
Epoch 375/400
13568/13776 [============================&gt;.] - ETA: 0s - loss: 0.1609 - acc: 0.9452Epoch 00375: val_loss did not improve
13776/13776 [==============================] - 1s 106us/step - loss: 0.1601 - acc: 0.9455 - val_loss: 0.2809 - val_acc: 0.9211
Epoch 376/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1614 - acc: 0.9442Epoch 00376: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1635 - acc: 0.9439 - val_loss: 0.2747 - val_acc: 0.9216
Epoch 377/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1618 - acc: 0.9446Epoch 00377: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1613 - acc: 0.9448 - val_loss: 0.2732 - val_acc: 0.9245
Epoch 378/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1636 - acc: 0.9451Epoch 00378: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1639 - acc: 0.9452 - val_loss: 0.2751 - val_acc: 0.9226
Epoch 379/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1583 - acc: 0.9458Epoch 00379: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1584 - acc: 0.9461 - val_loss: 0.2744 - val_acc: 0.9238
Epoch 380/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1624 - acc: 0.9455Epoch 00380: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1624 - acc: 0.9453 - val_loss: 0.2733 - val_acc: 0.9223
Epoch 381/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1573 - acc: 0.9470Epoch 00381: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1579 - acc: 0.9468 - val_loss: 0.2779 - val_acc: 0.9216
Epoch 382/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1583 - acc: 0.9450Epoch 00382: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1583 - acc: 0.9454 - val_loss: 0.2775 - val_acc: 0.9199
Epoch 383/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1650 - acc: 0.9446Epoch 00383: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.1652 - acc: 0.9441 - val_loss: 0.2758 - val_acc: 0.9226
Epoch 384/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1523 - acc: 0.9484Epoch 00384: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1520 - acc: 0.9485 - val_loss: 0.2785 - val_acc: 0.9231
Epoch 385/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1574 - acc: 0.9465Epoch 00385: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.1580 - acc: 0.9462 - val_loss: 0.2772 - val_acc: 0.9221
Epoch 386/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1684 - acc: 0.9445Epoch 00386: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.1673 - acc: 0.9450 - val_loss: 0.2822 - val_acc: 0.9206
Epoch 387/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1551 - acc: 0.9472Epoch 00387: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1560 - acc: 0.9466 - val_loss: 0.2771 - val_acc: 0.9228
Epoch 388/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1555 - acc: 0.9470Epoch 00388: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1545 - acc: 0.9472 - val_loss: 0.2797 - val_acc: 0.9224
Epoch 389/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1588 - acc: 0.9474Epoch 00389: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1577 - acc: 0.9477 - val_loss: 0.2785 - val_acc: 0.9229
Epoch 390/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1599 - acc: 0.9448Epoch 00390: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1601 - acc: 0.9450 - val_loss: 0.2773 - val_acc: 0.9226
Epoch 391/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1602 - acc: 0.9464Epoch 00391: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.1596 - acc: 0.9463 - val_loss: 0.2804 - val_acc: 0.9233
Epoch 392/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1593 - acc: 0.9453Epoch 00392: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1589 - acc: 0.9453 - val_loss: 0.2748 - val_acc: 0.9238
Epoch 393/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1550 - acc: 0.9473Epoch 00393: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1543 - acc: 0.9474 - val_loss: 0.2845 - val_acc: 0.9209
Epoch 394/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1608 - acc: 0.9459Epoch 00394: val_loss did not improve
13776/13776 [==============================] - 1s 104us/step - loss: 0.1604 - acc: 0.9456 - val_loss: 0.2781 - val_acc: 0.9195
Epoch 395/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1525 - acc: 0.9461Epoch 00395: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1538 - acc: 0.9459 - val_loss: 0.2748 - val_acc: 0.9219
Epoch 396/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1513 - acc: 0.9494Epoch 00396: val_loss did not improve
13776/13776 [==============================] - 1s 103us/step - loss: 0.1519 - acc: 0.9490 - val_loss: 0.2765 - val_acc: 0.9221
Epoch 397/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1547 - acc: 0.9474Epoch 00397: val_loss did not improve
13776/13776 [==============================] - 1s 101us/step - loss: 0.1552 - acc: 0.9476 - val_loss: 0.2816 - val_acc: 0.9197
Epoch 398/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1670 - acc: 0.9413Epoch 00398: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1677 - acc: 0.9416 - val_loss: 0.2781 - val_acc: 0.9231
Epoch 399/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1531 - acc: 0.9457Epoch 00399: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1532 - acc: 0.9454 - val_loss: 0.2749 - val_acc: 0.9238
Epoch 400/400
13312/13776 [===========================&gt;..] - ETA: 0s - loss: 0.1499 - acc: 0.9499Epoch 00400: val_loss did not improve
13776/13776 [==============================] - 1s 102us/step - loss: 0.1504 - acc: 0.9498 - val_loss: 0.2761 - val_acc: 0.9228
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[28]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># list all data in history</span>
<span class="nb">print</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="c1"># summarize history for accuracy</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;model accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;validation&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># summarize history for loss</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;model loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;validation&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>dict_keys([&#39;val_loss&#39;, &#39;val_acc&#39;, &#39;loss&#39;, &#39;acc&#39;])
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Test-the-Model">Test the Model<a class="anchor-link" href="#Test-the-Model">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[29]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">&#39;weights.hdf5&#39;</span><span class="p">)</span>
<span class="n">y_predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">example</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span> <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">20</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">1</span><span class="p">)]</span>

<span class="n">accuracy</span> <span class="o">=</span><span class="mi">100</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_predictions</span><span class="p">)</span><span class="o">==</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y_predictions</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;test accurac : </span><span class="si">%.4f%%</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>test accurac : 91.8909%
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="result-visualization">result visualization<a class="anchor-link" href="#result-visualization">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[30]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span>
                          <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Confusion matrix&#39;</span><span class="p">,</span>
                          <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function prints and plots the confusion matrix.</span>
<span class="sd">    Normalization can be applied by setting `normalize=True`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="n">cm</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Normalized confusion matrix&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Confusion matrix, without normalization&#39;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">cm</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">tick_marks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>

    <span class="n">fmt</span> <span class="o">=</span> <span class="s1">&#39;.2f&#39;</span> <span class="k">if</span> <span class="n">normalize</span> <span class="k">else</span> <span class="s1">&#39;d&#39;</span>
    <span class="n">thresh</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">/</span> <span class="mf">2.</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">format</span><span class="p">(</span><span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">fmt</span><span class="p">),</span>
                 <span class="n">horizontalalignment</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span>
                 <span class="n">color</span><span class="o">=</span><span class="s2">&quot;white&quot;</span> <span class="k">if</span> <span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">thresh</span> <span class="k">else</span> <span class="s2">&quot;black&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True label&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted label&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="c1">#p.savefig(&#39;conf.png&#39;)</span>

<span class="c1"># Compute confusion matrix</span>
<span class="n">cnf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_predictions</span><span class="p">))</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Plot non-normalized confusion matrix</span>
<span class="n">pl</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cnf_matrix</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                      <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Confusion matrix, without normalization&#39;</span><span class="p">)</span>

<span class="c1"># Plot normalized confusion matrix</span>
<span class="n">pl1</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cnf_matrix</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                      <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Normalized confusion matrix&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">pl</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;confusion.png&#39;</span><span class="p">)</span>
<span class="n">pl1</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;confusion1.png&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Confusion matrix, without normalization
[[620  19   9   2   1   1  27   0   0   0   3   5]
 [ 12 661   9   6   1   1  35   3   5   1   2   3]
 [ 16  11 580  15  22   4  11   4  17   0  14  11]
 [  0   5   7 664  11   1   0  20   1   3   8   1]
 [  2   3  18   7 658   0   1  21   3   2   3   0]
 [  3   4  13   7   1 631   3   2   9   0   1  16]
 [ 17  57   7   4   0   3 645   4   2   0   3   3]
 [  1   6   4  23   6   3   1 640   3   0   3   1]
 [  2   4  15   3   3   2   1   0 701   0   1   1]
 [  0   0   0   0   0   0   0   0   0 612   0   0]
 [  2   5  13   8   2   0   3   2   3   0 688   0]
 [  3   1   6   0   1   2   1   0   3   0   0 651]]
Normalized confusion matrix
[[ 0.9   0.03  0.01  0.    0.    0.    0.04  0.    0.    0.    0.    0.01]
 [ 0.02  0.89  0.01  0.01  0.    0.    0.05  0.    0.01  0.    0.    0.  ]
 [ 0.02  0.02  0.82  0.02  0.03  0.01  0.02  0.01  0.02  0.    0.02  0.02]
 [ 0.    0.01  0.01  0.92  0.02  0.    0.    0.03  0.    0.    0.01  0.  ]
 [ 0.    0.    0.03  0.01  0.92  0.    0.    0.03  0.    0.    0.    0.  ]
 [ 0.    0.01  0.02  0.01  0.    0.91  0.    0.    0.01  0.    0.    0.02]
 [ 0.02  0.08  0.01  0.01  0.    0.    0.87  0.01  0.    0.    0.    0.  ]
 [ 0.    0.01  0.01  0.03  0.01  0.    0.    0.93  0.    0.    0.    0.  ]
 [ 0.    0.01  0.02  0.    0.    0.    0.    0.    0.96  0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.    0.  ]
 [ 0.    0.01  0.02  0.01  0.    0.    0.    0.    0.    0.    0.95  0.  ]
 [ 0.    0.    0.01  0.    0.    0.    0.    0.    0.    0.    0.    0.97]]
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAUMAAAEYCAYAAADGepQzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzsnXd4VMX6xz9vOgmEFggkIYEkdAQUVEBRrtcOikgVsSsq1nvFjnT1KrarXsWCYqGEKgr4Q+9VBAVE6QSk1wSld0h9f3+ck2R3s9ns5myyAc73eebJ2Z15Z96Zc3ZyZs7M54iqYsuWLVvnuoIC7YAtW7ZsVQbZnaEtW7ZsYXeGtmzZsgXYnaEtW7ZsAXZnaMuWLVuA3RnasmXLFmB3hueURGS4iHxpHieKyHERCfZzGdtF5Ep/5ulFmQ+KyF9mfWpbyOe4iCT707dASUTSRaRLoP04k2R3hn6U2RH8JSJRDt/dKyLzA+iWW6nqTlWtqqp5gfbFikQkFHgDuNqsz4Gy5mXab/Wfd/6XiIwXkdGlpVPVlqo6vwJcOmtkd4b+VwjwmNVMxJB9fkpXLBABpAfakcogEQkJtA9nquwfm/81BhgsIjXcRYpIJxH5TUSOmH87OcTNF5EXReQX4CSQbH43WkQWmcO4b0SktohMEJGjZh4NHfL4t4jsMuOWiUjnEvxoKCIqIiEi0tHMuyCcFpHtZrogEXlGRLaIyAERmSIitRzyuU1Edphxz3tqGBGpIiKvm+mPiMjPIlLFjLvRHNodNuvc3MFuu4gMFpHVpl2aiESISBNgg5nssIj84Fgvl3a91zxOFZGfzHz2i0iaQzoVkVTzuLqIfC4i+0x/hxT8cxKRO03fXxORQyKyTUSu81Dv7SLypOn/CREZJyKxIvKtiBwTkf+KSE2H9FNF5E/TxwUi0tL8fiBwK/BUwbXgkP/TIrIaOGGe08LpChGZKyKvO+SfJiKfeDpX56RU1Q5+CsB24EpgBjDa/O5eYL55XAs4BNyGcQd5i/m5thk/H9gJtDTjQ83vNgMpQHVgHbDRLCcE+Bz41MGHAUBtM+4J4E8gwowbDnxpHjcEFAhxqUNBmS+bnx8HlgAJQDjwATDJjGsBHAcuM+PeAHKBK0ton/+YeccDwUAn064JcAK4yiz/KbPOYQ7tuhSIM9twPfCAu3q4q5dZ5r3m8STgeYwbgQjgUod0CqSax58Ds4BqZp4bgXvMuDuBHOA+sx4PApmAeLgulmDcxcYDe4HlwPlm/X8Ahjmkv9ssNxx4C1jpEDce89pyyX8l0ACo4ngtmsf1zDKvwOhMtwLVAv17qWwh4A6cTYGizrAVcASog3NneBuw1MVmMXCneTwfGOkSPx943uHz68C3Dp9vcPyxuPHpENDGPB5O6Z3h+8AcIMj8vB74u0N8fbMjCAGGApMd4qKAbNx0hmbnc6rAF5e4F4ApLmkzgC4O7TrAIf5VYKy7erirF86d4efAh0CCGz8USMXo4LKAFg5x9zucxzuBzQ5xkaZtPQ/Xxa0On6cD7zt8fgT4qgTbGmbe1c3P43HfGd7t7lp0+HwzsAvYj8M/ADsUBXuYXA5S1bXAbOAZl6g4YIfLdzsw7hYKtMtNln85HJ9y87lqwQcReUJE1ptDrMMYd5Mx3vgtIvcDXYD+qppvfp0EzDSHr4cxOsc8jLucOEd/VfUEUNIDjBiMO7EtbuKc2sUsexfO7fKnw/FJHOrso54CBFhqDsvvLsHXMJzPlet5KvRHVU+ah5588uocikiwiPzLnJY4itGpFfjkSe6uG0fNxujkN6jqz6WkPSdld4blp2EYwyjHH1AmRufiqESMu6AClRkjZM4PPg30AWqqag2MO1Tx0nYU0F1VjzhE7QKuU9UaDiFCVTOAPRhDs4I8IjGG6O60HziNMdx3lVO7iIiY+Wa4SVuaTph/Ix2+q1dwoKp/qup9qhqHcbf3XsE8oYuvOTifK9fzVF7qD3THGGFUx7jThaJzWNL1Udp18yLGP7L6InKLRR/PStmdYTlJVTcDacCjDl/PBZqISH9zkrsvxrzbbD8VWw1jzm4fECIiQ4Ho0oxEpIHp6+2qutEleizwoogkmWnriEh3M24a0E1ELhWRMGAkJVxT5t3eJ8AbIhJn3gF1FJFwYArQVUT+LsZSmScwhqmLfKq9Uc4+jE5rgFnG3Th0wCLSW0QSzI+HMDqRPJc88kyfXhSRambd/wl86as/ZVA1jLofwOjQX3KJ/wvwaS2kiFwG3AXcboZ3RCTes9W5J7szLF+NxJhHA0CNNXDdMH7sBzCGbN1Udb+fypsHfIsx2b8D406stOETwN8x7p6mSdET5YKlKv8Gvga+E5FjGA8CLjbrkw48BEzEuEs8BOz2UM5gYA3wG3AQeAVjbnIDxoOfdzDuym4AblDVbC/r7ar7gCcx2rglzp3qhcCvInLcrNdjqrrNTR6PYNxlbgV+NutYEU9gP8c4dxkYD8uWuMSPA1qY0xZflZaZiESbeT6sqhnmEHkc8Kl5B27LlJiTq7Zs2bJ1Tsu+M7Rly5Yt7M7Qli1bZ5hE5BMR2Ssia0uIFxF5W0Q2mwvdL/AmX7sztGXL1pmm8cC1HuKvAxqbYSDG2tlSZXeGtmzZOqOkqgswHsCVpO7A52poCVBDROqXlq+9qdtLSVhVlSo1S09Ygtqkxlor35K1dftAKt/iM76gM7nyWFh46gft3LGd/fv3+60Fg6OTVHNPeUyjp/alY6yEKNCHqvqhD8XE47yKYrf53R7PBVeCbTCVPFyrqhs2bdmmQ8Yt1Ihr33QKTW7/WH9YsUNXb92rP63apSkDPiqMu+e1/9NxX0zRRsmpmpycosNGvqRHTuU5hb2HT2qPnr21UXKKtmt/ka7+Y4seOZWnPyxYrOe1bqMNGzXSsLAwjY2tpyNGv6THs/KdwoGjp/TmXn00OTlF2194kaZv2KrHs/J1R+Y+7Xx5Fw0PD9fq1WtockqKjnzxZT2Vo07h8PHT2rN3H01OMez/2LStMG7k6Je0Xv36GhoaqvXq1/fZ/rbb79TQ0FANDQ3V2++4q+xl16uvQ0e8qIdO5DqFPw+ecGi7C3XVus1O8e999KmKiNaqXbvC6+53+9Ev6cnsfKdw6Ngp7dmryH79xq2FcQNuv6Ow7W+74y6vbXft2aeXXd5Fo6Ki9P4HB+n5F7RTf/6epEodjWj7kMcA/F5aPhiL0deWEDcH5z3n/wPalZanPUz2rGAMuMB1LS+6gt5dmtIssZZTgpfv7cyE/63nokETeGniEkbeeQkANauG80zfCxn4wCCONbmd9PR0pk+dzB/r1znZfz7+E2rUrMnK9I0MeuQxhj1v7OBr3rIV/1uwGBDmfvcjuXm5TJk8ifUu9p99Oo4aNWqwev0mHnr0cV4w7SMiInhuyDCqVqvGdV27smL1OqZOnsT6dc724z8ZR80aNUn/YzOPPPYPnn/uaQDWr1vHlMmTiIiIYM633xMREWGU76X92jVrSJs8kd+Wr+L3FWuYPGkCa9euKVPZ4RERbtvui88+oXqNmixfs4EHH36c4S88WxiXl5fH4Mce5u9XXc3j/xxcoXUvF/s0N/afjqNGzRqsXb+JRx59nCHPPVPY9lMmT2LpslX8tnw1aZMmkO7a9iXYRkREMHT4SF56ZQzlIhEICvYcrGs3DjujMCAjmaUZ2Z2hZ12EQU/ZmpOTw9SfNtKtg/NusmaJtZm/0rgj/2nVbrp1NDYHXNWuIePS5qIRtTlONYJCwujVpy9zZn/tZD939iz633o7ADfd3Iuf5v+AqhIZGcmqFctJTkmhTmwsQUFB9OzVhznfzHKyn/PN19x62x0A9Li5F/N//B+qSlRUFGFhYcTWjSU6ujphYWH07tuP2S72s7+ZVWh/c89ezP/BsJ/9zSw6dLqE1NTGdL78clJTG9OhYyev7T8Y+x4NEpNo2qw5TZo2pUFiIh+8/16Zyk5JSeWiDh2Z69J2387+mltuvQ2A7j16FrYdwDtvvU7tmBjatb+IkJCQCq17RdnP+eZrBhSc+55F5/7Dse/RIDGRps2aFbX92Pe8so2KiqLTJZcSERFBuUmCPAfr+hq43Xyq3AE4oqqeh8jYnWFpcpp7yNh/jPjaUU4J1mzdx02XGFtbu3dKIToynFrVIoiLiWLLth1IFQNrmK9Kg/h49mQ4b2/dk5lJfILxTywkJITo6OocPGCwDhYumM+y336jU/s2/Pvd92mQmEimi31mZgYJDvbVo6tzwLTPzMygZq2iO9n4+AQy3Nk3cCi/umGfkZFBWGhoYd7x8QmEhIR4bb9j+zbi4uIK09WvH8eO7du8snUtOy4+gZCQUPbsyXSxd992J06c4MvPP+GSSy8LSN39bR8XH09ISCiZmS72GRnO9S9o+x3biatftNuuXv367Ni+3SvbCpGI51CquUzCoD01FZHdInKPiDwgIg+YSeZi7BzaDHwEDPLGrTPuAYqIDAeOq+prFVGc6xeuk9nPfryQNwf9jQFXteCXNRlk7D9Gbl4+ghTepThl6HKyPaVJTknlxh4388hj/+TB++7invvu98neUvlF8y0l2nqyd/+98/9en8pGvDgbhv2/Rg/n6mu7cvzYMY/+l1fdy8XeiPTK3up1V74Sy0NhVfUImlCjcg/5mq99Z+hZTnMP8THVyDxwwinBnoMn6Dd6Nh0fnsiwz4wtsEdPZpOx/zipyQ3RU4cBCBJhV0YG9RzulsD4r5+x27j5zM3N5ejRI4V3c/HxCWTs3kXTZs2JjIpi5Yrl1Hexj49PYLeD/ZGjR6jlYH/oYNEKhIyM3U53a4X2uxzKP2LYxyckkJObW5h3RsZucnJyvLZv2CiZzMyiO7k9ezJJTEryyta17MyM3eTkZlOvnvPqiLg49233++9LmT41jalpE3n/P28z5l8vMX3a1Aqru7/tMzMyyMnNoX59F/uEBOf6F7R9w0Zk7im6i/xzz57ibV+CbblLqIhhcpl0RnSGIvK8iGwQkf8CTc3v2orIEnOF+UwRqSkidUVkmRnfRgyMe6L5eYuIRIrxQp23xcDobxWRXh6K/g1j4Waj0NBQel/ehDlLnHF8taMjCv9hP9n3Qj77zuAbfL9sO/f0vQ45vZ+qcoz83GymTUnj+q43ONlf3/VGJk74HICvZkzjssv/hoiwffs2Wrc9ny2bN7Pol5/ZuOEPFvw0n+u73ehs3+0GJnzxGQAzZ0zj8i5XFP6Hb9f+Qvbu/YujR4+SnZ3N1LTJdHWx79rtxkL7GdOncfnfDPuu3W5kyaJf2LRpIz8vWMCmTRtZsniR1/b3DXyAnTt2sGHDH2zcsIGdO3cy8IEHy1T25s2bWLpkMde5tN21XW9g0oQvAJg1c3ph2337/U+kb9xObL363HLrbfxj8FPs2rmjwupeUfbXd7uBLwvO/fSic3/vwAfYuWMnGzdsKGr7+x/0yrb8VcoQOZDsiEqwdKW0R+jtMEgnkRg4qs0Y9JPVwOVmmpHAW+ZxupnuYYzO7FYMLt1iM348MBXjH0ELHIjFbsoe2KdPn03btm07vXnLVh06/meNuPZNfXHCYu05bJZGXPum3jL6G920+6Bu3HVQP/l2jUbf8Hbh0pqBb8zTjz9P04aNUrRRcrIOGT5Kj5zK06eeHaKTps7UI6fy9K9DJ7R7j57aKDlFL2h3oa5ct0mPnMrTD8aN12bNW2hSw0YaFh6udWNjdeiIUXo8K1+ffm6Ipk37So9n5ev+Iyf1ppt7abK5vGTN+s2Fy24Sk5K0atWqKiIaHBysgx5+VE/lqD77/As6dcYsPZWjeujYKe3Rs5cmpxj26zZsKVzeMXzkaI2tV09DQ0M1tl49HT5ytE/2/QfcpiHm8o5bB9xe9rJj6+nzw0bqoRO5+uQzz+uEKTP10Ilc3XPguEPbtdcVazc6La1Jm/611qxVS2vVquWz71br7m/7YSNG6cnsfH3muSE6ZfpXejI7Xw8ePak9bi6yT/9jc+HSmf63FrV9/wG3+WSbmJSkNWvW1KioKA0NDVUciN+Wl9ZE1dOITs95DHixtKY8QqWn1ojI40AtVR1qfn4DA1h6j6oW3PWlAFNV9QIR+QjjHSR3Ybzv4lpgIdBaVZ8SkfHA96o6wbQ9pqrVSvMjqHoDDe/0RJnr8dfMR0tP5EHBFlcOW7UPpE5nW3ubaUSYX18NXeEK5G/0kg4XsnzZ7367eIKqxml423s8pjn9y+hlqtreX2V6qzNimIxvi/AXAp0x7gZnAW2AS4EFDmmyHI7P3F7Clq0zTUKlHSafCZ3hAqCHGK+ZrIYB/jwBHJKi12DeBvzkkH4AsEkNuvJB4Hrgl4p125YtW8UlEBTiOQRIlX5pjaouF+PdtisxCMALzag7gLHmeze2YgyLUdXt5kRwwZ3gzxhvQjtUoY7bsmXLvSrplE2l7wwBVPVFjBfauKpDCekTHY5fwuE9Eqp6p0vasr5lzZYtW76qYGlNJdQZ0RnasmXrbJH1RdflJbsztGXLVsWqkr6Hyu4MvVTb1FgWfFX25TF1enoF2y1R+2d4tb2yROVbhAIGBXCeJ9/i0pJALx+zupjZqn2g619MlXSYXDm9qkQSkWtFZEN6+lpeH/NKsfisrCzuGNCPNi2a8LfOHQs3xP/w3+/p3PFCnnh0EPv37GTN+30Y3Kv4qxga1KnK/73UncX/7sPSd/pyTXtj21RoSBAfPHYFb/auQ+y6N2ndojGvj/mX2/Jvv7UfrZs3psulHQrLP3DgANddfQW1o6sQX7cW5zVvzGse7M9r3pjL3djXiq5CXN1atGyWyphX3dsP6N+Xls1S6dzpYicgwMB77iI6MozoyDDuv/dun2zHvPIyjRLjiI4Mo0VqIm+9/qpb+3tu70/71s24qksndu4w7Hfu2E58TDUuaNWE+rWiiI2pwWsl+H5b/360at6Yyy7pUKz85KR4qkeFk5wU75P9gQMHuPaqK6hZrQr165St7Rzr3ygxzpK9r/4X2Ldq3ph16WsRkWuKGZdVFYPwKpPsztCDRKSQZ9i8eQumTSmBR1ijJqvWbeShRx5j6BCDC1c7JoYp02fx/tj3OXL8NC1atKD35Y1p1sCZlv103/ZMX7iZjo9N4fZXv+PfDxqklbuvaUF+Xh6DBj1E9nn3sXZtOlPTJpeZZ7hslWf7Nes38XAJ9teXgYfoT55hRAk8wy8/+4QaNWrw++o/ePChxxjxwnOFcUkNkxERFi9bw67MvUbdveQBrl+3jqlpk4mIiGD2t9/5zBOMiIjg+RfMtutW8SxJqzzE9evWMW1KGstWriUltTHAe+ZvwT+y1xmekboIY7ve1qCgIHr27svsb5yZenO+mUX/AUU8wvk/Gky9Nm3PJz4+jnyF5NTG5GSdZOr8DXTr0MjJXhWiI8MAqB4Vxp6DBgiiWYNaTJ39PyQyhkN5VQ0eYZ++lniGvfr0dcPUKx8eoj95hskpqVx0cUe+nfONk/23c76hn8kzvLFHTxY48AxPnzpFo+QUGjZKLrHuJTH9Zn8ziw4dOxnlX+Y7T7Cw7WIDw5K0ykOc/c0sevXpS3h4OOHh4WBsgb0Iv8i+MzxT5cQzjI+PZ48rUy4zs0SeoEljYtbM6QRFNyDj4MliPMQXJy6l39+asnn8Hcwc3o1/jjWWUa7Ztp8WsUEERdYiKbYaQQIJ8QmWeYauPEVX+2g/8RD9yzOMJzQkpFjb78nMJM6FyVfAgtyzJ4OVy5dxwzVX8MvPC4mPT/CaB5iZmUFoaCjxCQmF5fvCEyyIq1kzMCxJqzxEx2vCVME7RKzLptacsSp2z+4rF+7woUMMff5ZQs7rZ6Z3Ttvn8sZ8+b8/SL3zM3oMn824J65EBD77fj0Hjp7mhg6NGHNfZ/LU9/JL882tQz7al5TG/fdl5xki4nXZsfXq89pb79L1xu6M+tcY7rz9Vk6dPuWT764xgv/b3pf6iw/1d2vvQ/klPHDx01MYsTvDM1ROPMOMjAzquTLl4uNL5Anu/Wsvf6xP54Nx4wmKqkN8TFUyDzrzEO+4qgXTF24G4Nc//iIiLJiY6Crk5SsTFv3JrB+W0Wf0XATYlbHbMs+wOE/R2f6on3iI/uUZZpCTk1Os7ePi48l0YfLVrFWL8PBwmjRtRsbu3bQ9vx3JySmsWb3aax5gfHwCuTk5ZOzeXVS+DzzBgrhDhwLDkrTKQ3S8pkx59Q4Rr2UPkwMjEXlBRP4Qke9FZJKIDHbHQizB/DegsYg0ys/PZ/rUNLp2c+ERdruRiV8W8Qgv72Iw9Q4fPkz3G66n7fnn06nTJYSGBNH7ssbM+XW7k/2ufcfo0sYYjjVNqElEaAj7jpyiSngIUXVT0BP76NAwlOxsk4dogWc4bUqaG6Ze+fAQ/ckz3LJ5E0t/Xcx113dzsr/2+m5MNnmGX8+cTmeTZ7h/3z5at72ArVs288vCn9i0aSMLF/zkNQ+wa7cbWbJ4UZl5goVt91dgWJJWeYhdu93ItClpZGVlkZWVBQbTcyn+UiV9gBJQVmF5B6A9xp7mKkA1YBMeWIhu7AeaNqeDg0N06PBReux0nj797BCdPG2mHjudp/sOn9Cbbu5ZyBNcvW6THjudpy8MG6mRkZH64KCHdNv27bp56zYd+sl8jej6rr44can2HDFbI7q+q20fmKCL0jN11dZ9unLLPu06ZJZGdH1Xm9z1mW7YdVDHjp+s8YmNtFGjZMs8wwcfekRPZJlcu2lf6YmsfD1wxOTamfZr12/WE1n5esIPPER/8gyfGzpSDxzP0cFPP69fps3QA8dzNGP/Mb3xJoNneH679rpszQY9cDxHP/0yTZs2a6GJSUkaFhausbG+8wCHjRhliScYaJakVR7isBGjtFFysoaHhytwnb9+k1IjSSNu+shjwOYZ+l8mC7Gmqg4zP3tkIXrK64J27XXBorL/cwz0omur/28Duej6ZFauJfsqAeYZVgxBumRZ+Y37nWdYs6FGXDHUY5pTM+4JCM/wbN+BUjn3/diydY7K3YOcyqKzfc7wZ+AGEYkQkapAVzyzEG3ZslWeEi9CgHRW3xmq6m8i8jWwCoOF+DvGMNktC9GWLVvlLSEoqHLeg53VnaGp11R1uNnxLQBeV9WVlMBCtGXLVvmqsg6Tz4XO8EMRaQFEAJ+p6vJAO2TL1rksuzMMkFS1f6B9sGXLliERQWzs/7ktq0tjEu+bbMl+x4d9Ldnn5lpbghUaXPYfQHhoYJfG5OZZrHuItR9/bl6+JfvKdidW2fwpUOWcyaxEssozbNkkmZjqkTRumOATj/CH/37PpR3a889HHmRf5g5+e+lqHuvavJh9fK1IvnrmCn4ceS0LRl/Hla3rA9ClZT3+N+IaXroylGoLX6BNiyaW/G+a3IA3SrC/c0A/2rZswhWdO7LDZAou+20pbVqkUqd6JHWqV+HuO29za+sND7B1c99YjgVt16JxI2pHVyElKcESj7BNyyYe696mpdl2Zt1/+N/3XNbpQlo1TaZOjUiSk+It8QgbN0rw+dy1admE2tFVqB0dyUMP3Odz2110QWsu7dCeY8eOFrO1qoJ91iWFQMnuDD3IKs9w0pQZBAcHM2nKDBT1iUdYOyaGqTO+NniIJ07TvEULbu6QRNO4aCf7J7q3ZNbSnfxt6P9x33u/MOZ2Y63qgWNZ9H/9RwYNeoja1zxL+rp0S/6j6pYp+Pn4T6hRsyYr0zcy6JHHGGb636SZ0XEvXbmWZav/YGraZNauWe1k6y0P8PdSWIzu2m7y1JmG71NnFrV9GXmEv61YyzQPdV+VbrRdQd1r145hYtoMgoKDmZg2A1W1xjMMj2Bq2iSvz12NmjXJycnh95Xp/LDgFyZ88ZnP193S5av5YNx4pw7aLxKQIPEYAiW7M/QsSzzDjIzdJKekcOXV15CdlcVNN/fymkfoyENMSW1CzumTzFi8jesuSHCyV4VqEaEAVKsSyp+HTwGwZuchdm9ZS1C1WLafjCQ8LMyS/1nZWXTv0ZM5s53t586eRf9bi+x/MpmC69PXkpySSqNGyeRrvvHDnv2NS9ne8wB7+cByLPI9lasc2t4Kj7Bn777F6j5n9ixucaj7/Pkln/sePYuX7y2PMCU1lYs7dvL63OXk5NCkSVMaJSfTuk1bQkNDmTVzhtdtVwADadGiJfn5+YhIOH6UP+4MC0ZsIrJZRJ5xE58oIj+KyAqTQXB9aXnanaFnWeIZ7sk0eHGzZk6ndZvzSUpK8olHKBid3VczpxNcK4k9h7OoX7OKk/2rM9fQu1ND1rzZnbQnuvDMl8sK4/TkIYKjanND+wbkqXX/E5OSivEQ92RmOjPxog2mYGZmBuHhEVx8wXl0at+GO+66mz//3ONcto88QF9ZjgkNEvhq5nTatDXb3gKPMC4+vlj5e1zarqDuBW2XYLadUX5DCzzCBLc8x5LOXcF5A4OlmdSwEXv/+tPrtivQVzOnUyUyElXNwk8SPHeE3nSGjiM2oAVwi7lixFFDgCmqej7QD3iPUlRpOkMRuVNE3g20Hy6yxDNUVQ4fNniGb/9nrM/2gGH/3DNEdbzXTO+c9uYOSUz6eRvn/WMWfV+fz/sDOxaBP1SpHhXGsL5tyMpRS/6/9e77PtmrKjF16vDr8jX8+POvzJ0zm3yXBwGllV1SnLf2hw4dZuhzz/BOGdq+rOXj2HaHDjF0yLO8+94HvpVfBApx+t5XHuH6dekMff5ZbrvjLp+vu3Xr0hn63DMkJiYVS2dVfhgmF47YVDUbmAx0d0mjQMGcUnW8QJBVms6wksoSzzA0NIzv583jg3HjSU5JIcNHHuHevXv5Y106H37yGcHVYomrFVk4DC7QgMtT+GrpTgB+33KA8NBgalc1RjV169WnSY1cBn24BLXqf3KKYV+MhxjvzMQ7ajAF4+OLWHlNzflDcdl54CsP0Je2CwsN4/t5/8eHn3xW1PYWeISZGRnFyo9zaTtHFmRoWBjffzePDz8uOvdl5xHuJtsNz7GkcxcXn8CWzZu5pU9PPhg3npycbJ/aLmP3bvr3vpkPP/msAPvvP4lXw+QYEfndIQx0ycVpxIZ7EvdwYICI7AbmAo+U5lq5dYYi0lBE1jp8Hiwiw0W5Hd6mAAAgAElEQVRkvoi8IiJLRWSjwx5hR9uuIrJYRGJEZLyIvC0ii0Rkq4j0MtOIiIwRkbUiskZE+prfvyciN5rHM0XkE/P4HhEZbfq1XkQ+EpF0EflORKq4+mDKEs9w9MhhVK1Wlfr143zmER4+fJgbu11XxEMMDqLHxYl8u2K3k/3uAye4vEUsAE3qRxMRGsT+Y1lER4Yy/eV7WLl2PYuXryM7O9uy/zOmpnF9Vxf7rjcycUKR/WUmU7BWTAxbNm9i+/ZtbNm0id27dtG7b79idfeWB+hr240aMbRY21vhEU4voe6THOp++eVFbffiyGFUrVqVenFxlnmGmzdt4tfFi7w+dympjVn66xIeeuQx2rW/0Oe263lTN4aPfomOnS6hPORFZ7hfVds7hA9ds3CTreut7i3AeFVNAK4HvhBX1HqxHMqPJdgQWOvweTBGbz0fY0scppP/NY/vBN4FegALMdBbAOOBqRgddwuM22OAnsD3QDAQC+wE6mPMD4wx0ywFlpjHnwLXmH7lAm3N76cAA0qog2WeYVLDRhoWFqZhYWH6xJPPeM0jfGG4Kw9xu46a+KvWun2ivjpzjfZ/8yetdftE7fjMbF2yca+u2XFQV28/qDe/+oPWun2ivjhtlR4/naP/+TRNkxqlaHJysmX//zn4aT1yKk+fenaITpo6U4+cytO/Dp3Q7j0MpuAF7S7Ules26ZFTefrBuPGa0KBBoW3ffv0t8QAffOgRn9vO0ffBTz1jqfwHBj2iR826T546U4+eytO9h07oTQ51X7Vukx49ladD3LTd4KeescQj9PXchYeHF5YdG1tPt+7606e2O691Gz2vdRutUqWKAnX91S+ExqRo/YHTPQZK4RkCHYF5Dp+fBZ51SZMONHD4vLW0epQbz1BEGgKzVbWV+XkwUBXoAjyvqr+ISCzwi6qmisidwJPAMeBqVT1q2o0HvlfVCebnY6paTUTeBNaoasGd3xcYneYyYDpwN/AUUBN4APgRuBCobebX2LR7GghV1dGe6mOVZ+jtU7KSFOhF1xbfQW9p0bXVsq2u1rC+6NraACyQi647d/QvzzCsTqrG9Cz+DmxH7fmgp0eeoYiEABuBvwMZGCO4/qqa7pDmWyBNVceLSHPgf0C8eujwynPOMNcl/wiH44KnU3k474LZikGkbuKSl+PTLHH56yRVzcDoAK/FADMsBPoAx1X1mJv8XH2wZctWecm7OUOPUtVc4GFgHrAe46lxuoiMLJgiA54A7hORVcAk4E5PHSGUbyfwF1BXRGoDx4FuwP+VYrMDYzg9U0R6O/b0brQAuF9EPgNqAZdh3FkCLAYeB67AuBOcZgZbtmwFWP5AeKnqXIwHI47fDXU4Xgf4NOlZbp2hquaIyEjgV2Ab8IeXdhtE5FZgqojc4CHpTIy5g1UYk6dPqWrBYqqFGEPtzSKyA6OzXFjGqtiyZcufqpxbk8t3eKiqbwNve4jfj/FAA1Udj/GwBFVdgfGwBIwHK442Vc2/inEn+CQuUtVxwDjzOAeIcojbDrRy+PyaT5WyZctWmSViw11t2bJlC6i81Bq7M7Rly1aFyu4Mz3AJEBJc9tt7q0uYdn3Ur/REHlS76xhL9vvnFJuN8ElWfgAWVuUAkJ1rbWlKmMWlMVbPfZDFzsPKa17Lo9uqrHDXyjl4r0QSB56hFSZdclK8Jabeec0b85oHpt95zRtz+aXO9tddfQX9+/Tg6KG9rB1/H4P7XlzMPrFuNHNf7cvSD+5k3mv9iI+pCkDrlLq83CuOels+pM7G93njtbLzBBs3TLDUdo0S4yzZN22U4DOL8dKLLzB5jFVomFC/Qs99gX1yUjzVo8JJaRhfpnNfK7oKcXVr0bJZapn8b9kslfT0tYjINcWMyyo/LK0pL9mdoQeJC8/QEpMuIoIpaW7svWTqLSuF6bdm/SYeduDSRURE8MKwkXz22We8++5/OP/ecfT+W3OaJdZ2sn/5/i5M+H4tF90/npe+XMTIey4H4MTJLJ4a/A8y6/ck+KJ/MM0qT9Bq21mwD4+IYNqUEniAbliMzVu24n8LFgPC3O9+JC8vlymTJ1bYuV+/bh1T0yYTERHB7G+/K+QZ+nLunxtiXjtdu7Ji9Tqf239q2mSWr0onNbUxwHvmb8GyjPcmew6Bkt0ZepYTz7B3335lZtKlpjamQ8dOlph6vfr0dVO+ey5dVFQUl156KUePHWf//gPk5OYzdf56unVKdbJvlhjD/BU7APhp5U66dTTiN61fTV5YTYIia/Pn4dP06WuNJ2iF51dS2/nEA+zQyWsWY2RkJKtWLCc5JYU6sbEUsSwr5tzP/mYWHTp2Muwvu5yU1MZ06OCu/iWf+7CwMGLrFl07vl67vfv2Izw8vADUsBnjt+AHCUFBnkOgZHeGnuXCM0ywwKSLJyQk1BJTLz4+oRhP0JVLF+3CQzx+4kRh2oz9x4iPqeZkv2brXm7q3BSA7pc2JjoqnFrVItCsI0hEDQDaN61HQkLZeYIGy7HsPL/4+ARCQkIs8QBDQkK8ZjECLFwwn2W//Uan9m14+z9jSWyQWGHnPjMzg9DQUOITDJBvfEI8IaGhPp37zMwMatZyvnZ88b8gX1PuqDBllj1MPjNVdp6hOyadL/alxDlkUHoaD+U9++F8OrduwOL376Bz6wZk7DvmtBe2Xq0oxj3djdy8svMEfWY5WuT5WbYHklNSubHHzfz486+MeeVlsnNyKuzcq2oxBIujbw4ZlJjGm2vHk/9u5B+IQSlDZHuYXHnlwjO0wqTLICc3xxJTLyNjtxueYEKJTD0FqkYVrjcnPqYamQeOO9nvOXCcfiO+ouODnzHsE2OTztGT2Uh4dYJyjjJjdC9GjF/Ibj/wBMvadhkZu8nJybHEA8zJyfGaxViQd8buXTRt1pyoqChWLl9WYec+Pj6B3JwcMnYbuLaM3RnkuvW/5HMfH5/AoYPO144v/hfkayoBL+Co3kiA4GDxGAKls7ozFM9MxbfEYCSuFZGS5kOceIZWmHSbNm1kyeJFlph67ph8XUvg0oFBe6keHU3t2rUJDQmid5fmzFm82cm+dnSVwv/GT97Sgc/mrQEgrFYiMWEneW/S90yfn26ZJ1gebecTD3DJIq9ZjNu3b6N12/PZsnkzi375mQ0b/mDBgp8q7Nx37XYjSxYvcvB/I0uWuKt/yee+XfsL2bu36Nrxtf2npk0mKyuLrKwsgMYYODy/qLIOk8uNZ1gZAp6Zih+Z313mmMbFvohnGBKiw0eOtsSkGzZilGWm34ks037aV3oiK18PHDHtTS7d2vWb9URWvp7IMux79+6tGzdu1M1btuoL78/RiCtf0Re/+EV7vjBdI658RW8ZMVM37T6oG3cd0E/mrtLo617TiCtf0Ttf/kZnff21JjZM1oQGSTpylOG7FZ6glbaz1Pax9XTI8FE+sRibNW9h+B8errGxsRV+7oeNGFXM3tdz73jtDHr4UZ/9b5ScrOHh4Qpc56/fZET9xtpqyHceA6XwDMsrlBvPsDKoFKbiSFX9wfx+J9BaVQ+XlFe7du31l19/L7MvVtvZ6mkK9KLr4AA+JTzTF11bPfdWntBecnF7lvmRZxgZ11Qb3+f53UyrR17pkWdYXjrbd6B4Yiq6XmJn738FW7YqkSrpbryze84QB6aiGO9+7eYQV/DOlEuBI6p6JBAO2rJ1rqmyzhme1XeG6pmpeEhEFmG8TvDuQPhny9a5JhFrw/by1FndGYJ7pqKIzAemq+qzAXHKlq1zWJV1mHzWd4a2bNmqXLIRXpVIqtol0D7YsnVOyh4m27KqPIvvyzxgcWlM7W6vW7I/NHewJftAyurSGOt3QmfPQocCak1l1Nn+NNmyAsUzLLAvYNo1KYXJ16ZlE/7mwOT7/belXHLxBbRukUpM9SokJ8b5zMQbNuRpdm/fxPHD+xnct/gmncS60cx9pTdLx97BvDF9i1iIyXWY/1Z/3rq1AbGbxxKy7PUzmmfYqEGcpXNn1X+r105Zyy8XniGenyTboIZKqkDyDF2ZduEREUz1wORblb6RhxyYfC1atuIHByZfbl6u4b8PTLwRI0awaMlSRowcRe8uzYqzEAdezoT/ruOiBz7jpQmLGHl3ZwBOZuVy979m8+CDg8hrcQdr16af+TxDKzxCi/5b5iGWofzy4hkCNsLrDFXAeIbFmXbumXxzZs/iFgcm33wHJt9Kk8lX12Ty3dyrj9dMvGpVowgPjyArO4e8vDym/vQH3TqlONk2S6ztwELcVchC3JxxiM1/rEYiY9ifXYUjJ3PLpe0qlGfopu28PXdW/bd67ZSl/HLjGdrUmjNWAeMZFmPaxScQGhJSjCm4JzOzGNPOlcnXsX0b3n73fRokJnrNxBOct4Fl7DtOfG1XFuI+brq0CQDdLyliIQJo1lEkvAbtm9YjLDTYUtsFmmf473ffp0GDREvnzpr/1q8dX8svL55hAcrMHiafeQoYz1DVDdPOSyZfwb/XlJRUuve4mfk//8prr/6LHDdMPncbX0u6IF3LMliICSx+7zY6t05wYSEqVcJDGPfU9dz/2v+5zdentgsgz7CktvPl3Fn2H2vXjq/lu5HfnuLYw+QzUwHjGRZj2plMPlemYFx8fIlMuwLeXSGTb8Vyr5l4ivOQJb5OVTIPurAQD56g38iv6TjoC4Z9+jNgsBABoqrH0Co+jBHjf2bpH3vOeJ7hihXLLZ+7QF87lYFnCPadYcAkIv80mYVrReRxk3G4XkQ+EpF0EflORKqUYB4wnqEr026LBybfJAcm3+UOTL42bc9nqwOTb+FP871m4uWr0RlWq1qV4OBgel/ejDmLtzjZOrEQ+13MZ/MMdGRoSBBTX3+Y9es3MO27JWh+7hnPM1y4oHjbeXvurPpv9dopS/nlxTMUqbzvQPHEAoz2FALBG/M1AO2ANUAUBrorHTgfg2bT1kwzBRhQgn1AeYZOTLvYevrC8FF61GTyTZ46U4+eytO9h07oTQ5MvlXrNulRN0y+urGxPjPx7rjjDt20aZNu2bJFnxs6UsM6DtYXv1ikPV+YoRFXjdFbRs5yZiFe/4ZGXDVG7/zXbM3OydV3Pp5QyEO02naB5BkWtJ0VHmGgeYhlKb88eIbVGjTTK95e5DFQ2XiGIrILY57Asasu+Kyqmuin/rjcJCKPAbVVdaj5eRSwD3hEVRub3z0NhKrqaE95BZpnmJtnzd4qT/BMXnRtlWcYahFFb3XoZ/XasVK+v3mG0YnN9eKnPvWY5r+PdCyVZygi1wL/BoKBj1W12EJKEemDAXNWYJWq9veUZ4k7UFS1QUlxZ5BKOolZDsd5QEnDZFu2bPlR/qDWOKz/vQpjXv83EflaVdc5pGkMPAtcoqqHRKRuafl6NWcoIv1E5DnzOEFE2pWlEgHQAuAmEYkUkSigB7AwwD7ZsnVOK0g8By9UuP5XVbOByUB3lzT3Af9R1UMAqrq3VL9KSyAi7wJ/A24zvzoJjPXK5QBLVZcD4zEmf38FPgYOBdInW7bOdXnxNDlGRH53CANdsnBa/4v7dZBNgCYi8ouILDGH1R7lDaihk6peICIrAFT1oIiEeWFXKaSqbwBvuHzdyiH+tYr1yJatc1cCBJU+h7m/lDlDdxm4TqyGYDwF74KxNGihiLRSD+858maYnCMiQQWFiUhtwNqMtC1bts5Z+WGY7LT+F/frIHcDs1Q1R1W3ARswOseS/fKi4P8A04E6IjIC+BkojgCxZcuWrdJUyhDZyyffhet/zVFqP+BrlzRfYUzvISIxGMPmrZ4yLbUzVNXPgSHAa8BBoLeqTvbG47NBYiK81qavZcwrLxdbm3T69GkG3FKEQdq+bVth3Kv/eqnMGKYDBw5w7VVXULNaFerXqUXblk1447VXim1qz87O4s7bTAzVZR3ZuWM7IvDj/77nsk4X0qppMnVqRJLaMMFnhNd1V19BregqxNWtRdzWsTzXJZdDcwc7hT9nPkLXGr8Rt3UsrQ5PZeV7vQrjxt3VgPSxN/PHR70Z9d50al7/mlNoc+eHLF6zg00797E0fRetbh9bGLdgxTZmf/M1qSnJtGyWaqntmyYn8O83XiUsJMgpaF4Od99+C+e3asLfL+vInt07C+P+/forNE1OICY6guSkeMa8+i/yFadw6nQWA/r3o1Uz49xt27adfIV9+w9wjcO5a9ks1TrCa8wrxTqN7Oxsbru16NrZuWNHYdz9995N9ahwqkeFc989d5Gbl+8UTpw8xa239KFlU6PsLVu2kpuXz3fz5tHxona0a3seHS9qx9FjR/3yOyqQYCzz8hRKk6rmAg8D84D1wBRVTReRkSJSsLJ8HnBARNYBPwJPquoBT/l6uwMlGMgBsn2wOeMlrgivtMnWMEo+YJgiIiJ4/oVhVK1Wjeu7deW3lWuZNmWyewxVjZqsWmcgvIYOMexrx8QwacoMgoODmTRlBooa/vuA8HpuiFl+166sWL3OZwxX50s7EVMvnlPZ+fTp0pSmDWo62ZaEAAN4I+1X7rxnICPeGMfyVenW294KQis8gqlpJePPVq/fxEMltN11ZWw7KwiztWvWkDZ5Ir8tX8XvK9YwZfJE0teucbL1dN1MmT6LX5et4oOPP3XqoP0lf1BrVHWuqjZR1RRVfdH8bqiqfm0eq6r+U1VbqOp53tzAefM0+XlgEhCHMTafKCLnyouUnBBevfr09TtGqST7qKgowsLCiI2NJTq6OmFhYfTs3ZfZ37ggvL6ZRf8BDgivHw0MVZu255ORsZvklBSuvPoasrOy6HFzL68RXoXl1y0q3xcM14rlv5OdnUNYWDgNGzUibdY8ul1Qx8m2JAQYwI8LfiYiOpb68YmEhYWVS9t7jwBrbCDA3JRfHm1nFWH2wdj3aJCYRNNmzWnStCkNGiTy4Qfvu/he8nVTsP+9eYuW5OfnI8Zrdv2mM3lv8gDgQlUdoqrPY3QQt5evW5VGxRBeZcUo+YphKoirWbNWkTPx8exxtXdBeFWPLrLfk2nkPWvmdFq3OZ/EpCSvEV4FcQXggoL6e4uBys3JJi+v6Dlbxr6jxNV2XtteGgIsvFoRTNZK21tFaMUnxBMSGloMn+badtX91HZWEWY7tm9zgjLUq1+fnS53eJ6umwLNmjmdKpGRqGoWfpKI9WFyecmbznAHzktwQihlIjJQEpHjXqR51AQ1TBCRLiLSyVNyN/ZOn91tlSoRo+SDfWlx3tofPnyIoc8/yzv/GevWnnIs31WuSUtDgJW1bLdtX4EIrdLiylS+D/7747ytX5fO0OefJTExqVg6q5JSQqBUYmcoIm+KyBsYi6zTReRjEfkIA3xQ4lqdM0CDgOtV9VaMNUieOsNiCK+yYpR8xTAVxB06dLAwbUZGBvVc7V0QXkccEF6hoWF8P28eH4wbT3JKChkZu71GeBl5J3DooGP53mO4QkLDCA4uurzi60Sz5+ApJ1tPCDAJr07WsaI7FSttbxWhlbE7wy0+Ld6l7Y74qe2sIswaNkomM7Nopcmfe/bQICnJxbbk6yZj925u6dOTD8aNLyBd+1Vn4jB5LQblZQ7GZufFwBJgJPBDuXtmUSLypIj8JiKrzSVBiMhYIBn4WkT+ATwA/ENEVopIZzfZOCG8pk1J8ztGqSR7gHbtL2TvX39x9OhRsrOzmT41ja7dXDBU3W5k4pcOCK8uBobq8OHDjB45jKrVqlK/fhzZ2dlu/S8J4VVY/t6i8n3BcLU9vx3h4WFkZ2exfds2+na/hjkr9jnZloQAA5DoBpw8/Cd/Zu4q0feKQmht3rTRQIC5Kb882s4qwuy+gQ+wc8cONmz4g40bNrBz507uG/iAi+8lXze9etzAiFEv0rHTJfhbIp6HyIEcJlc4Jqc8A3Dc/Hs18CHmgndgNnCZGbcdiDGPhwODS8nzemBjWFiYZYySr/aJSUlatWpVFRENDg7WBx96RI+dztOnnx2ik6fN1GOn83Tf4RN60809CxFcq9dt0mOn8/SFYSM1MjLSwFCFhWlYWJg+8eQzPiG8XMsf9PCjPmGgvpw4Sbdu3ao7duzQF941sF/eIMAirhqjP6/epRPTpmtqaqomJyfr8JGBRWgNHTFKj2fl69PPDdG0aV/p8ax83X/kpN7k0HZr1m/W41n5etwPbWcVYdZ/wG0aEhqqoaGhesutt/l83ZzXuo2e17qNVqlSRYG6/vqN1mrUQgd8udJjoLIhvAokIinAi0ALIMKhE23irw7ZXxKR46paVUReA3pRNJyvCrysquNEZDvQXlX3i8hwjA7U7ZY8c0/kQIAGiYntNmzeXt5VKFFW35vsxRYoz/YW/2PXvL7sux4PznnCUtlWh15W2z6gdzvgMA/ruy7rdBHL/Yjwqp3cUruOnuQxzRe3tikV4VUe8mZv8nhgNMai6+uAu6j82/EEo/P7wEomqvohxh0mF7Rrf/a8yduWrQCpYNF1ZZQ3T5MjVXUegKpuUdUhmNtcKrHmAXeLSFUAEYkX9zyzY0A1N9/bsmWrnHTGPU12UJYY44wtIvKAiNwAlApKDKRU9TtgIrBYRNYA03Df6X0D9PDwAMWWLVt+lIgxZeMpBEreDJP/gTHn9ijG3GF14O7ydKqsUtWqDsf/xsCCu6Zp6HC8EWhdIc7ZsmULsD7/XF4qtTNU1V/Nw2MUAV5t2bJlq0wK4M2fR5XYGYrITDy8OFpVby4Xj2zZsnXWSgjsUNiTPN0ZvlthXpwhsrLE4kRWnqWyq0V4M6NRsgJ9/VlZHlP7Fs9vUytNBybdZck+0E8/S1v+VpoC7b+T/PBCqPJSiQ9QVPV/nkJFOhlIFfAM09PX8vqY4kzbrKws7hjQjzYtmvC3zh0LkUc//Pd7One8kJZNkompHknbpkm888arbu0H3tmfDm2bc90Vl7Bzh2Gfk5PDIw/cTfvzUmkQE0VC/RhLPMJWzRtb4imWhck38J67iI4MIzoyjPvvu8frssHg+Q26/x727NrOmrdu5Imbik/tJsRE8e3w61g8pju/vn4T15xvgBn6dk5myZjuvN6tKnWXv8R5Aai7I4+wUWKcdZ6hD/4X2CcnxVM9KrzM9q2aN2Zd+lpE5JpixhYUVEoIlM4ZNmFZ5MoztMYThJnT09jwh7P9xM8/pUaNmixZuZ77Bz3K6GHPAfDNV9PIOn2a4KAQvl/4G1WrVmXiF5+XmUfoKxPQlafoK5PPlamXNmlCMaaeJx7hjGlT+fjjj8mXYFqc14belzaiWUINJ/tnerZlxqJtdHxyFne8OZ+37usIQNrCrVw8eCYPPfww0V2eYG16xdbdKo/Qrb0PLEyrLM3169YxbUoay1auJSW1McB75m/BsgqAF2fa3mRbLjxDqzzBbt1vZt6cb5zs5839hj79jedS3W7qyc8//WhsDRJhT2YGDRslUzc2lvCwcG6yyCP0hQnojqfoC5OvGFMvMZEPxr7nVdmzv5nFo489DgjxCQ3IDavJlO9/p9uFiU72qkq1yFAAoiND2XPoZGFc/sGtSFRdbuvWieCQiq27VR6ht/blxdKc/c0sevXpS3h4eAGoYTPGb8EvCgnyHAIlr4v2N+DxDJELz7DsPMFWrdvSIDGJPXuc31uzZ08GcfEJhfbVoqtz8OABunXvCcCSRQtp1zKFx/7xBKmNG1vmEVrjKXrP5HPH1HOlJnviETZs2LDw6Z1E1iQjYw9xtSKd7F+csoJ+nVPY9EFfZj53NU+MW1IUeeowElmLnp0akZdfsXW3yiN0tfeVhWmVpel4TZly9yrOMsmgWZ+hd4YicpG5cHmT+bmNiLxT7p5VDpWZZ1gQV8ATHPPWf3yyX7HsN0SEHr36sXT1Rt5+6w327d1boTxCK/b+tkWKV7X3pcl8OX8zje9Po8dL3/HxI5c5PChS6kRHcDIrt6hTPUN4hG7tfShf1RpLs4QHNn7bjhoc5DkESt4U/TbQDTgAoKqrqPzb8fwlF55h2XmCDZNT2JORQb169Z3s4+ISyMzYXWh/7OgRatasxYypk7n0si78uSeTOnXq0qFTJ1YsX2aZR2iNp+g9k88dUy/RlanngUe4ffv2wv9EevIQ8XH1nYbBAHf8vQnTF20DYOnGfUSEhRBjkrKpUpMawSeY+svWCq+7VR6hq72vLEyrLE1HTqMpd6/iLJOEyrsDxZvOMEhVd7h8Z22dSAVJRBqKQbX+SETSReQ7EakiIm1FZInJOpwpIjVLyMKJZ2iVJ/jVjClcfX03J/urr+/GlIlfADD7q+lcclkXRIy5sl27drJ1yyb+WL+OpUuWsGLFcks8Ql+YgIX2f5WNyeeOqTfw/ge9Krtrtxt55+1/A0rGrl2EZB2kz1XtmfPbTif73ftP8LfzjH8uTeOrExEazL6jpwEIrtWIkwczmTD31wqvu1Ueobf25cXS7NrtRqZNSSMrK4usrCww3je8FD+psj5N9oYROB1j8nQ5xlvyHgemBoI35msAGgK5QFvz8xSMd7qsBi43vxsJvFWC/UCM6YHTwcEhOnT4qDLzBEPDwvSRfz6lfx7J1n889Zx+Nmm6/nkkW7f/dVS7db9ZGzZK0bYXtNdfV/6hfx7J1i0ZB7Vb95s1LqGBhoaGas1atXTYiFHWeIQPPWKJp+grk8+Rqdd/wG0+8wjvuvtu3bp1q27ZvU+HTfhdq/Qcpy9NWa69Xv5Oq/Qcp+c/Nl0Xrf9TV207oKu27tduI7/VKj3HaZWe4/TqoXP0jQ8nqFSN1UbJyZZZkhXNI3S1r2iW5rARo7RRcrKGh4crcJ2/fpP1G7fSId9u9BioxDzDuhhD5SvNr/4LPKyq+/3SG5ejRKQh8L2qNjY/P43BZLxHVRPN71IwOvcLPOV1Qbv2umBR2f85numLrq1ObJd2nXlSoBddB3JSH6wvuraiSzpc6FeeYVyT8/Ted2Z4TDPq2iaVk2eoqnsx3lh/psrxzV55QI2SEseb4MEAACAASURBVNqyZav8VUk3oJTeGYrxEqhi/5pUdWC5eFT+OgIcEpHOqroQAz7xU4B9smXrnFBlhrt6M/b6r8NxBNADh7V3Z6juAMaKSCTGa0+tjaNs2bLlneQMvjNU1TTHzyLyBfB9uXnkR6nqdqCVw2fHF3F0qHCHbNmyhQSUZ12yyjIr3wjw/5ulbdmyddZLCOyWO0/yZs7wEEVzhkHAQeCZ8nTKli1bZ68C/XS+JHnsDM13n7QBCjY15msgn/MHUAKEWNgrVC3C2gVg9frJzrX2QsNQi/ukrPh/cLK1t0zUvOZlS/YH/8/a//5A//itlO9vz40dKH7IR+RajNd6BAMfq2pxRpmRrhcwFbhQVX/3lKfHK9zs+Gaqap4ZzrmO0JFnaIVJl9IwPqA8wvNbNeXN19zzGO+67RbOb9WUv1/WkR0mT3HZb0tp06IxdWtEUbdGJPfcVfyND9767itPsKDtCnh8ZeUBPjjwbjJ3bWPNp3czuF/xKeLEutHMHXMLSz+6h3mv9yc+plrh97+8fyev39qQetvG+dz2/vLfXzzDspbfslkq6f7mGYrxNNlTKDULB7QexvvcbxGRFm7SVcN4d9OvrnHu5M2/+6Ui4nFB8tkqV56hJaZdeART0yYFjEf46/I1TJuaVozH+IXJY1yxdgODHnmc4UOeBaBJs+YIsHTFGn5ftZ6paZNZu2Z1mXxf5qPvbnl8Prb99KlT+PjjcSghtGzVht5XtKBZUm0n+5cfuIIJ36/lovvG8dIXvzDy3i4A7Dl4nC6PfMbDDz3M4aTepPvIQ/SH/37nGfpY/tS0ySxflU5qOfAMg8Rz8EKFaD1VzQYmA93dpBsFvAqc9ibTEjtDESkYQl+K0SFuEJHlIrJCRJZ75fKZLyeeoRWmXUpqYzp0cMe0qzgeYc9efZg725nHOHfO19wywLjr696jJz/NN3iM69etJTk1lYaNksnXfOOHOduZxVhevnvL4/PU9o8+/g9AiG/QgNzQaKZ8u5hunZo42TdLimH+8u0A/LRyB906NQYgJzefrAM7kMjaRNaIDYj/FcEz9FR+7779yo1naGC8Sg5eyAmthxvEmIicDzRQ1dne+uXpzrBg79lNQFPgeqA30Mv8ey7IhWdogWmXEE9IaGhAeYRx8QnsyXThKWZmEh/vXP7BAwfYk5lJeFg4Hdq15pIL23LHXXfz1597KsR3Vx5fWXiATjzE8GgyMjILh8EFWrNlLzdd1hSA7pc2IToqnFrRVQCoFZ5N9ysvZNOkh8j1kYfoD//9yTMsS/nlxjNECBbPAYgRkd8dgusGD3ddZuEUnogEAW8CPr14x1NnKACqusVd8KWQyiQRedQk2UwQkXAR+a/5Evm+7pK7sXf67G4atSQmnTt7PKTxxt5TGrdTvD4w7WLq1GXJstX8sHAJc+fMJi/f5SFMOfmu6obH5yMPsPivpTin79kPfqBz60QWj72Lzm0Sydh3lNw8o44Hjp7i64UbaXX7WEKk4v13jbPMM/SxfDfyz/OCUobI5jB5v6q2dwgfuuTihNajOGKsGsb64vkish1jTfHXIuJxv7OnzrCOiPyzpOBdzSulBgHXq+qtwPlAqKq2dV1cbsqFZ2iBabc7g9ycnIDyCDMzdlO/vgtPMT6ejAzn8mvWquX0fdNmzQEICgpysS0f3115fGXhAW5z5CFmHSU+vj6ZB4472e85cJx+w2fQ8YFPGTbO2JF59ISxlV3Cq6NZR9hz4Dj5QGZmxfrvT55hWcovL54h+IVnWIjWE5EwDHZC4fyPqh5R1RhVbaiqDYElwI1WniYHA1Uxell3odLL7LjXmuFxERkLJGP8l3ga+BJoa94ZprjJwolnaIVpt3nTRpYscce0qzge4fRpU7iuqzOP8brrb2DSlwZPcdbM6Vx2ucFjrFU7hi2bN7F9+za2bN7E7l276N3HmddRXr57y+Pz1Pbv/PstpICHmHOEPtd1ZM6iTU72taOrFN4oP9m/I5/9n/GAKD6mGlVqJ6En91NVTpCbk83UtIr1vyJ4hp7Kn5o2uVx4hgV7k608TVbVXOBhYB6wHpiiqukiMlJEbvRs7TnjkliAywPBFPNXANoBa4AojE49HeNOcDsQY6bpAsz2kEcRzzAkxC9MukDyCO8f9LAePpmrTz7zvE6cMlMPn8zVPw8e1+49emqj5BS9oF17XZm+UQ+fzNWxH4/XhAYNNCwsTMPCwrRvv/4V6rsrj68sbV/AQ9y88y8d+vF8jbjiJX3x84Xa8/kpGnHFS3rL8Om6adcB3bjrgH4yZ4VGX/OKRlzxkl7/5ERdveUvfffjLzWpYXKZeIj+8N+fPMOylF8ePMOkZufpuKU7PAYqG89QRFao6vll7mUDLBF5DKitqkPNz6OAfcA/gfaqul9EugCDVbVbyTkZateuvf7yq8e7bI/Kt/ACeji3F11bXbR8pi+6Luk3WhHlX3Jxe5b5kWfYqHlrHfa55we8d12UVOl4hn+vMC/KR5Vzz48tW+eyhIC+58STSvx3r6oHS4o7Q7QAuElEIkUkCgM9tjDAPtmydU6rMr8QyhpLvhJLVZeLyHiKJn4/VtUVgd4nasvWua7K+gs8aztDAFV9A3jD5buGDsfzgfkV6pQtW+e0hKBKSnc9qztDW7ZsVS4JAX4dqAfZnaEtW7YqVJX1AYrdGVaQrJ7/PItLc8ICjBcO5FztoXnPWrKv2WWItfLnj7Zkf1bNc0vlrU9lvWOtNPIXz9BXJp0rj7BNiya8PsY9j/COAf1o06IJf+vcsdD+h/9+T+eOF9KySTIx1SNJSUqwxMQrC1Nv4L13UT0qnOpR4dx/b3FAq7dtZ5UHaJmHOPFhBg+4rJh9YmwN5r51F0vHP8y8d+4hvk50YVyD2OoM7Z1MvczJtGqW+v/tnXl4VEXWxn+HkBC2QFhkCUIARRAVEEVUQGSUUQEXcAHE3XHHbfQbEYSRcR3XcV9xQ1lUUEAcREVGHRhBlEVFUAQkYVPZBCEs5/vjVJJO091Jp7vTCdSb5z65S52qc6vvPfdU1am3wtZdeb7/RPAZ5jeTI23JgjeGERBXPsMoOemC+QjnfL2It8aP3YuP8FXHRzj/2yVcO/gGhg8z+br16jFm/ARSUlIYM34CisbOyReF/osWLmT82DF88eV85sxbwNgxr7No0cLS112sdR8TH+IRnPOnw2mdXb+I/L3XncLr//6aThc/wT0vzWDklT0Lrj075CzuGnEbuXX+zLwF35a5/rHKJ4rPEMpvaI03hpERNz7DaDnpQvIRnnMeUyZPCpJ/l4GDLgTgzL5n88kM4yNs174DOTmraNGyJSf1/DN5O3ZwZt+z486JF07+uWee4sCmTTmkdWtaHXIIBzZtyrNPPxXXukukfBE+xJSajJ/6Gb27tCki3zq7Pp98aQROM+cto3fX1gXnl363gO1Sk0rpmaSlpSXk2UmkfDnnM0wIvDGMjLjxGUbLSZd/LZCPMCsri9XB8rm5RTgFawVwCq7Otbzfnfg27dp3oFmzZqXmxItW/xUrltO4USEFXqNGjVmx/KdS1V2sfIAx8yGm1SQnZ3WRZjDAwh/WcGb3tgCc0e1QMqqnUyejKgcfWI/lK36mx3HtmTXqGipXipELMwn3nzg+Q6iERNyShQpvDEVkqojULibNJ6G4zESkvYicFkk0hEyR41DzRuPBSVfctZLKb9y4geFDh/D4k89EJa8aghMvSvm9zxd93KKqu1j5AGPmQ9w7/ZAn/k3X9tnMGnUNXTtkk7NuE7t276FySiUOaVaPOd+tostfnnH1FuOzU8b3HwJxWv8ochPZN5NLCbFft7eqbixlFu0xBu9wiBufYbScdPnXAvkIc3JyaBgsn5VVhFNwUwCnYGpqGtOnTePZF1+mRcuWMXEKRqt/dnZzclcXeiKrV+fStFnR5bZLzAUZIx9gzHyIeVuMD/GXLUXkV/+6hf5Dx3DspU8x4rkPAeNDzFm/ia2709my4Rd2797DbjUuybLUP1b5RPIZ+mZynCAi2Y6p+ilgHrBbROq5a3eIyGIRmS4iY0TklgDRc0TkCxFZIiJdHSnkSOC8CEzXceMzjJaTDkLwEb45jl69+wTJn84bo18F4J0Jb3FCd+Mj3LhxI3eNHEGNmjVo1KhxwjgFw8lffsVVrFyxkiXff8+S779n5cqVXHHV1XGtu0TKF+FD3LWFc0/rwnufLy4iX7dWtYLf6tYLuvHKe7Y00NzvcujcuROyYwN7tm9k1848xiXg2UmkfCL5DMtrMznpvIPRbkA2sAfo7I6XA/WAo4CvgaoY+exSjJ4LbMrdQ27/NOBDt38x8ESEsuLOZxgLH+HV1w7WLdt369+GDNOxb03ULdt36/qNW/XMvv0KOAUXfLtUt2zfrXeMGKnVqlXTZtnNCzgJb/m/22LixItW/4HnX6CVU1M1NTVVzx90YUx1F2vdx8SHuGKNDn/2A00/fqjePepj7fd/r2n68UN1wNA3dOnK9bpk5XodNWmOZnQfrunHD9X044fqaTeM0iefNz7EFi1aJEX/8shneHDbdjrt23URN8obn2F5hYhkAzNUtbk7Xo4ZwkFApqqOcOcfBnJV9UER+QQYqqqfi0gD4HNVPUhELsa4Da8rrtxY+QxjredYg65LwiCcSJTXQNuSINlB18lEvPkMWx3WXp9888OIaXoeWr/c8RmWZ2wNca64H2yH+7+binvfHh4VGkbhlWwtQqPC9RlGwGdAHxFJF5EaQK8SyGyhgqzn4uGxr8CPJicYqjoHWyFrPjABmAtsKkZsBnBohAEUDw+POEOK+UsWKlxzUVWXY2ui5h9nB1x+UFX/LiLVMKbrh1ya7gHpf8EGYVBj8z460Tp7eHgYynMzucIZw2LwnIgcCqQDr6jqvGQr5OHhEYAkN4UjYZ8yhqo6MNk6eHh4REb5NIX7mDEsz4g1tKRySnl9hPZ9xBoak3l0sZFbkcuf80RM8uUJ+QtClUfsMwMoiUK8+AyTxUmXTPkrLruEjGppZFRLq5B8hrHIL/5mPls3/crKZUvYvXnlXrIAD/3f2Sx6dwRfjBtC+9ZNCs6f3+cYFr47nCdv6kKVlW/TtvVB+wyfIfjpeBUScecz3I/kFy1cyLixbzBn3nzmfrWwwvEZxir/r0cfJa1aTZo1y2bXqpmo7iki++cuh9KyaX0OO+NOrrtrDI/d3h+AzIxqDL3iVLoO+idXXHkV706azFcLvi3zZy+RfIbldTTZG8PISDif4b4q/+wzT3Fg02Yc0rpNheQzjFW+xUGtSEtNIzUtDalSC922rohs7xOO4I0pNt33i4XLqVWzKg3rZXDycW34aPZifluzHE3NYOnq7aRXKXs+RM9n6BGMuPEZJoOTLpnyK5b/VIQlpaLxGcaTD1BSa6A7fy8i2/iA2qxas6HgOGftRhofUJvG9Wuzau0GdOfvSGoNctZtRCj7Zy9RfIbgjWFUEJEXXIgMIrI8n5UmGarsdaICcdIlUz70+YrDZxir/N6QoHR7p1DViMZgX+AzFHwzOSqo6uWq+m3xKROOuPEZJoOTLpny2c1bkJtbSIFX0fgM48kHaF5e9SKyOWs30qRhZmFeDWqzev0mctZtpEmDzAJvMuuA2ihl/+wljM+wGK+wpJ5h/sCmiPwgIreFuH6ziHwrIgtE5CMRaRYqn0Ak3RiKSHUReU9E5ovIIhE5LwIz9SDHSfi1iDyb36krIr+LyN0uj9mOmQYRaSAiE935+SJyXKR8QiDhfIb7qvxfrriKlStW8P33iyskn2Fc+ADzdrAzLw/dsQmpdkAR2fdmLmRgb+uG63R4Npt//4M1v2xm+n+/46RjW5PZoBmyczOHZFVl+468Mn/2EsVnCLEbw8CBTeBQYEB+SzIAX2GMVEcAbwH/LDbjZPCGBW5AP+D5gONaGP/gUe54OcZX2AaYDKS6808BF7p9Bfq4/X8Cw9z+OOBGt5/i8g6bTwjd4spnuL/JDxxUsfkMY5FfsHCRrl27VvPy8vTn3PV65d9H63V3jdHr7hqj6e2v1fT21+rTY2fqjyvX6cIlOXrcwPsLzl8x4jX9YcU6feGlNzSjTkNtniQ+xETwGbY5vIPOW7454kYxfIbAscC0gOMhwJAI6TtgtH3lm89QRFoB04DxwBRV/dTxD96iqnMD+Ar7A7cD+cNyVYExanORdwDpqqqOcOFkVb1cRNYDTVR1R0B514XLJ5KesfIZeuy/qMhB1/HmM2x7xJH6xpSZEdO0b5YRkc9QRM4GTlHVy93xBcAxGoaXVESeANaoasTo+aTPQFHVJSLSEWOgvldEPgiTVLD5xkNCXNuphVa9OL7CSPl4eHgkGsWb1noiEuh5PKeqzxWTQ0ivTkQGYc7UCcUVWh76DBsD21R1NPAgcGSYpB8BZ4vIAU6uTgk6RT8CrnbpU0Qko5T5eHh4xAklGE3+RVWPCtieC8qiyMAmYQZ4ROQkYChwemDrMBySbgyBw4EvRORrTPGQrqwbXR4GfCAiC4DpQKNi8r4BOFFEFgJfAm1LmY+Hh0ecUEkibyVAwcCm2MJu/TEu0wKISAfgWcwQrguRx14oD83kaVifYSC6B1zPDtgfhw2KBOdRI2D/LWz0CFVdC5wRIn3IfDw8PBIMIWbaGlXd5fr+p2EDo6NU9RsRGYkNvkwCHgBqAG+6+MqVqnp62EwpB8bQw8Nj/0I8AqtVdSowNejc8ID9k6LN0xtDDw+PMoNnuvYIOfUpGsS6VGisHHKVkvgEx1p3sXJJxlp+rKExmT1GxFb+x3fGJB93lFNjWB4GUMo14sVn2KJZFg+Gkb9gYH8Oa3Mw3Y7vXCD/66+/csrJPcisWZVG9evQ7tBWPPTA/SHlLxrUn3aHtuLErscWyH/84XS6Hns0bVu1oF6tahyU3YQHHwhd/oXn9+fwNgdzQpei5Z/aswd1MqrS+IA6peLUixefYbR1ly/folkWtapXiZnPMFnlX33FpeT+/BMLR1/NLed32Uu+aYNaTH3kIr546Wqm/etisupnFFz7fcYIHrroEBqsGs1hpeRDTBifoZ+bXPEQdz7DcSHkX3qR2pm1WfTdUgZffyPDbrdplunp6Qy9YwQ1atbktN69mPP1It4aP5bF3xWVf/XlUdSuncn8b5dw7eAbGD7M5OvWq8eY8RNISUlhzPgJKMqb48byXZD8Ky+9SO3atVn43VKuu/5G7hhaWP7tw1z5vXpFzakXdz7DKOoun48vPT2dKe9/EB8+wzIu/+03x/PCCy+iVKbtYe0450+H07pZ/SLy917zZ16f9jWdLnmae16ZycgrCrvJft+2neuuu47VdXsxrxR8iInkM4zDaHJC4I1hZCScz/C9yZMY5OTP6nc2n8ww+erVq5OWlkaDBg3IyKhFWloa/c45jymTJwXJv8vAQRcCcGbfs/lkxseoKu3adyAnZxUtWrbkpJ5/Jm/HDs7qe3YI/ScV6H9W3xDlH1BYfjT3XxZ8huHqbsrkd+l87HEm3y1xfIaJLP/6G28ChKwDD2RXSg3GT/2M3l1aF5FvnV2fT740WrSZ836id5dDCq7NnTsHSc+kUtXMqH+7RPMZFowoh9uSBG8MIyNufIaNs7KoXDmV3Nwg+ZwcsprsLZ9/LTOzTqEyWVmsDpbPzS0oo3LlytTKKJRfnWt5vzvxbY5o14GmzZqxOpT+geUHyOfm5pBZJ7D8kt9/PPkMo6273NwcUlNTyWrSpEDvWPgMk1F+dnZ2wZQKqZJBTs5qsurXLCK/8Ic1nHmC8ROc0a0NGdXTqZNRFYBf1q2lz586MfPpy6kk0T+7ieIz9BReZQQRuVFszeS4ZRmijCLHoTrXw3LKRSNfzLWSym/cuIHhQ4fw+JPPhJQPxV0Xj/JDn4+BzzDKsoOvxMxnWMblh5xvFpR8yFMf0LV9M2a9cBVd22eTs24Tu3bb8gLXPzSZKZ99z0Uj3ya1UkDeJSw/VPGhTkaNYprIvpkcP9wIxNMYxo3PMDcnh527dtKoUZB8kybkrNpbPv/ahg2/FaTNycmhYbB8VlZBGbt27WLT5kL51NQ0pk+bxrMvvkyLli3JyVlFwyD9G2c1KSK/OUA+K6sJG34LLL/k9x9PPsNo6y4rqwm7du4kZ9WqAr1j4TNMRvk/LV9eYBB1x2ayshqR+8uWIvKrf91C/2HjOPbyZxjx/EcAbN5qs8425qWiOzaxfPUG9ijkRvnsJozPEHwzOd4IwYM4AmgMzBCRGS7NABFZ6K7fHyD7u4g8JCLzHPFj/TDFJJzP8LTefRjt5Ce+/RYndO9R8AXveNTRrFu7ls2bN5OXl8fbb46jV+8+QfKn88boVwF4Z8JbnND9RESEjRs3ctfIEdSoWYNGjRqTl5fHW+PHhdC/T4H+EyeEKH9dYfnR3H9Z8BmGq7tevU9n9qz/JpzPMJHlP/6vRxGUnJ9/pvLuLZx7Whfe+3xxEfm6taoV/Fa3nt+VV6Z+BUDtGulUyTwQ3f4btVK3s3NnHuOjfHYTx2dYXCM5idYwXjxlZb0RmgdxOVDPHTcGVgL1sXjKj4Ez3TUFznf7w4EnwpQRVz7DEXf+Q7fl7dHbbh+m499+R7fl7dHfNm/Ts/oWyn+z+AfdlrdHt+Xt0abNmmmNGjVURDQlJUWvvnawbtm+W/82ZJiOfWuibtm+W9dv3Kpn9u2nLVqY/IJvl+qW7bv1jhEjtVq1atosu7mmpaVpWlqa/vXW23TrDlf+W+/o1h179NdNrnwnv+i7H3Trjj26dcfe5V9z3fVJ4zOMtu5G3PmPuPIZJqP8Sy69VJctW6Y/rFyjw5/7UNO7Dte7X5qh/W57XdO7DtcBw8bq0p9/0SUr1+uoyXM1o8edmt51uHa/+nld+OMafeL50dosu4W2KCUfYiL4DA9vd6T+tP6PiBvF8Bkmaks6n2FpEYYHcTlGCvuLiJwB9FPVC136yzCihptFZDdQRW2OYwtggqq2j1RerHyGsdazD7ouPZIddB1r+ckMuo43n+ER7TvqpI8+j5imeb2qEfkME4UKOwNFi+dBjOYHrJhfBA+PCoikNoUjoCL3GYbiQdwC5Mcf/A84QUTquYDRAUA+xW4l4Gy3PxD4rMwU9/DYz1FeR5MrrGeI8SA+ICJ7gJ0YieuxwPsislpVTxSRIcAMzEucqqr5UadbgbYi8iWwCTiv7NX38NgPUcJFn5KBCmsMNTQP4lzg8YA0bwBvhJG/A7gjYQp6eHiEQfm0hhXWGHp4eFQ8eAqvcgYNYMb28PAoW/hmcgWHEluIRazhFbF+TZMZGhMrYq27io5Y+Qgzj7+11LI7Fq+KqexQ8KPJFRSBfIbJ5NRrmZ1V5nyEgeWXRv948RnGWncVlc8wFv0nvDmO3JXLWJuzgpvPabeXbNOGtZn6xBV8Mfpmpj11FVkH1AKgW8eWzH7tpoJt26/LAc7cK4MYIBJ5Sxa8MYyAvfgMx41NHqdelXTeHDemzPgIQ3L6JZPPsAx1Lw98hrHq37XLcdRrmMUfeXs4t2d7DskO4kK8vjevT/2SToMe5p4XpzPymlMB+M+XP9L5gkfofMEjnHrtM2zb9gdAuLXMo0ZxhtAbw/KLInyGZ597XtI49VoedDCdO4eSTwwfYUk5/ZLJZ5hs+WTzKYaT/2reXPLydpKWVoXs5s0ZN3EqvTsXJclo3bwBn8z9AYCZX/5I725tCcZZPY7g/ekfA2zb62IMKK9zk70xjIy9+AyTxamX1SSLyqmpZcZHuFf5UeofTz7DmOuuAvIZxqL/rp157HZUXgA5azfQuF7RMcOFS1dz5omHA3BG98McF2JRwqdzTm7P2PHvEG94z7BiotR8hhoHTruw14pmEDZNSeSjKT8a/UOfj4HPMFY+wgrGZxiT/iEQnHbIY1Po2qEFs169ka5HtiBn3cYCLkSAhnVr0rZlQ6Z9OCNkfrHAG8OKib34DJPFqZezKoddO3eWGR/hXuVHqX88+QxjrrsKyGcYi/6VU9NISSl8tbMaZLL6161FZFf/spn+t73KsRc+yoin/w3A5q3bC673O6kdk2YuYteuXcQTglBJIm/Jwj5jDEXkHyJyQ8Dx3SJyvYjcKiJzRGSBiNzprgVzIYabjleEzzAUH2BZcer9sHQJs2eHkk8MH2FJOf2SyWeYbPlk8ymGk2/foSNVqqSRl7eD5T/9xHlnncZ7/1tRRLYIF+JFPXhl8pwi18/t2Z7xH3zNfoVk8IYlYgOygXluvxLwIzbn+Dlc4DswBehGCC7EMHkW4TMsD5x6ZclHGFx+tPrHk88w1rqriHyGsciPfmOMLlu2TFesWKF3PPy6pne6Re9+4QPt99dRmt7pFh1w2yu6dOU6XbJinY56Z7ZmHP83Te90i6Z3ukVbnXG35qzdqFWPuVWlWgON53vavkNH3bBtV8QNz2cYO0RkOvB/QAPgcozs9Wxgo0tSA7gX+JQgLsTi8j6y41H6+ew5xSWLpFupZQH2xMpnWIGDrpONWN+RZAeNxxR0veg19mxdE7cb6NDxKJ35eWTS7FpVUzyfYRzwAnAx0BAYBfwJuFdVnw1OGMyFqKojy1JRD4/9EUle5iQi9jVjOBEYCaRiPIW7gH+IyOuq+ruIZGF0X5WB31R1tIj8jhlQDw+PskA5tYb7lDFU1Ty3GNRGVd0NfCAibYBZrqnyOzAIOIi9uRA9PDzKAPEYMRaRU4B/ASnAC6p6X9D1KsCrQEfgV+A8VV0eKc99yhiKBbJ1Bs7JP6eq/8IqLRA/sjcXooeHRxkgVlMYME32ZCz8bY6ITFLVwPmKlwEbVPUgEekP3E8xJM77UmjNocAPwEequjTZ+nh4eISBFLMVj4JpsqqaB4wFzghKcwbwitt/C/iTFDOStc94hu6r0CLZenh4eISHkbvG3EwuMk0W8w6PCZdGbRXMTUBd4Jdwme4zxjDR+Grel79Ub/uriQAAFNdJREFUS6u0IkKSekSo6GIQi6yX93WfSPlmEa5FjXnzvpxWNVXqFZMsXUQC1+V9TlWfCzgOZU33mgFZgjRF4I1hCaGq9SNdF5G5pY2NikXWy/u6T6Z8tFDVU+KQTZFpskATIDdMmlUiUhmoBfxGBOwzfYYeHh77DQqmyYpIGtAfmBSUZhJwkds/G/hYi4me956hh4dHhYLrA7wOiwhJAUap6jciMhKbyjcJeBF4TUR+wDzC/sXl641h/PBc8UkSIuvlfd0nUz4pUNWpwNSgc8MD9rcTEGJXEuxTc5M9PDw8SgvfZ+jh4eGBN4Ye+zlEpG5xwbge+we8MfTYbyEivTFij8xk6+KRfHhjmAAEexrlxfPI1yOSPhK8UEkZwIVHlGn5IlId+DswAagqIjUiSxSbX7qIZLr9RoH3FGO+5eLZ2R/gjWGcISKSH88kIlUBVFXd5PJwMo1FpEr+C5kIgxCoFzYtKfBaWxE5VEQaqeqeWF7AaGVFpBpwtIhUFZFeQPvSlu3yK7buROQ4LAj3eeBB4BNga2nv25XZHrhYRK4E7nP5lwpBeqSWQibsuQjyjdxvsd/Ch9bEEUGG8GbgKBf9foGq7hCRFEctFihzCjACWAxUF5HbVfUHEamkqnv2KiR2vQZjL+00jPE7D3gJmAF0F5H+qjqrpOWLyCVAU+A/wJequjnI8EaSPRFoBVQDbseo1aKaDSEi5wPNgS3ARFVdGUl3ETkGo3bqiU3PaowF8VZR1e2lqXf3AckFTgCOBf6qqutL+xsG/FaXAZ1FZD7wrap+HElGRLpgc3K3qOpUd65YHUTkdOBS4FZsmYv9Et4zjCMCHuKeQF/gAWA7MFdEqqjq7kAPUURaAI9hSxXcC3wBvC4iB8bLEAbpdRo2of1mYAdGaXQr0F9VLwLuAl4VkXYlNIRnYFyQ9TAy3ctFJNO9hBG9EhE5AngKW5dmN0a9NjGa+xKRa4HBmCFsBrwtIgdFMISVgIOBd7ApXDWBPpghfy2/3qPxzPPTqupK4DvgY6CtiLTJ16M0HqeIXI7NoHgJo6M6Lky6/K6PjpineyJwqYi86fSK6OmLSFfgTmC4qi51zf2M0updoZGMhVf2tQ0Xr+n2e2CUQTcFnHsF+BrzPvLPNQIOB55yx5Xc/8eAixKg40FYJP6N7rgL8D2wGuN6S3HnhwKjA+8pTH5nALOBRu74LOBh4Cagbgn0OQlbg6YvcIP7/w9XfkuXpkG+XqHqG3gG6BRw/m+urqtGkKkOrAE2AbXduRbYQMobQHYJ67NSwP5hmHdaBagN3ON+x0zMaz65BPUZ+AxVxloLBwIXAtPdOQHqh6nLsUCXgHt8BXi4uPKwRc8eBdoC12DT2MYDzZL9XpX15j3DOEDzny6R1ph3sAM4QoxlGzWv60esbwoRORzrvD8VOENELtFCb2YjQX16sUJEjsemLT0K/NUd98C8mE2Yt9jKJV8E7Mi/pzD5VcMMSmvckgmqOhHzsNoAA4rzrlT1Q8yIvIE1byc4+bpAP7GpVfdgBiYYB4tIKubddQ84/z6Qp6p/BOlbKeB+amPGJRc43+myDFs/JxcY7ro2wkJEmgDXiKGny+9u4N+YwXoS81ZHA58D24qpz8BujPqqugv7XT4CBqnqye7cNcDJITy2BsC5QAd3/Af2oaga4TZquv9zXLo3sW6DF4GfsHrav5Bsa1yRNwq9uRSswzwH83DqY82bIUCbgPSNgd7YSzMbeA0zUD9jfWZ9MQ+ye4x6BXoZNYB/YrTnAMOAuU6324HHgc8wz2IE1o94RoS8rwJexppWl2J9nZcEXO8NNIikV0C9PYwZkteByu5cD+AWzFAfESKP6zAS3wcwA7QKuNRdO9/JhVv69Up3vw9hk/dXALcEXD+QEJ5XiHwOwYzIHe73O86df8Cdz8SM4qlAtyh+t5ux6XF1sb7Ttyn05AcCC7APUH49HgCkuf1+WP/v8e64p/ud6xLklQK9MEM9EvPwM3HePGZQvwPaJfv9KuvNT8eLA0SkhaouE5HO2Mt6O/a1vRXzNkar6hIRaYA94Jep6veuz6sB1nfbAvsiz1bVyXHSqyPwLeY9XYMZgSzspf8L5sFUw5pgx2BN5udUdXaY/PphRnAQZgiXY8a2FzBeVR+KoEug99MFa7L/rKpbRGQysF1VzwlIX1X39vBOx4zt/djLnoF5oj2xej0RM/rfhNF9JGYwL8PquiZmFKeq6t/C6R6UT4pa328rbDmJqsANqjrfXf8ncDpwgqquLUmeTm4AcD3QT1VzXd9yX+AUrAleGbhGVRe59H2AazFv7lPME+yBNXFfwdb7+VBV3w0q52jsY3Ym5gUux54FwQzwS1gXz5SS6r7PINnWuCJuFH6ZBev430OhZ3cNhV5YJ+AJCr+6mcB/KezbScWaZxOAc4Lzj1HHo7Cm+RisD+l5bMR4GcbgcRM2mHAP1ry8H5iMeTch+/zcPd7i9tMwL/ERVwczsKZVsBcSfHyrS/sa1j/V3J1/F8dCEqbsLGAlxlAC1nweiPUT3od9TML2VYbR/VGsqf4ZUC+Kum2FfVCauLr7GwHeKPbR6RLl73UbcJXbrxHwfFTCvLuMoPIXY15cL8yTftpd6w9sDXgGKwfINcUMbn9s1PsLXN+gq992wFHJfr+StSVdgYq8UTjo8CrW+f8I1jSZBDRx16oEydyMNa8Oc8cnYf01LxOi47+UeqW7/485A/Ig5sl9gTWT5wMDXZpuwDisaV8Hay6HbCpi3sS7wKEB52ZggzPpYWTym78pWCzeu+74HldPaQFpxwFZEe6rL+Zp93fHldx93YMbDIkgG0r3mc7QVIok69LmfwCPxj4wz2Oj6NmYEb8VyCzh77NXee53eTro3OlA2xBp2wBTgo7HAie64wuwD3TngDQNsO6SgVhXwsL8usa847sDf4v9cUu6AhV1w0Zj/4N5e5dg3kCKMzy/OwNXib09oyZYCMv77gH8BlvOcDJx6KfBYt0ewJqTmViz/Xr3EvyBDVicBqzDvNYl+S+Rk68cIe/aTve7sRHS07E+spBelTMWy4E67vhQ7IPxMEa/lN/f1SuK++uF9Z0FGsSaJZALpftcStZHmG8IT8U8+3uxgZF7MI8qG/Muh0SqvxD5nocZ6bZYn/My7KPaHBtFXoqFDJ2Dfai6Yx58DffsXR6Q16PA4IDjgcAhAccpWEjRW8DTmCfaBjPu84E+yX6nkr0lXYGKsgUbNXfuTsybOdcZhcsp7Dg/MEJeGVhf0N+w8JrO7gUPOfAQjV4Uxvw9icXwXYP1Q4F5cAe7/cOwsIou4e4vTHmNsdi+D7DmfUQDjsXxfY/zmrDBkk/z7xXrrwprUMPkeSo26HR2lHUVre6NAvarYU37U9zx0dgHMD+EpiUBYT4l0KUvNhD0HDaYcSpmsN/B+vKm47xCrI/2V6xp3NmdO8vJPoJ5998Dx4a551Zuv6nT9zpssOwT94ycEc0zsK9uSVegom1YB/xwLNi4OjbwMBhreizONzZR5HciNrIck1eIDQrcjzXBm2J9ao9gnucuAppMcaqHakD1EqY9FfNyamCd/C84A5DvGe/VFCxBnicDLRKlO/ZRe56izeqnnAHM7x45FQtFGgJUy5crQfmXYx+r+pjH1hf7SPQJSJNB4ah7Yywg/SegsTtXF+vjexXz9E4PUU519wx8jH34Dndp80e/a1Lote/XhlDVG8PiK8gexKpuf7AzXDdiI4mzKewb/BPm4TSOMv9GxBjginkOC5wOk9wLe7i7dop7iQ9Kcj2ehoVsVMOalpe4FzSpeoXRNTA0qRvwkts/DmviD3DHBzsj9QlwTHH5Bfy/D+vT6+SO6zqD+A5wYVDaM4GvMM9zsKvD9u5ak3B6B5xLB47EWjBDgfXuOQ3bctlft6QrUJ4399I+jsWnVXVG5piA60OwMIbq7rhMO6Bxy9A6Y5c/IFId6yh/OSBdarLr0unRC/ME6yRbl+Lq1f0/2n1kVmGeXA1swGYc8CHWEsjG+hAHRcrL7R9Mobc3FBsMyp/BUx/rx2wckL491p8X2Pc3GpiFeZe5QIcS3lMtLC7xDiwM6dhg/fb3zRM1REYu8CXWvBiIdXSfAPzPXX8PC+nY5o53JlqhwIn3ak+zii16c7SIzFDV1SIyHJgqIg1VdY2qJlyvkkBV33OzOz5yMZDq7qFcQVVVRDphs4QGqC029F9swORmbHCsMzb41BwbjX0hXF4AInI95h0vFZHVqnq3iNQEPheRbqq6SkSmaNF51Tswr7C7iJyHDaCsAjZgAysXq+pXJbynTW73HyIyFPPMZ5XH+k8W/HS8MAgIEt6DzTjoB8wDBrtJ9GBGsiXWv0OiHyynU/7k/9NE5BwRaYSFjGQAp4nIwVh/WhUs3qxcQS0IuKuq7innL2Jt4M9YMxWsr/MI4BVV3aKq07Em/1DgLFX9MVxGjpmoHzaY1BIbyEJVb8PFV7og6+D6+Bkb8b4Q65u8HuuaeREYoaofRHNDAdP4fgSaiaOY8zD4GSgR4OihbsQGJ67A+ltqYw/2FKw/KeSMhwTrdTnWRP8Qe8E6YoMmA7DwlUoYjdT8stRrX4Nj5fknMFJVXxeRKlhc5VWqusClqaOqvwXJdcP6gV9zx/2wfsE9WJhMH1XNE5FDVfVbNx95fQQ90lz6o7B41BtU9aNS3pNgYVc/qZvN4uGQ7HZ6ed6w6Vu3uv00zDCOwyi3sokiHCSOOnXDXogD3fHtWHO+qTtuSDnvk6tIG9a0/RI3/5pCByLkTBl37UTsw5nfj9sOG/iYFZBmMObhVaGYfjtsxLkj1j0Tdt6432LbfJ9hZMzDiFCnqnl/j7q+mx+ADVrYD1MmcM2a8zHv7ygRWaWq94iIAsucp7GkLHXa16GqUx1Dzr1ihLhrgd0aRNILBcSxVVR1hoicA4xyntgYLNg5Q0QuwOYZX4xRte0ogQ67RWQxFmj+U+A8b4/4wTeTI0BEamPTrBSL1aqKjSxfqaqry6D8mqq6xe1f6MqfiDWRtwLvqOpcd/0m4D1vDBOD4pqyLk0fbMBjl6quEZHuwChs7vDnQFcsQH8d8Iz6Zmq5gjeGxUBEGmMxYH2x4OW/qurCMii3OTZDZZSqfuEYbn5T1TFu0GQYxn04RVVnJVofj/AIHOEXkXbAs8A9qjpJbGmDF4HbVHW843mUUJ6lR3Lhm8nFQFVzgSdE5CXsIf69jIpOxyi1LhKRPzDvNM3ptFpE/o517p8sIl+p6vYy0ssjAEEj/NdgYVZvAteLyB5VnSK2Tsy7Lu24ZOrrER7eMyxnCOL9a415pA2wmSo52HSqDMxL3QL8rlHw5nkkBmKr4v0FG+DIEZErsLCcR1R1uuNwXKOqPyRVUY+w8J5hOUKQIUxV1cXOI70C6286BtiMTa/KwEYrvSFMMtzA1qlYzGGeiFyFsRPVwZYRqKSq05Kpo0fx8J5hOUGQIbwZOB5bB+NOjLFkMBZL+Igaa7YfUSxHcJ7gVVig9BJsSYG6GJ/kx6q6IonqeZQA3hiWM7iA3TsxiqUTsalep2NTsG7D+g1vp5hFmzzKFiKSjs1I+lFVfxOR/KURTlHVvORq51ESeGNYjuBmPFwA/EdVH3PnhmA07X2xQN40Vf0leVp6RIIbLb4EC9Af4MNnKg783ORyAjfVqgfWtGotIgcAqOq9GC3XaGzJSW8IyzfSsWl353pDWLHgPcMkIb/PLz9GTUQuxZhIUjHevPcxUoA1Ln1dVf01iSp7lBC+P7diwhvDJENEDlbVpY615DzMINbGVj77H/C4qq5Lpo4eHvsDfDM5iRCRpsB0EbnAzUgYD6zBlqH8BiP39DMVPDzKAN4YJhGquhILmblJRAao6i5VfQlb1Gk9tti8bxp7eJQBfNB1kqGqk0VkN3CfC979DcjDaPt989jDo4zg+wzLCUTkBCy+cBs2qX9BklXy8Niv4I1hOYKIVMNWD/gj2bp4eOxv8MbQw8PDAz+A4uHh4QF4Y+jh4eEBeGPo4eHhAXhj6OHh4QF4Y+jh4eEBeGPoEQQR2S0iX4vIIhF504X7lDav7iIyxe2fLiK3RUhb260hEm0ZfxeRW0p6PijNyyJydhRlZYuIZ6LZR+GNoUcw/lDV9qp6GDYT5qrAi2KI+rlR1Umqel+EJLWBqI2hh0e84I2hRyR8ChzkPKLvROQpYB5woIj0FJFZIjLPeZA1AETkFBFZLCKfYYS0uPMXi8gTbr+BiEwUkfluOw64D2jpvNIHXLpbRWSOiCwQkTsD8hoqIt+LyIfAIcXdhIj8xeUzX0TeDvJ2TxKRT0VkiYj0dulTROSBgLKvjLUiPco/vDH0CAkRqYwtcpS/RvQhwKuq2gFbwH4YcJKqHgnMBW521PfPA32wBawahsn+MWCmqrbDFrf6BlvS4Efnld4qIj2Bg4FOGHtPRxHpJiIdMebvDpixPboEtzNBVY925X0HXBZwLRs4AegFPOPu4TJgk6oe7fL/i9g61h77MDxRg0cwqorI127/U2wB9MbAClWd7c53Bg4FPhcRsHVZZgGtgZ9UdSmAiIzGVvYLRg/gQgBHXbZJRDKD0vR021fuuAZmHGsCE1V1mytjUgnu6TARuQtritcAAleqG+/WPV4qIsvcPfQEjgjoT6zlyl5SgrI8Kii8MfQIxh+q2j7whDN4WwNPAdNVdUBQuvbYYvfxgAD3quqzQWXcWIoyXgbOVNX5InIx0D3gWnBe6soeHLy8p4hkR1muRwWCbyZ7lAazgeNF5CAwggkRaQUsBpqLSEuXbkAY+Y+Aq51siohkAFswry8f04BLA/ois9y6MP8BzhKRqiJSE2uSF4eawGoRSQXOD7p2johUcjq3AL53ZV/t0iMirUSkegnK8ajA8J6hR9RQ1fXOwxojIlXc6WFuPecrgPdE5BfgM+CwEFncADwnIpdhTN5Xq+osEfncha687/oN2wCznGf6OzBIVeeJyDjga2xt4k9LoPId2BIKK7A+0ECj+z0wE2gAXKWq20XkBawvcZ5Y4euBM0tWOx4VFZ61xsPDwwPfTPbw8PAAvDH08PDwALwx9PDw8AC8MfTw8PAAvDH08PDwALwx9PDw8AC8MfTw8PAA4P8Bs5LzakNqs6MAAAAASUVORK5CYII=
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Final-Model-Training-using-kfold">Final Model Training using kfold<a class="anchor-link" href="#Final-Model-Training-using-kfold">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">11</span> <span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">X_train</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">20</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">i</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">train_index</span><span class="p">,</span><span class="n">validate_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">):</span>
    <span class="n">i</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span>
    <span class="n">model</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">model</span><span class="o">=</span><span class="n">_model</span><span class="p">()</span>
    <span class="nb">print</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;</span><span class="se">\n\n\n</span><span class="s2"> trianing </span><span class="se">\n\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">checkpointer</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span><span class="o">=</span><span class="s1">&#39;weights&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;.hdf5&#39;</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">train_index</span><span class="p">]),</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> 
     <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">validate_index</span><span class="p">],</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">validate_index</span><span class="p">])),</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpointer</span><span class="p">]</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>1


 trianing 



Train on 17885 samples, validate on 1795 samples
Epoch 1/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 5.6827 - acc: 0.0953Epoch 00001: val_loss improved from inf to 2.47160, saving model to weights1.hdf5
17885/17885 [==============================] - 3s 147us/step - loss: 5.5532 - acc: 0.0948 - val_loss: 2.4716 - val_acc: 0.0958
Epoch 2/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4791 - acc: 0.1007Epoch 00002: val_loss improved from 2.47160 to 2.43523, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 2.4783 - acc: 0.1006 - val_loss: 2.4352 - val_acc: 0.1075
Epoch 3/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4423 - acc: 0.1097Epoch 00003: val_loss improved from 2.43523 to 2.43117, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4422 - acc: 0.1093 - val_loss: 2.4312 - val_acc: 0.1075
Epoch 4/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4340 - acc: 0.1098Epoch 00004: val_loss improved from 2.43117 to 2.42977, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 2.4347 - acc: 0.1089 - val_loss: 2.4298 - val_acc: 0.1081
Epoch 5/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4315 - acc: 0.1092Epoch 00005: val_loss improved from 2.42977 to 2.42890, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.4308 - acc: 0.1093 - val_loss: 2.4289 - val_acc: 0.1081
Epoch 6/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4280 - acc: 0.1094Epoch 00006: val_loss improved from 2.42890 to 2.42821, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4282 - acc: 0.1095 - val_loss: 2.4282 - val_acc: 0.1081
Epoch 7/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4272 - acc: 0.1071Epoch 00007: val_loss improved from 2.42821 to 2.42766, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4276 - acc: 0.1080 - val_loss: 2.4277 - val_acc: 0.1081
Epoch 8/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4278 - acc: 0.1087Epoch 00008: val_loss improved from 2.42766 to 2.42718, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.4269 - acc: 0.1098 - val_loss: 2.4272 - val_acc: 0.1081
Epoch 9/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4280 - acc: 0.1086Epoch 00009: val_loss improved from 2.42718 to 2.42679, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4271 - acc: 0.1082 - val_loss: 2.4268 - val_acc: 0.1081
Epoch 10/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4260 - acc: 0.1090Epoch 00010: val_loss improved from 2.42679 to 2.42645, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4256 - acc: 0.1093 - val_loss: 2.4265 - val_acc: 0.1081
Epoch 11/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4250 - acc: 0.1090Epoch 00011: val_loss improved from 2.42645 to 2.42615, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 2.4248 - acc: 0.1100 - val_loss: 2.4261 - val_acc: 0.1081
Epoch 12/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4241 - acc: 0.1086Epoch 00012: val_loss improved from 2.42615 to 2.42589, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 2.4244 - acc: 0.1087 - val_loss: 2.4259 - val_acc: 0.1081
Epoch 13/250
17664/17885 [============================&gt;.] - ETA: 0s - loss: 2.4237 - acc: 0.1095Epoch 00013: val_loss improved from 2.42589 to 2.42568, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 2.4241 - acc: 0.1094 - val_loss: 2.4257 - val_acc: 0.1081
Epoch 14/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4244 - acc: 0.1094Epoch 00014: val_loss improved from 2.42568 to 2.42550, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.4241 - acc: 0.1095 - val_loss: 2.4255 - val_acc: 0.1081
Epoch 15/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4232 - acc: 0.1095Epoch 00015: val_loss improved from 2.42550 to 2.42535, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4230 - acc: 0.1096 - val_loss: 2.4254 - val_acc: 0.1081
Epoch 16/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4234 - acc: 0.1089Epoch 00016: val_loss improved from 2.42535 to 2.42523, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4227 - acc: 0.1092 - val_loss: 2.4252 - val_acc: 0.1081
Epoch 17/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4239 - acc: 0.1100Epoch 00017: val_loss improved from 2.42523 to 2.42512, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4238 - acc: 0.1097 - val_loss: 2.4251 - val_acc: 0.1081
Epoch 18/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4244 - acc: 0.1086- ETA: 0s - loss: 2.4236 - acc: 0.108Epoch 00018: val_loss improved from 2.42512 to 2.42504, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.4234 - acc: 0.1090 - val_loss: 2.4250 - val_acc: 0.1081
Epoch 19/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4216 - acc: 0.1070Epoch 00019: val_loss improved from 2.42504 to 2.41733, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.4206 - acc: 0.1081 - val_loss: 2.4173 - val_acc: 0.1231
Epoch 20/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4025 - acc: 0.1322Epoch 00020: val_loss improved from 2.41733 to 2.38180, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4025 - acc: 0.1327 - val_loss: 2.3818 - val_acc: 0.1526
Epoch 21/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.3717 - acc: 0.1435Epoch 00021: val_loss improved from 2.38180 to 2.33376, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.3709 - acc: 0.1436 - val_loss: 2.3338 - val_acc: 0.1604
Epoch 22/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.3413 - acc: 0.1491Epoch 00022: val_loss improved from 2.33376 to 2.26483, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.3385 - acc: 0.1506 - val_loss: 2.2648 - val_acc: 0.1705
Epoch 23/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.2536 - acc: 0.1774Epoch 00023: val_loss improved from 2.26483 to 2.09905, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.2521 - acc: 0.1781 - val_loss: 2.0991 - val_acc: 0.2513
Epoch 24/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.1193 - acc: 0.2308Epoch 00024: val_loss improved from 2.09905 to 1.87145, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.1145 - acc: 0.2327 - val_loss: 1.8715 - val_acc: 0.3471
Epoch 25/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.9313 - acc: 0.3074Epoch 00025: val_loss improved from 1.87145 to 1.63858, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 1.9265 - acc: 0.3085 - val_loss: 1.6386 - val_acc: 0.4150
Epoch 26/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.7690 - acc: 0.3670Epoch 00026: val_loss improved from 1.63858 to 1.49420, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 1.7651 - acc: 0.3672 - val_loss: 1.4942 - val_acc: 0.4780
Epoch 27/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.6313 - acc: 0.4188Epoch 00027: val_loss improved from 1.49420 to 1.42845, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 1.6314 - acc: 0.4195 - val_loss: 1.4285 - val_acc: 0.5075
Epoch 28/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.5051 - acc: 0.4637Epoch 00028: val_loss improved from 1.42845 to 1.31203, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 1.5045 - acc: 0.4643 - val_loss: 1.3120 - val_acc: 0.5376
Epoch 29/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.4001 - acc: 0.5002Epoch 00029: val_loss improved from 1.31203 to 1.08448, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 1.3951 - acc: 0.5009 - val_loss: 1.0845 - val_acc: 0.6396
Epoch 30/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.3085 - acc: 0.5349Epoch 00030: val_loss improved from 1.08448 to 1.02065, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 1.3080 - acc: 0.5358 - val_loss: 1.0207 - val_acc: 0.6596
Epoch 31/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.2302 - acc: 0.5694Epoch 00031: val_loss improved from 1.02065 to 0.97711, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 1.2301 - acc: 0.5704 - val_loss: 0.9771 - val_acc: 0.6758
Epoch 32/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.1714 - acc: 0.5926Epoch 00032: val_loss improved from 0.97711 to 0.89050, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 1.1693 - acc: 0.5929 - val_loss: 0.8905 - val_acc: 0.7103
Epoch 33/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.0907 - acc: 0.6193Epoch 00033: val_loss improved from 0.89050 to 0.82108, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 1.0883 - acc: 0.6211 - val_loss: 0.8211 - val_acc: 0.7309
Epoch 34/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.0622 - acc: 0.6275Epoch 00034: val_loss improved from 0.82108 to 0.79389, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 1.0592 - acc: 0.6292 - val_loss: 0.7939 - val_acc: 0.7382
Epoch 35/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.0047 - acc: 0.6529Epoch 00035: val_loss improved from 0.79389 to 0.74931, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 1.0015 - acc: 0.6537 - val_loss: 0.7493 - val_acc: 0.7482
Epoch 36/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.9559 - acc: 0.6659Epoch 00036: val_loss improved from 0.74931 to 0.69882, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.9545 - acc: 0.6664 - val_loss: 0.6988 - val_acc: 0.7755
Epoch 37/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.9234 - acc: 0.6799Epoch 00037: val_loss improved from 0.69882 to 0.66949, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 90us/step - loss: 0.9209 - acc: 0.6804 - val_loss: 0.6695 - val_acc: 0.7777
Epoch 38/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.8853 - acc: 0.6935Epoch 00038: val_loss improved from 0.66949 to 0.64638, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 91us/step - loss: 0.8848 - acc: 0.6946 - val_loss: 0.6464 - val_acc: 0.7861
Epoch 39/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.8595 - acc: 0.7045Epoch 00039: val_loss improved from 0.64638 to 0.61502, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 90us/step - loss: 0.8594 - acc: 0.7047 - val_loss: 0.6150 - val_acc: 0.7989
Epoch 40/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.8290 - acc: 0.7115Epoch 00040: val_loss improved from 0.61502 to 0.58541, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 91us/step - loss: 0.8235 - acc: 0.7128 - val_loss: 0.5854 - val_acc: 0.8056
Epoch 41/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.8004 - acc: 0.7260Epoch 00041: val_loss improved from 0.58541 to 0.57191, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 91us/step - loss: 0.8001 - acc: 0.7266 - val_loss: 0.5719 - val_acc: 0.8100
Epoch 42/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.7806 - acc: 0.7341Epoch 00042: val_loss improved from 0.57191 to 0.56430, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.7783 - acc: 0.7354 - val_loss: 0.5643 - val_acc: 0.8089
Epoch 43/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.7533 - acc: 0.7395Epoch 00043: val_loss improved from 0.56430 to 0.53556, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.7539 - acc: 0.7399 - val_loss: 0.5356 - val_acc: 0.8189
Epoch 44/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.7414 - acc: 0.7499Epoch 00044: val_loss improved from 0.53556 to 0.52584, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.7424 - acc: 0.7496 - val_loss: 0.5258 - val_acc: 0.8273
Epoch 45/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.7248 - acc: 0.7508Epoch 00045: val_loss improved from 0.52584 to 0.50879, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 91us/step - loss: 0.7240 - acc: 0.7515 - val_loss: 0.5088 - val_acc: 0.8251
Epoch 46/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6914 - acc: 0.7656Epoch 00046: val_loss improved from 0.50879 to 0.49487, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.6919 - acc: 0.7648 - val_loss: 0.4949 - val_acc: 0.8323
Epoch 47/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6817 - acc: 0.7701Epoch 00047: val_loss improved from 0.49487 to 0.48851, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.6797 - acc: 0.7701 - val_loss: 0.4885 - val_acc: 0.8345
Epoch 48/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6789 - acc: 0.7708Epoch 00048: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.6806 - acc: 0.7704 - val_loss: 0.4933 - val_acc: 0.8362
Epoch 49/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6574 - acc: 0.7759Epoch 00049: val_loss improved from 0.48851 to 0.47056, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.6556 - acc: 0.7768 - val_loss: 0.4706 - val_acc: 0.8384
Epoch 50/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6510 - acc: 0.7836Epoch 00050: val_loss improved from 0.47056 to 0.46212, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.6526 - acc: 0.7835 - val_loss: 0.4621 - val_acc: 0.8490
Epoch 51/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6432 - acc: 0.7801Epoch 00051: val_loss improved from 0.46212 to 0.45240, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 91us/step - loss: 0.6413 - acc: 0.7809 - val_loss: 0.4524 - val_acc: 0.8462
Epoch 52/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6216 - acc: 0.7895Epoch 00052: val_loss improved from 0.45240 to 0.44292, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.6249 - acc: 0.7884 - val_loss: 0.4429 - val_acc: 0.8529
Epoch 53/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6190 - acc: 0.7893Epoch 00053: val_loss improved from 0.44292 to 0.43327, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.6170 - acc: 0.7897 - val_loss: 0.4333 - val_acc: 0.8607
Epoch 54/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6038 - acc: 0.7966Epoch 00054: val_loss improved from 0.43327 to 0.42979, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.6019 - acc: 0.7971 - val_loss: 0.4298 - val_acc: 0.8546
Epoch 55/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5948 - acc: 0.7989Epoch 00055: val_loss improved from 0.42979 to 0.41708, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.5954 - acc: 0.7989 - val_loss: 0.4171 - val_acc: 0.8602
Epoch 56/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5880 - acc: 0.8034Epoch 00056: val_loss improved from 0.41708 to 0.41507, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.5882 - acc: 0.8037 - val_loss: 0.4151 - val_acc: 0.8607
Epoch 57/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5751 - acc: 0.8078Epoch 00057: val_loss improved from 0.41507 to 0.41135, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.5740 - acc: 0.8080 - val_loss: 0.4113 - val_acc: 0.8635
Epoch 58/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5701 - acc: 0.8075Epoch 00058: val_loss improved from 0.41135 to 0.40212, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.5694 - acc: 0.8082 - val_loss: 0.4021 - val_acc: 0.8669
Epoch 59/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5632 - acc: 0.8104Epoch 00059: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.5608 - acc: 0.8115 - val_loss: 0.4080 - val_acc: 0.8641
Epoch 60/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5483 - acc: 0.8179Epoch 00060: val_loss improved from 0.40212 to 0.39500, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.5470 - acc: 0.8185 - val_loss: 0.3950 - val_acc: 0.8691
Epoch 61/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5376 - acc: 0.8176Epoch 00061: val_loss improved from 0.39500 to 0.38580, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.5388 - acc: 0.8174 - val_loss: 0.3858 - val_acc: 0.8685
Epoch 62/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5294 - acc: 0.8224- ETA: 0s - loss: 0.5213 - acc: 0.Epoch 00062: val_loss improved from 0.38580 to 0.38516, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.5286 - acc: 0.8229 - val_loss: 0.3852 - val_acc: 0.8730
Epoch 63/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5355 - acc: 0.8185Epoch 00063: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.5356 - acc: 0.8187 - val_loss: 0.3866 - val_acc: 0.8691
Epoch 64/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5217 - acc: 0.8232Epoch 00064: val_loss improved from 0.38516 to 0.37728, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.5227 - acc: 0.8228 - val_loss: 0.3773 - val_acc: 0.8741
Epoch 65/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5092 - acc: 0.8300Epoch 00065: val_loss improved from 0.37728 to 0.37704, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.5095 - acc: 0.8302 - val_loss: 0.3770 - val_acc: 0.8669
Epoch 66/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5177 - acc: 0.8284Epoch 00066: val_loss improved from 0.37704 to 0.36769, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.5161 - acc: 0.8293 - val_loss: 0.3677 - val_acc: 0.8741
Epoch 67/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4934 - acc: 0.8347Epoch 00067: val_loss improved from 0.36769 to 0.36319, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.4934 - acc: 0.8344 - val_loss: 0.3632 - val_acc: 0.8735
Epoch 68/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4908 - acc: 0.8317Epoch 00068: val_loss improved from 0.36319 to 0.34971, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.4895 - acc: 0.8318 - val_loss: 0.3497 - val_acc: 0.8802
Epoch 69/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4833 - acc: 0.8400Epoch 00069: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.4811 - acc: 0.8408 - val_loss: 0.3546 - val_acc: 0.8769
Epoch 70/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4817 - acc: 0.8392Epoch 00070: val_loss improved from 0.34971 to 0.34821, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4826 - acc: 0.8392 - val_loss: 0.3482 - val_acc: 0.8786
Epoch 71/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4852 - acc: 0.8382Epoch 00071: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.4853 - acc: 0.8379 - val_loss: 0.3490 - val_acc: 0.8769
Epoch 72/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4686 - acc: 0.8424Epoch 00072: val_loss improved from 0.34821 to 0.34779, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4674 - acc: 0.8428 - val_loss: 0.3478 - val_acc: 0.8780
Epoch 73/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4671 - acc: 0.8431Epoch 00073: val_loss improved from 0.34779 to 0.34522, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.4663 - acc: 0.8434 - val_loss: 0.3452 - val_acc: 0.8819
Epoch 74/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4556 - acc: 0.8457Epoch 00074: val_loss improved from 0.34522 to 0.34498, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4550 - acc: 0.8455 - val_loss: 0.3450 - val_acc: 0.8836
Epoch 75/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4575 - acc: 0.8437Epoch 00075: val_loss improved from 0.34498 to 0.33927, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.4558 - acc: 0.8442 - val_loss: 0.3393 - val_acc: 0.8852
Epoch 76/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4585 - acc: 0.8438Epoch 00076: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.4589 - acc: 0.8438 - val_loss: 0.3400 - val_acc: 0.8808
Epoch 77/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4481 - acc: 0.8498Epoch 00077: val_loss improved from 0.33927 to 0.33486, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4476 - acc: 0.8504 - val_loss: 0.3349 - val_acc: 0.8830
Epoch 78/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4478 - acc: 0.8470Epoch 00078: val_loss improved from 0.33486 to 0.32776, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4450 - acc: 0.8483 - val_loss: 0.3278 - val_acc: 0.8847
Epoch 79/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4349 - acc: 0.8518Epoch 00079: val_loss improved from 0.32776 to 0.31975, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4320 - acc: 0.8532 - val_loss: 0.3197 - val_acc: 0.8869
Epoch 80/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4249 - acc: 0.8559Epoch 00080: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.4274 - acc: 0.8549 - val_loss: 0.3245 - val_acc: 0.8875
Epoch 81/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4362 - acc: 0.8544Epoch 00081: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.4386 - acc: 0.8539 - val_loss: 0.3235 - val_acc: 0.8886
Epoch 82/250
17664/17885 [============================&gt;.] - ETA: 0s - loss: 0.4332 - acc: 0.8542Epoch 00082: val_loss improved from 0.31975 to 0.31706, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.4326 - acc: 0.8546 - val_loss: 0.3171 - val_acc: 0.8908
Epoch 83/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4305 - acc: 0.8567Epoch 00083: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.4281 - acc: 0.8577 - val_loss: 0.3171 - val_acc: 0.8903
Epoch 84/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4183 - acc: 0.8598Epoch 00084: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.4186 - acc: 0.8596 - val_loss: 0.3221 - val_acc: 0.8852
Epoch 85/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4123 - acc: 0.8609Epoch 00085: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.4124 - acc: 0.8613 - val_loss: 0.3188 - val_acc: 0.8875
Epoch 86/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4230 - acc: 0.8586Epoch 00086: val_loss improved from 0.31706 to 0.30681, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 95us/step - loss: 0.4225 - acc: 0.8589 - val_loss: 0.3068 - val_acc: 0.8953
Epoch 87/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4167 - acc: 0.8583Epoch 00087: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.4187 - acc: 0.8575 - val_loss: 0.3097 - val_acc: 0.8897
Epoch 88/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4073 - acc: 0.8600Epoch 00088: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.4078 - acc: 0.8601 - val_loss: 0.3104 - val_acc: 0.8914
Epoch 89/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4075 - acc: 0.8624Epoch 00089: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.4075 - acc: 0.8628 - val_loss: 0.3122 - val_acc: 0.8903
Epoch 90/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4093 - acc: 0.8658Epoch 00090: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.4085 - acc: 0.8658 - val_loss: 0.3085 - val_acc: 0.8897
Epoch 91/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3992 - acc: 0.8649Epoch 00091: val_loss improved from 0.30681 to 0.30139, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3976 - acc: 0.8657 - val_loss: 0.3014 - val_acc: 0.8969
Epoch 92/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3950 - acc: 0.8667Epoch 00092: val_loss did not improve
17885/17885 [==============================] - 2s 93us/step - loss: 0.3961 - acc: 0.8670 - val_loss: 0.3029 - val_acc: 0.8947
Epoch 93/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3869 - acc: 0.8693Epoch 00093: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3900 - acc: 0.8687 - val_loss: 0.3016 - val_acc: 0.8981
Epoch 94/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3867 - acc: 0.8708Epoch 00094: val_loss improved from 0.30139 to 0.29452, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3871 - acc: 0.8705 - val_loss: 0.2945 - val_acc: 0.9008
Epoch 95/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3897 - acc: 0.8707Epoch 00095: val_loss did not improve
17885/17885 [==============================] - 2s 90us/step - loss: 0.3895 - acc: 0.8710 - val_loss: 0.2966 - val_acc: 0.9014
Epoch 96/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3927 - acc: 0.8710Epoch 00096: val_loss did not improve
17885/17885 [==============================] - 2s 90us/step - loss: 0.3903 - acc: 0.8718 - val_loss: 0.3053 - val_acc: 0.8925
Epoch 97/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3782 - acc: 0.8728Epoch 00097: val_loss improved from 0.29452 to 0.29350, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.3790 - acc: 0.8728 - val_loss: 0.2935 - val_acc: 0.8964
Epoch 98/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3760 - acc: 0.8727Epoch 00098: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3758 - acc: 0.8729 - val_loss: 0.2939 - val_acc: 0.9008
Epoch 99/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3743 - acc: 0.8740Epoch 00099: val_loss improved from 0.29350 to 0.28958, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.3750 - acc: 0.8736 - val_loss: 0.2896 - val_acc: 0.8997
Epoch 100/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3737 - acc: 0.8762Epoch 00100: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3737 - acc: 0.8764 - val_loss: 0.2942 - val_acc: 0.9003
Epoch 101/250
17664/17885 [============================&gt;.] - ETA: 0s - loss: 0.3704 - acc: 0.8758Epoch 00101: val_loss improved from 0.28958 to 0.28897, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3705 - acc: 0.8760 - val_loss: 0.2890 - val_acc: 0.8997
Epoch 102/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3680 - acc: 0.8747Epoch 00102: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3671 - acc: 0.8743 - val_loss: 0.2893 - val_acc: 0.9047
Epoch 103/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3591 - acc: 0.8797Epoch 00103: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3598 - acc: 0.8794 - val_loss: 0.2899 - val_acc: 0.8942
Epoch 104/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3560 - acc: 0.8806Epoch 00104: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3550 - acc: 0.8810 - val_loss: 0.2890 - val_acc: 0.8992
Epoch 105/250
17664/17885 [============================&gt;.] - ETA: 0s - loss: 0.3538 - acc: 0.8798Epoch 00105: val_loss improved from 0.28897 to 0.28319, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.3544 - acc: 0.8797 - val_loss: 0.2832 - val_acc: 0.9053
Epoch 106/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3571 - acc: 0.8790Epoch 00106: val_loss did not improve
17885/17885 [==============================] - 2s 93us/step - loss: 0.3574 - acc: 0.8788 - val_loss: 0.2884 - val_acc: 0.9008
Epoch 107/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3547 - acc: 0.8801Epoch 00107: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3549 - acc: 0.8801 - val_loss: 0.2874 - val_acc: 0.8997
Epoch 108/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3502 - acc: 0.8809Epoch 00108: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3515 - acc: 0.8808 - val_loss: 0.2878 - val_acc: 0.9025
Epoch 109/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3414 - acc: 0.8851Epoch 00109: val_loss improved from 0.28319 to 0.28189, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3420 - acc: 0.8849 - val_loss: 0.2819 - val_acc: 0.8992
Epoch 110/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3453 - acc: 0.8845Epoch 00110: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3461 - acc: 0.8842 - val_loss: 0.2835 - val_acc: 0.9008
Epoch 111/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3414 - acc: 0.8842Epoch 00111: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3440 - acc: 0.8830 - val_loss: 0.2853 - val_acc: 0.9003
Epoch 112/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3422 - acc: 0.8853Epoch 00112: val_loss improved from 0.28189 to 0.27979, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3418 - acc: 0.8852 - val_loss: 0.2798 - val_acc: 0.9019
Epoch 113/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3430 - acc: 0.8819Epoch 00113: val_loss improved from 0.27979 to 0.27809, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3448 - acc: 0.8817 - val_loss: 0.2781 - val_acc: 0.9047
Epoch 114/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3362 - acc: 0.8851Epoch 00114: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3370 - acc: 0.8847 - val_loss: 0.2785 - val_acc: 0.9014
Epoch 115/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3383 - acc: 0.8857Epoch 00115: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3366 - acc: 0.8862 - val_loss: 0.2799 - val_acc: 0.9019
Epoch 116/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3481 - acc: 0.8829Epoch 00116: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3475 - acc: 0.8831 - val_loss: 0.2825 - val_acc: 0.9014
Epoch 117/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3291 - acc: 0.8887Epoch 00117: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3297 - acc: 0.8888 - val_loss: 0.2800 - val_acc: 0.9047
Epoch 118/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3284 - acc: 0.8889Epoch 00118: val_loss did not improve
17885/17885 [==============================] - 2s 90us/step - loss: 0.3281 - acc: 0.8888 - val_loss: 0.2817 - val_acc: 0.9053
Epoch 119/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3313 - acc: 0.8875Epoch 00119: val_loss improved from 0.27809 to 0.27695, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.3318 - acc: 0.8873 - val_loss: 0.2769 - val_acc: 0.9058
Epoch 120/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3239 - acc: 0.8917- ETA: 0s - loss: 0.3200 - accEpoch 00120: val_loss improved from 0.27695 to 0.27303, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.3252 - acc: 0.8915 - val_loss: 0.2730 - val_acc: 0.9042
Epoch 121/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3274 - acc: 0.8869Epoch 00121: val_loss did not improve
17885/17885 [==============================] - 2s 90us/step - loss: 0.3287 - acc: 0.8867 - val_loss: 0.2765 - val_acc: 0.9064
Epoch 122/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3244 - acc: 0.8897Epoch 00122: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3243 - acc: 0.8897 - val_loss: 0.2747 - val_acc: 0.9053
Epoch 123/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3268 - acc: 0.8893- ETA: 0s - loss: 0.3309 - acc: Epoch 00123: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3251 - acc: 0.8902 - val_loss: 0.2790 - val_acc: 0.9003
Epoch 124/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3216 - acc: 0.8921Epoch 00124: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3234 - acc: 0.8910 - val_loss: 0.2776 - val_acc: 0.9081
Epoch 125/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3236 - acc: 0.8932Epoch 00125: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3233 - acc: 0.8932 - val_loss: 0.2735 - val_acc: 0.9109
Epoch 126/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3219 - acc: 0.8915Epoch 00126: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3234 - acc: 0.8912 - val_loss: 0.2742 - val_acc: 0.9070
Epoch 127/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3138 - acc: 0.8944Epoch 00127: val_loss improved from 0.27303 to 0.27042, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3149 - acc: 0.8947 - val_loss: 0.2704 - val_acc: 0.9114
Epoch 128/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3200 - acc: 0.8928Epoch 00128: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3185 - acc: 0.8937 - val_loss: 0.2722 - val_acc: 0.9058
Epoch 129/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3175 - acc: 0.8920Epoch 00129: val_loss improved from 0.27042 to 0.27017, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3191 - acc: 0.8916 - val_loss: 0.2702 - val_acc: 0.9047
Epoch 130/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3159 - acc: 0.8940Epoch 00130: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3165 - acc: 0.8935 - val_loss: 0.2749 - val_acc: 0.9081
Epoch 131/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3097 - acc: 0.8934Epoch 00131: val_loss improved from 0.27017 to 0.26884, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.3104 - acc: 0.8935 - val_loss: 0.2688 - val_acc: 0.9097
Epoch 132/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3099 - acc: 0.8955Epoch 00132: val_loss improved from 0.26884 to 0.26818, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3099 - acc: 0.8952 - val_loss: 0.2682 - val_acc: 0.9058
Epoch 133/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3037 - acc: 0.8976Epoch 00133: val_loss did not improve
17885/17885 [==============================] - 2s 93us/step - loss: 0.3036 - acc: 0.8973 - val_loss: 0.2749 - val_acc: 0.9064
Epoch 134/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3067 - acc: 0.8963Epoch 00134: val_loss improved from 0.26818 to 0.26798, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3075 - acc: 0.8957 - val_loss: 0.2680 - val_acc: 0.9064
Epoch 135/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3071 - acc: 0.8964Epoch 00135: val_loss improved from 0.26798 to 0.26752, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3052 - acc: 0.8967 - val_loss: 0.2675 - val_acc: 0.9081
Epoch 136/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3036 - acc: 0.8966Epoch 00136: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3018 - acc: 0.8969 - val_loss: 0.2693 - val_acc: 0.9047
Epoch 137/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2962 - acc: 0.8987Epoch 00137: val_loss improved from 0.26752 to 0.26698, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.2979 - acc: 0.8982 - val_loss: 0.2670 - val_acc: 0.9092
Epoch 138/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2959 - acc: 0.8994Epoch 00138: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2964 - acc: 0.8996 - val_loss: 0.2688 - val_acc: 0.9064
Epoch 139/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3042 - acc: 0.8982Epoch 00139: val_loss improved from 0.26698 to 0.26630, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3062 - acc: 0.8979 - val_loss: 0.2663 - val_acc: 0.9092
Epoch 140/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2918 - acc: 0.9005Epoch 00140: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2931 - acc: 0.9002 - val_loss: 0.2729 - val_acc: 0.9070
Epoch 141/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2953 - acc: 0.9000Epoch 00141: val_loss did not improve
17885/17885 [==============================] - 2s 93us/step - loss: 0.2945 - acc: 0.9003 - val_loss: 0.2688 - val_acc: 0.9058
Epoch 142/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2907 - acc: 0.9002Epoch 00142: val_loss improved from 0.26630 to 0.26626, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.2914 - acc: 0.9001 - val_loss: 0.2663 - val_acc: 0.9097
Epoch 143/250
17664/17885 [============================&gt;.] - ETA: 0s - loss: 0.2912 - acc: 0.9017Epoch 00143: val_loss improved from 0.26626 to 0.26209, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.2918 - acc: 0.9016 - val_loss: 0.2621 - val_acc: 0.9097
Epoch 144/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2890 - acc: 0.9012Epoch 00144: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2905 - acc: 0.9011 - val_loss: 0.2626 - val_acc: 0.9075
Epoch 145/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2934 - acc: 0.9004Epoch 00145: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2928 - acc: 0.9006 - val_loss: 0.2634 - val_acc: 0.9109
Epoch 146/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2971 - acc: 0.8986Epoch 00146: val_loss improved from 0.26209 to 0.25874, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.2959 - acc: 0.8992 - val_loss: 0.2587 - val_acc: 0.9114
Epoch 147/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2926 - acc: 0.8987Epoch 00147: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2939 - acc: 0.8984 - val_loss: 0.2590 - val_acc: 0.9097
Epoch 148/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2835 - acc: 0.9033Epoch 00148: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2838 - acc: 0.9034 - val_loss: 0.2620 - val_acc: 0.9070
Epoch 149/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2803 - acc: 0.9071Epoch 00149: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2794 - acc: 0.9072 - val_loss: 0.2621 - val_acc: 0.9120
Epoch 150/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2900 - acc: 0.9019Epoch 00150: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2905 - acc: 0.9014 - val_loss: 0.2627 - val_acc: 0.9114
Epoch 151/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2871 - acc: 0.9023Epoch 00151: val_loss improved from 0.25874 to 0.25682, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.2863 - acc: 0.9028 - val_loss: 0.2568 - val_acc: 0.9142
Epoch 152/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2818 - acc: 0.9061Epoch 00152: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2810 - acc: 0.9064 - val_loss: 0.2645 - val_acc: 0.9070
Epoch 153/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2784 - acc: 0.9052Epoch 00153: val_loss did not improve
17885/17885 [==============================] - 2s 90us/step - loss: 0.2778 - acc: 0.9052 - val_loss: 0.2631 - val_acc: 0.9103
Epoch 154/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2811 - acc: 0.9061Epoch 00154: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2823 - acc: 0.9057 - val_loss: 0.2602 - val_acc: 0.9136
Epoch 155/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2765 - acc: 0.9062Epoch 00155: val_loss improved from 0.25682 to 0.25506, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 91us/step - loss: 0.2761 - acc: 0.9062 - val_loss: 0.2551 - val_acc: 0.9092
Epoch 156/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2722 - acc: 0.9063Epoch 00156: val_loss did not improve
17885/17885 [==============================] - 2s 89us/step - loss: 0.2716 - acc: 0.9065 - val_loss: 0.2573 - val_acc: 0.9125
Epoch 157/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2791 - acc: 0.9065Epoch 00157: val_loss did not improve
17885/17885 [==============================] - 2s 90us/step - loss: 0.2784 - acc: 0.9067 - val_loss: 0.2575 - val_acc: 0.9103
Epoch 158/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2681 - acc: 0.9081Epoch 00158: val_loss did not improve
17885/17885 [==============================] - 2s 90us/step - loss: 0.2691 - acc: 0.9074 - val_loss: 0.2555 - val_acc: 0.9114
Epoch 159/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2746 - acc: 0.9050Epoch 00159: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2755 - acc: 0.9049 - val_loss: 0.2599 - val_acc: 0.9125
Epoch 160/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2740 - acc: 0.9054Epoch 00160: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2752 - acc: 0.9049 - val_loss: 0.2624 - val_acc: 0.9097
Epoch 161/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2755 - acc: 0.9055Epoch 00161: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2735 - acc: 0.9061 - val_loss: 0.2654 - val_acc: 0.9125
Epoch 162/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2716 - acc: 0.9058Epoch 00162: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2704 - acc: 0.9064 - val_loss: 0.2602 - val_acc: 0.9131
Epoch 163/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2697 - acc: 0.9096Epoch 00163: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2719 - acc: 0.9090 - val_loss: 0.2748 - val_acc: 0.9114
Epoch 164/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2582 - acc: 0.9137Epoch 00164: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2584 - acc: 0.9137 - val_loss: 0.2618 - val_acc: 0.9125
Epoch 165/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2645 - acc: 0.9102Epoch 00165: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2659 - acc: 0.9101 - val_loss: 0.2626 - val_acc: 0.9125
Epoch 166/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2612 - acc: 0.9095Epoch 00166: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2621 - acc: 0.9091 - val_loss: 0.2588 - val_acc: 0.9120
Epoch 167/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2672 - acc: 0.9105Epoch 00167: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2661 - acc: 0.9107 - val_loss: 0.2582 - val_acc: 0.9125
Epoch 168/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2681 - acc: 0.9092Epoch 00168: val_loss improved from 0.25506 to 0.25252, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.2684 - acc: 0.9090 - val_loss: 0.2525 - val_acc: 0.9131
Epoch 169/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2618 - acc: 0.9110Epoch 00169: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2612 - acc: 0.9113 - val_loss: 0.2533 - val_acc: 0.9142
Epoch 170/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2644 - acc: 0.9094Epoch 00170: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2630 - acc: 0.9100 - val_loss: 0.2561 - val_acc: 0.9097
Epoch 171/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2659 - acc: 0.9106Epoch 00171: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2636 - acc: 0.9115 - val_loss: 0.2553 - val_acc: 0.9153
Epoch 172/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2652 - acc: 0.9095Epoch 00172: val_loss improved from 0.25252 to 0.25077, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.2643 - acc: 0.9099 - val_loss: 0.2508 - val_acc: 0.9164
Epoch 173/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2657 - acc: 0.9086Epoch 00173: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2660 - acc: 0.9089 - val_loss: 0.2572 - val_acc: 0.9153
Epoch 174/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2614 - acc: 0.9116Epoch 00174: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2603 - acc: 0.9118 - val_loss: 0.2516 - val_acc: 0.9175
Epoch 175/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2611 - acc: 0.9110Epoch 00175: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2613 - acc: 0.9106 - val_loss: 0.2554 - val_acc: 0.9114
Epoch 176/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2600 - acc: 0.9099Epoch 00176: val_loss did not improve
17885/17885 [==============================] - 2s 93us/step - loss: 0.2603 - acc: 0.9098 - val_loss: 0.2562 - val_acc: 0.9198
Epoch 177/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2629 - acc: 0.9114Epoch 00177: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2623 - acc: 0.9120 - val_loss: 0.2533 - val_acc: 0.9181
Epoch 178/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2553 - acc: 0.9120Epoch 00178: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2547 - acc: 0.9124 - val_loss: 0.2538 - val_acc: 0.9142
Epoch 179/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2501 - acc: 0.9121Epoch 00179: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2506 - acc: 0.9115 - val_loss: 0.2602 - val_acc: 0.9103
Epoch 180/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2494 - acc: 0.9142Epoch 00180: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2496 - acc: 0.9138 - val_loss: 0.2673 - val_acc: 0.9114
Epoch 181/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2507 - acc: 0.9146Epoch 00181: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2509 - acc: 0.9144 - val_loss: 0.2527 - val_acc: 0.9170
Epoch 182/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2418 - acc: 0.9172Epoch 00182: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2426 - acc: 0.9175 - val_loss: 0.2528 - val_acc: 0.9170
Epoch 183/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2522 - acc: 0.9133Epoch 00183: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2535 - acc: 0.9133 - val_loss: 0.2547 - val_acc: 0.9131
Epoch 184/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2544 - acc: 0.9124Epoch 00184: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2562 - acc: 0.9125 - val_loss: 0.2557 - val_acc: 0.9175
Epoch 185/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2510 - acc: 0.9139Epoch 00185: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2509 - acc: 0.9141 - val_loss: 0.2547 - val_acc: 0.9159
Epoch 186/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2524 - acc: 0.9134Epoch 00186: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2528 - acc: 0.9132 - val_loss: 0.2638 - val_acc: 0.9125
Epoch 187/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2484 - acc: 0.9145Epoch 00187: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2477 - acc: 0.9150 - val_loss: 0.2576 - val_acc: 0.9170
Epoch 188/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2487 - acc: 0.9174Epoch 00188: val_loss did not improve
17885/17885 [==============================] - 2s 93us/step - loss: 0.2487 - acc: 0.9169 - val_loss: 0.2551 - val_acc: 0.9153
Epoch 189/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2477 - acc: 0.9155Epoch 00189: val_loss did not improve
17885/17885 [==============================] - 2s 93us/step - loss: 0.2468 - acc: 0.9155 - val_loss: 0.2534 - val_acc: 0.9181
Epoch 190/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2425 - acc: 0.9181Epoch 00190: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2423 - acc: 0.9181 - val_loss: 0.2553 - val_acc: 0.9153
Epoch 191/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2475 - acc: 0.9170Epoch 00191: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2467 - acc: 0.9170 - val_loss: 0.2519 - val_acc: 0.9175
Epoch 192/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2409 - acc: 0.9177Epoch 00192: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2417 - acc: 0.9178 - val_loss: 0.2563 - val_acc: 0.9153
Epoch 193/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2439 - acc: 0.9198Epoch 00193: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2434 - acc: 0.9200 - val_loss: 0.2590 - val_acc: 0.9153
Epoch 194/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2422 - acc: 0.9163Epoch 00194: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2427 - acc: 0.9162 - val_loss: 0.2537 - val_acc: 0.9181
Epoch 195/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2420 - acc: 0.9169Epoch 00195: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2413 - acc: 0.9175 - val_loss: 0.2557 - val_acc: 0.9142
Epoch 196/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2345 - acc: 0.9209Epoch 00196: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2366 - acc: 0.9200 - val_loss: 0.2614 - val_acc: 0.9136
Epoch 197/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2429 - acc: 0.9183Epoch 00197: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2430 - acc: 0.9183 - val_loss: 0.2520 - val_acc: 0.9142
Epoch 198/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2337 - acc: 0.9204Epoch 00198: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2335 - acc: 0.9203 - val_loss: 0.2577 - val_acc: 0.9159
Epoch 199/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2364 - acc: 0.9177Epoch 00199: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2348 - acc: 0.9189 - val_loss: 0.2528 - val_acc: 0.9159
Epoch 200/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2408 - acc: 0.9180Epoch 00200: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2407 - acc: 0.9177 - val_loss: 0.2528 - val_acc: 0.9153
Epoch 201/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2399 - acc: 0.9177Epoch 00201: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2388 - acc: 0.9179 - val_loss: 0.2591 - val_acc: 0.9164
Epoch 202/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2401 - acc: 0.9173Epoch 00202: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2380 - acc: 0.9180 - val_loss: 0.2564 - val_acc: 0.9159
Epoch 203/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2324 - acc: 0.9207Epoch 00203: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2312 - acc: 0.9210 - val_loss: 0.2508 - val_acc: 0.9142
Epoch 204/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2313 - acc: 0.9199Epoch 00204: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2315 - acc: 0.9202 - val_loss: 0.2552 - val_acc: 0.9175
Epoch 205/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2384 - acc: 0.9200Epoch 00205: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2379 - acc: 0.9198 - val_loss: 0.2523 - val_acc: 0.9170
Epoch 206/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2354 - acc: 0.9200Epoch 00206: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2347 - acc: 0.9200 - val_loss: 0.2573 - val_acc: 0.9159
Epoch 207/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2327 - acc: 0.9208Epoch 00207: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2328 - acc: 0.9209 - val_loss: 0.2593 - val_acc: 0.9131
Epoch 208/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2303 - acc: 0.9204Epoch 00208: val_loss improved from 0.25077 to 0.25021, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.2312 - acc: 0.9204 - val_loss: 0.2502 - val_acc: 0.9164
Epoch 209/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2337 - acc: 0.9209Epoch 00209: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2347 - acc: 0.9205 - val_loss: 0.2521 - val_acc: 0.9148
Epoch 210/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2315 - acc: 0.9204Epoch 00210: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2311 - acc: 0.9207 - val_loss: 0.2507 - val_acc: 0.9181
Epoch 211/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2245 - acc: 0.9190Epoch 00211: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2240 - acc: 0.9193 - val_loss: 0.2605 - val_acc: 0.9148
Epoch 212/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2296 - acc: 0.9215Epoch 00212: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2303 - acc: 0.9216 - val_loss: 0.2537 - val_acc: 0.9142
Epoch 213/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2243 - acc: 0.9227Epoch 00213: val_loss improved from 0.25021 to 0.24901, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.2239 - acc: 0.9227 - val_loss: 0.2490 - val_acc: 0.9170
Epoch 214/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2241 - acc: 0.9219Epoch 00214: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2246 - acc: 0.9217 - val_loss: 0.2502 - val_acc: 0.9209
Epoch 215/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2236 - acc: 0.9227Epoch 00215: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2245 - acc: 0.9228 - val_loss: 0.2542 - val_acc: 0.9175
Epoch 216/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2289 - acc: 0.9231Epoch 00216: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2301 - acc: 0.9228 - val_loss: 0.2515 - val_acc: 0.9159
Epoch 217/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2276 - acc: 0.9231Epoch 00217: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2264 - acc: 0.9235 - val_loss: 0.2502 - val_acc: 0.9159
Epoch 218/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2339 - acc: 0.9222Epoch 00218: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2333 - acc: 0.9220 - val_loss: 0.2536 - val_acc: 0.9170
Epoch 219/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2237 - acc: 0.9238Epoch 00219: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2249 - acc: 0.9235 - val_loss: 0.2553 - val_acc: 0.9170
Epoch 220/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2255 - acc: 0.9218Epoch 00220: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2244 - acc: 0.9220 - val_loss: 0.2506 - val_acc: 0.9187
Epoch 221/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2214 - acc: 0.9209Epoch 00221: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2221 - acc: 0.9211 - val_loss: 0.2518 - val_acc: 0.9203
Epoch 222/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2243 - acc: 0.9234Epoch 00222: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2241 - acc: 0.9237 - val_loss: 0.2513 - val_acc: 0.9164
Epoch 223/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2231 - acc: 0.9217Epoch 00223: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2252 - acc: 0.9207 - val_loss: 0.2499 - val_acc: 0.9198
Epoch 224/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2123 - acc: 0.9264Epoch 00224: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2118 - acc: 0.9265 - val_loss: 0.2564 - val_acc: 0.9148
Epoch 225/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2167 - acc: 0.9253Epoch 00225: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2162 - acc: 0.9255 - val_loss: 0.2506 - val_acc: 0.9164
Epoch 226/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2218 - acc: 0.9239Epoch 00226: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2232 - acc: 0.9235 - val_loss: 0.2502 - val_acc: 0.9198
Epoch 227/250
17664/17885 [============================&gt;.] - ETA: 0s - loss: 0.2169 - acc: 0.9285Epoch 00227: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2167 - acc: 0.9287 - val_loss: 0.2496 - val_acc: 0.9214
Epoch 228/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2146 - acc: 0.9254Epoch 00228: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2134 - acc: 0.9263 - val_loss: 0.2506 - val_acc: 0.9159
Epoch 229/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2208 - acc: 0.9234Epoch 00229: val_loss improved from 0.24901 to 0.24810, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.2195 - acc: 0.9241 - val_loss: 0.2481 - val_acc: 0.9187
Epoch 230/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2217 - acc: 0.9261Epoch 00230: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2207 - acc: 0.9263 - val_loss: 0.2563 - val_acc: 0.9175
Epoch 231/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2139 - acc: 0.9264Epoch 00231: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2134 - acc: 0.9264 - val_loss: 0.2525 - val_acc: 0.9198
Epoch 232/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2159 - acc: 0.9275Epoch 00232: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2160 - acc: 0.9272 - val_loss: 0.2488 - val_acc: 0.9164
Epoch 233/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2155 - acc: 0.9264Epoch 00233: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2160 - acc: 0.9266 - val_loss: 0.2540 - val_acc: 0.9175
Epoch 234/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2229 - acc: 0.9239Epoch 00234: val_loss improved from 0.24810 to 0.24491, saving model to weights1.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.2220 - acc: 0.9243 - val_loss: 0.2449 - val_acc: 0.9237
Epoch 235/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2175 - acc: 0.9267Epoch 00235: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2185 - acc: 0.9259 - val_loss: 0.2576 - val_acc: 0.9181
Epoch 236/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2219 - acc: 0.9247Epoch 00236: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2221 - acc: 0.9242 - val_loss: 0.2530 - val_acc: 0.9192
Epoch 237/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2130 - acc: 0.9274Epoch 00237: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2121 - acc: 0.9277 - val_loss: 0.2560 - val_acc: 0.9164
Epoch 238/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2086 - acc: 0.9272Epoch 00238: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2071 - acc: 0.9276 - val_loss: 0.2551 - val_acc: 0.9198
Epoch 239/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2171 - acc: 0.9267Epoch 00239: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2159 - acc: 0.9269 - val_loss: 0.2527 - val_acc: 0.9170
Epoch 240/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2150 - acc: 0.9250Epoch 00240: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2150 - acc: 0.9250 - val_loss: 0.2511 - val_acc: 0.9192
Epoch 241/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2141 - acc: 0.9266Epoch 00241: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2153 - acc: 0.9266 - val_loss: 0.2500 - val_acc: 0.9203
Epoch 242/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2180 - acc: 0.9234Epoch 00242: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2183 - acc: 0.9235 - val_loss: 0.2480 - val_acc: 0.9198
Epoch 243/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2178 - acc: 0.9269Epoch 00243: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2166 - acc: 0.9273 - val_loss: 0.2525 - val_acc: 0.9192
Epoch 244/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2093 - acc: 0.9278Epoch 00244: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2088 - acc: 0.9279 - val_loss: 0.2487 - val_acc: 0.9192
Epoch 245/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2029 - acc: 0.9297Epoch 00245: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2033 - acc: 0.9292 - val_loss: 0.2516 - val_acc: 0.9175
Epoch 246/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2070 - acc: 0.9285Epoch 00246: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2061 - acc: 0.9288 - val_loss: 0.2509 - val_acc: 0.9198
Epoch 247/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2075 - acc: 0.9288Epoch 00247: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2063 - acc: 0.9297 - val_loss: 0.2572 - val_acc: 0.9175
Epoch 248/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2118 - acc: 0.9258Epoch 00248: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2124 - acc: 0.9260 - val_loss: 0.2526 - val_acc: 0.9148
Epoch 249/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2021 - acc: 0.9308Epoch 00249: val_loss did not improve
17885/17885 [==============================] - 2s 93us/step - loss: 0.2016 - acc: 0.9309 - val_loss: 0.2502 - val_acc: 0.9214
Epoch 250/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2120 - acc: 0.9293Epoch 00250: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2102 - acc: 0.9298 - val_loss: 0.2541 - val_acc: 0.9192
2


 trianing 



Train on 17885 samples, validate on 1795 samples
Epoch 1/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 9.2069 - acc: 0.0984Epoch 00001: val_loss improved from inf to 2.47064, saving model to weights2.hdf5
17885/17885 [==============================] - 3s 140us/step - loss: 8.9345 - acc: 0.0986 - val_loss: 2.4706 - val_acc: 0.1042
Epoch 2/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4929 - acc: 0.1024Epoch 00002: val_loss improved from 2.47064 to 2.44080, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4909 - acc: 0.1034 - val_loss: 2.4408 - val_acc: 0.1036
Epoch 3/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4474 - acc: 0.1049Epoch 00003: val_loss improved from 2.44080 to 2.43080, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4474 - acc: 0.1048 - val_loss: 2.4308 - val_acc: 0.1036
Epoch 4/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4382 - acc: 0.1044Epoch 00004: val_loss improved from 2.43080 to 2.42973, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4377 - acc: 0.1044 - val_loss: 2.4297 - val_acc: 0.1042
Epoch 5/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4320 - acc: 0.1059Epoch 00005: val_loss improved from 2.42973 to 2.42886, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 2.4325 - acc: 0.1058 - val_loss: 2.4289 - val_acc: 0.1081
Epoch 6/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4288 - acc: 0.1074Epoch 00006: val_loss improved from 2.42886 to 2.42816, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 2.4286 - acc: 0.1072 - val_loss: 2.4282 - val_acc: 0.1081
Epoch 7/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4300 - acc: 0.1083Epoch 00007: val_loss improved from 2.42816 to 2.42757, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4297 - acc: 0.1091 - val_loss: 2.4276 - val_acc: 0.1081
Epoch 8/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4286 - acc: 0.1095Epoch 00008: val_loss improved from 2.42757 to 2.42707, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 2.4287 - acc: 0.1090 - val_loss: 2.4271 - val_acc: 0.1081
Epoch 9/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4266 - acc: 0.1086Epoch 00009: val_loss improved from 2.42707 to 2.42663, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.4270 - acc: 0.1084 - val_loss: 2.4266 - val_acc: 0.1081
Epoch 10/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4259 - acc: 0.1091Epoch 00010: val_loss improved from 2.42663 to 2.42631, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.4264 - acc: 0.1087 - val_loss: 2.4263 - val_acc: 0.1081
Epoch 11/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4267 - acc: 0.1083Epoch 00011: val_loss improved from 2.42631 to 2.42605, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.4255 - acc: 0.1093 - val_loss: 2.4260 - val_acc: 0.1081
Epoch 12/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4247 - acc: 0.1091Epoch 00012: val_loss improved from 2.42605 to 2.42579, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 91us/step - loss: 2.4252 - acc: 0.1089 - val_loss: 2.4258 - val_acc: 0.1081
Epoch 13/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4238 - acc: 0.1095Epoch 00013: val_loss improved from 2.42579 to 2.42557, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.4238 - acc: 0.1091 - val_loss: 2.4256 - val_acc: 0.1081
Epoch 14/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4245 - acc: 0.1092Epoch 00014: val_loss improved from 2.42557 to 2.42540, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4248 - acc: 0.1084 - val_loss: 2.4254 - val_acc: 0.1081
Epoch 15/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4246 - acc: 0.1086Epoch 00015: val_loss improved from 2.42540 to 2.42524, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4248 - acc: 0.1085 - val_loss: 2.4252 - val_acc: 0.1081
Epoch 16/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4245 - acc: 0.1084Epoch 00016: val_loss improved from 2.42524 to 2.42508, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4243 - acc: 0.1086 - val_loss: 2.4251 - val_acc: 0.1081
Epoch 17/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4244 - acc: 0.1077Epoch 00017: val_loss improved from 2.42508 to 2.42497, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4238 - acc: 0.1088 - val_loss: 2.4250 - val_acc: 0.1081
Epoch 18/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4234 - acc: 0.1092Epoch 00018: val_loss improved from 2.42497 to 2.42487, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 2.4236 - acc: 0.1088 - val_loss: 2.4249 - val_acc: 0.1081
Epoch 19/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4236 - acc: 0.1084Epoch 00019: val_loss improved from 2.42487 to 2.42477, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 95us/step - loss: 2.4238 - acc: 0.1088 - val_loss: 2.4248 - val_acc: 0.1081
Epoch 20/250
17664/17885 [============================&gt;.] - ETA: 0s - loss: 2.4225 - acc: 0.1094Epoch 00020: val_loss improved from 2.42477 to 2.42466, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 96us/step - loss: 2.4230 - acc: 0.1090 - val_loss: 2.4247 - val_acc: 0.1081
Epoch 21/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4230 - acc: 0.1077Epoch 00021: val_loss improved from 2.42466 to 2.42462, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 2.4235 - acc: 0.1079 - val_loss: 2.4246 - val_acc: 0.1081
Epoch 22/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4228 - acc: 0.1078Epoch 00022: val_loss improved from 2.42462 to 2.42454, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4234 - acc: 0.1082 - val_loss: 2.4245 - val_acc: 0.1081
Epoch 23/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4232 - acc: 0.1080Epoch 00023: val_loss improved from 2.42454 to 2.42442, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4229 - acc: 0.1083 - val_loss: 2.4244 - val_acc: 0.1081
Epoch 24/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4233 - acc: 0.1093Epoch 00024: val_loss improved from 2.42442 to 2.42435, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.4230 - acc: 0.1082 - val_loss: 2.4243 - val_acc: 0.1081
Epoch 25/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4233 - acc: 0.1080Epoch 00025: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 2.4226 - acc: 0.1080 - val_loss: 2.4244 - val_acc: 0.1081
Epoch 26/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4236 - acc: 0.108 - ETA: 0s - loss: 2.4239 - acc: 0.1071Epoch 00026: val_loss improved from 2.42435 to 2.42434, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4232 - acc: 0.1068 - val_loss: 2.4243 - val_acc: 0.1081
Epoch 27/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4228 - acc: 0.1089Epoch 00027: val_loss improved from 2.42434 to 2.42424, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4225 - acc: 0.1088 - val_loss: 2.4242 - val_acc: 0.1081
Epoch 28/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4223 - acc: 0.1076Epoch 00028: val_loss improved from 2.42424 to 2.42423, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.4218 - acc: 0.1072 - val_loss: 2.4242 - val_acc: 0.1081
Epoch 29/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4225 - acc: 0.1084Epoch 00029: val_loss improved from 2.42423 to 2.42391, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.4231 - acc: 0.1080 - val_loss: 2.4239 - val_acc: 0.1086
Epoch 30/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4234 - acc: 0.1088Epoch 00030: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 2.4224 - acc: 0.1094 - val_loss: 2.4239 - val_acc: 0.1086
Epoch 31/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4228 - acc: 0.1072Epoch 00031: val_loss improved from 2.42391 to 2.42391, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4224 - acc: 0.1080 - val_loss: 2.4239 - val_acc: 0.1086
Epoch 32/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4232 - acc: 0.1099Epoch 00032: val_loss improved from 2.42391 to 2.42383, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4225 - acc: 0.1092 - val_loss: 2.4238 - val_acc: 0.1086
Epoch 33/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4223 - acc: 0.1095Epoch 00033: val_loss did not improve
17885/17885 [==============================] - 2s 90us/step - loss: 2.4224 - acc: 0.1093 - val_loss: 2.4239 - val_acc: 0.1086
Epoch 34/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4222 - acc: 0.1080Epoch 00034: val_loss improved from 2.42383 to 2.42373, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 91us/step - loss: 2.4224 - acc: 0.1076 - val_loss: 2.4237 - val_acc: 0.1086
Epoch 35/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4227 - acc: 0.1069- ETA: 0s - loss: 2.4219 - acc: 0.10Epoch 00035: val_loss improved from 2.42373 to 2.42370, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 91us/step - loss: 2.4225 - acc: 0.1075 - val_loss: 2.4237 - val_acc: 0.1086
Epoch 36/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4212 - acc: 0.1097- ETA: 1s - loss: 2.4250Epoch 00036: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 2.4218 - acc: 0.1096 - val_loss: 2.4237 - val_acc: 0.1086
Epoch 37/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4221 - acc: 0.1082Epoch 00037: val_loss improved from 2.42370 to 2.42361, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.4220 - acc: 0.1091 - val_loss: 2.4236 - val_acc: 0.1086
Epoch 38/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4218 - acc: 0.1087Epoch 00038: val_loss improved from 2.42361 to 2.41863, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.4217 - acc: 0.1091 - val_loss: 2.4186 - val_acc: 0.1109
Epoch 39/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4224 - acc: 0.1125Epoch 00039: val_loss improved from 2.41863 to 2.41104, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.4214 - acc: 0.1124 - val_loss: 2.4110 - val_acc: 0.1153
Epoch 40/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.4133 - acc: 0.1170Epoch 00040: val_loss improved from 2.41104 to 2.39100, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.4118 - acc: 0.1167 - val_loss: 2.3910 - val_acc: 0.1237
Epoch 41/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.3969 - acc: 0.1258Epoch 00041: val_loss improved from 2.39100 to 2.36114, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.3965 - acc: 0.1263 - val_loss: 2.3611 - val_acc: 0.1415
Epoch 42/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.3779 - acc: 0.1315Epoch 00042: val_loss improved from 2.36114 to 2.33276, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.3774 - acc: 0.1313 - val_loss: 2.3328 - val_acc: 0.1426
Epoch 43/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.3570 - acc: 0.1381Epoch 00043: val_loss improved from 2.33276 to 2.32892, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.3573 - acc: 0.1384 - val_loss: 2.3289 - val_acc: 0.1421
Epoch 44/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.3482 - acc: 0.1397Epoch 00044: val_loss improved from 2.32892 to 2.32388, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.3466 - acc: 0.1403 - val_loss: 2.3239 - val_acc: 0.1426
Epoch 45/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.3390 - acc: 0.1410Epoch 00045: val_loss improved from 2.32388 to 2.32149, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.3390 - acc: 0.1410 - val_loss: 2.3215 - val_acc: 0.1432
Epoch 46/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.3342 - acc: 0.1405Epoch 00046: val_loss improved from 2.32149 to 2.31855, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 2.3354 - acc: 0.1411 - val_loss: 2.3185 - val_acc: 0.1426
Epoch 47/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.3324 - acc: 0.1416Epoch 00047: val_loss improved from 2.31855 to 2.31717, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 2.3319 - acc: 0.1414 - val_loss: 2.3172 - val_acc: 0.1432
Epoch 48/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.3301 - acc: 0.1428Epoch 00048: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 2.3297 - acc: 0.1425 - val_loss: 2.3202 - val_acc: 0.1443
Epoch 49/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.3214 - acc: 0.1546Epoch 00049: val_loss improved from 2.31717 to 2.28856, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 2.3203 - acc: 0.1552 - val_loss: 2.2886 - val_acc: 0.1838
Epoch 50/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.2854 - acc: 0.1737Epoch 00050: val_loss improved from 2.28856 to 2.21715, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 2.2851 - acc: 0.1733 - val_loss: 2.2171 - val_acc: 0.1939
Epoch 51/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.2539 - acc: 0.1813Epoch 00051: val_loss improved from 2.21715 to 2.18880, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 2.2525 - acc: 0.1808 - val_loss: 2.1888 - val_acc: 0.2061
Epoch 52/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.1981 - acc: 0.2003Epoch 00052: val_loss improved from 2.18880 to 2.10502, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 2.1975 - acc: 0.2003 - val_loss: 2.1050 - val_acc: 0.2501
Epoch 53/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.1235 - acc: 0.2255Epoch 00053: val_loss improved from 2.10502 to 1.97618, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 2.1226 - acc: 0.2259 - val_loss: 1.9762 - val_acc: 0.2942
Epoch 54/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 2.0400 - acc: 0.2630Epoch 00054: val_loss improved from 1.97618 to 1.87849, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 2.0400 - acc: 0.2635 - val_loss: 1.8785 - val_acc: 0.3560
Epoch 55/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.9067 - acc: 0.3081- ETA: 0s - loss: 1.9359 - accEpoch 00055: val_loss improved from 1.87849 to 1.62886, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 1.9023 - acc: 0.3084 - val_loss: 1.6289 - val_acc: 0.4368
Epoch 56/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.7664 - acc: 0.3635Epoch 00056: val_loss improved from 1.62886 to 1.46716, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 1.7649 - acc: 0.3639 - val_loss: 1.4672 - val_acc: 0.5003
Epoch 57/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.6391 - acc: 0.4119Epoch 00057: val_loss improved from 1.46716 to 1.40964, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 1.6392 - acc: 0.4129 - val_loss: 1.4096 - val_acc: 0.5103
Epoch 58/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.5440 - acc: 0.4482Epoch 00058: val_loss improved from 1.40964 to 1.23503, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 1.5415 - acc: 0.4491 - val_loss: 1.2350 - val_acc: 0.5978
Epoch 59/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.4508 - acc: 0.4857Epoch 00059: val_loss improved from 1.23503 to 1.11034, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 1.4500 - acc: 0.4866 - val_loss: 1.1103 - val_acc: 0.6351
Epoch 60/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.3564 - acc: 0.5203Epoch 00060: val_loss improved from 1.11034 to 1.03889, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 1.3561 - acc: 0.5199 - val_loss: 1.0389 - val_acc: 0.6802
Epoch 61/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.2812 - acc: 0.5485Epoch 00061: val_loss improved from 1.03889 to 0.94663, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 1.2769 - acc: 0.5497 - val_loss: 0.9466 - val_acc: 0.6969
Epoch 62/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.1997 - acc: 0.5809Epoch 00062: val_loss improved from 0.94663 to 0.86632, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 1.1979 - acc: 0.5814 - val_loss: 0.8663 - val_acc: 0.7304
Epoch 63/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.1524 - acc: 0.5960Epoch 00063: val_loss improved from 0.86632 to 0.82564, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 1.1520 - acc: 0.5959 - val_loss: 0.8256 - val_acc: 0.7359
Epoch 64/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.1152 - acc: 0.6120Epoch 00064: val_loss improved from 0.82564 to 0.77586, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 1.1131 - acc: 0.6135 - val_loss: 0.7759 - val_acc: 0.7549
Epoch 65/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.0586 - acc: 0.6319Epoch 00065: val_loss improved from 0.77586 to 0.77149, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 1.0578 - acc: 0.6323 - val_loss: 0.7715 - val_acc: 0.7643
Epoch 66/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 1.0144 - acc: 0.6466Epoch 00066: val_loss improved from 0.77149 to 0.71032, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 1.0124 - acc: 0.6474 - val_loss: 0.7103 - val_acc: 0.7788
Epoch 67/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.9858 - acc: 0.6604Epoch 00067: val_loss improved from 0.71032 to 0.70181, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.9863 - acc: 0.6599 - val_loss: 0.7018 - val_acc: 0.7850
Epoch 68/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.9517 - acc: 0.6751Epoch 00068: val_loss improved from 0.70181 to 0.66547, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.9512 - acc: 0.6746 - val_loss: 0.6655 - val_acc: 0.7928
Epoch 69/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.9132 - acc: 0.6862Epoch 00069: val_loss improved from 0.66547 to 0.63798, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.9136 - acc: 0.6864 - val_loss: 0.6380 - val_acc: 0.8000
Epoch 70/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.8773 - acc: 0.6958Epoch 00070: val_loss improved from 0.63798 to 0.60606, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.8770 - acc: 0.6957 - val_loss: 0.6061 - val_acc: 0.8078
Epoch 71/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.8572 - acc: 0.7041Epoch 00071: val_loss improved from 0.60606 to 0.60221, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.8553 - acc: 0.7044 - val_loss: 0.6022 - val_acc: 0.8061
Epoch 72/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.8256 - acc: 0.7227Epoch 00072: val_loss improved from 0.60221 to 0.57517, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.8227 - acc: 0.7226 - val_loss: 0.5752 - val_acc: 0.8111
Epoch 73/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.8150 - acc: 0.7218Epoch 00073: val_loss improved from 0.57517 to 0.56177, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 95us/step - loss: 0.8152 - acc: 0.7214 - val_loss: 0.5618 - val_acc: 0.8106
Epoch 74/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.7842 - acc: 0.7362Epoch 00074: val_loss improved from 0.56177 to 0.54811, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.7824 - acc: 0.7363 - val_loss: 0.5481 - val_acc: 0.8173
Epoch 75/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.7707 - acc: 0.7380Epoch 00075: val_loss improved from 0.54811 to 0.53829, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.7729 - acc: 0.7367 - val_loss: 0.5383 - val_acc: 0.8306
Epoch 76/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.7564 - acc: 0.7445Epoch 00076: val_loss improved from 0.53829 to 0.52289, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.7551 - acc: 0.7451 - val_loss: 0.5229 - val_acc: 0.8228
Epoch 77/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.7235 - acc: 0.7512Epoch 00077: val_loss improved from 0.52289 to 0.50417, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.7281 - acc: 0.7500 - val_loss: 0.5042 - val_acc: 0.8429
Epoch 78/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.7208 - acc: 0.7573Epoch 00078: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.7208 - acc: 0.7573 - val_loss: 0.5070 - val_acc: 0.8267
Epoch 79/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6995 - acc: 0.7643Epoch 00079: val_loss improved from 0.50417 to 0.49176, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.6986 - acc: 0.7642 - val_loss: 0.4918 - val_acc: 0.8334
Epoch 80/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6917 - acc: 0.7650Epoch 00080: val_loss improved from 0.49176 to 0.47978, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.6935 - acc: 0.7646 - val_loss: 0.4798 - val_acc: 0.8429
Epoch 81/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6799 - acc: 0.7717Epoch 00081: val_loss improved from 0.47978 to 0.47737, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.6783 - acc: 0.7728 - val_loss: 0.4774 - val_acc: 0.8384
Epoch 82/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6664 - acc: 0.7741Epoch 00082: val_loss improved from 0.47737 to 0.46961, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.6648 - acc: 0.7741 - val_loss: 0.4696 - val_acc: 0.8418
Epoch 83/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6518 - acc: 0.7788Epoch 00083: val_loss improved from 0.46961 to 0.45983, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.6517 - acc: 0.7789 - val_loss: 0.4598 - val_acc: 0.8485
Epoch 84/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6444 - acc: 0.7811Epoch 00084: val_loss improved from 0.45983 to 0.44810, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.6437 - acc: 0.7814 - val_loss: 0.4481 - val_acc: 0.8451
Epoch 85/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6342 - acc: 0.7886Epoch 00085: val_loss improved from 0.44810 to 0.44393, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.6331 - acc: 0.7893 - val_loss: 0.4439 - val_acc: 0.8507
Epoch 86/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6182 - acc: 0.7913Epoch 00086: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.6167 - acc: 0.7911 - val_loss: 0.4471 - val_acc: 0.8451
Epoch 87/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6048 - acc: 0.7973Epoch 00087: val_loss improved from 0.44393 to 0.43078, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.6035 - acc: 0.7975 - val_loss: 0.4308 - val_acc: 0.8591
Epoch 88/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.6053 - acc: 0.7963Epoch 00088: val_loss improved from 0.43078 to 0.42071, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.6051 - acc: 0.7961 - val_loss: 0.4207 - val_acc: 0.8568
Epoch 89/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5982 - acc: 0.8010Epoch 00089: val_loss improved from 0.42071 to 0.41677, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.6015 - acc: 0.7997 - val_loss: 0.4168 - val_acc: 0.8596
Epoch 90/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5911 - acc: 0.8024- ETA: 0s - loss: 0.5884 - acc: Epoch 00090: val_loss did not improve
17885/17885 [==============================] - 2s 90us/step - loss: 0.5891 - acc: 0.8029 - val_loss: 0.4184 - val_acc: 0.8513
Epoch 91/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5791 - acc: 0.8054Epoch 00091: val_loss improved from 0.41677 to 0.41526, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.5785 - acc: 0.8054 - val_loss: 0.4153 - val_acc: 0.8568
Epoch 92/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5705 - acc: 0.8117Epoch 00092: val_loss improved from 0.41526 to 0.39984, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.5685 - acc: 0.8123 - val_loss: 0.3998 - val_acc: 0.8607
Epoch 93/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5566 - acc: 0.8135Epoch 00093: val_loss improved from 0.39984 to 0.39694, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.5565 - acc: 0.8140 - val_loss: 0.3969 - val_acc: 0.8669
Epoch 94/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5520 - acc: 0.8129Epoch 00094: val_loss improved from 0.39694 to 0.39559, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.5505 - acc: 0.8135 - val_loss: 0.3956 - val_acc: 0.8630
Epoch 95/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5480 - acc: 0.8165Epoch 00095: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.5486 - acc: 0.8161 - val_loss: 0.3961 - val_acc: 0.8618
Epoch 96/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5428 - acc: 0.8172Epoch 00096: val_loss improved from 0.39559 to 0.39215, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.5410 - acc: 0.8185 - val_loss: 0.3922 - val_acc: 0.8624
Epoch 97/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5358 - acc: 0.8219Epoch 00097: val_loss improved from 0.39215 to 0.38689, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.5368 - acc: 0.8217 - val_loss: 0.3869 - val_acc: 0.8691
Epoch 98/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5234 - acc: 0.8224Epoch 00098: val_loss improved from 0.38689 to 0.38201, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.5246 - acc: 0.8218 - val_loss: 0.3820 - val_acc: 0.8674
Epoch 99/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5192 - acc: 0.8254Epoch 00099: val_loss improved from 0.38201 to 0.37402, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.5183 - acc: 0.8256 - val_loss: 0.3740 - val_acc: 0.8652
Epoch 100/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5187 - acc: 0.8263Epoch 00100: val_loss improved from 0.37402 to 0.37112, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.5197 - acc: 0.8258 - val_loss: 0.3711 - val_acc: 0.8680
Epoch 101/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5072 - acc: 0.8304Epoch 00101: val_loss improved from 0.37112 to 0.37005, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.5041 - acc: 0.8311 - val_loss: 0.3700 - val_acc: 0.8769
Epoch 102/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.5005 - acc: 0.8316Epoch 00102: val_loss improved from 0.37005 to 0.36682, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.5014 - acc: 0.8315 - val_loss: 0.3668 - val_acc: 0.8708
Epoch 103/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4905 - acc: 0.8355Epoch 00103: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.4915 - acc: 0.8344 - val_loss: 0.3702 - val_acc: 0.8719
Epoch 104/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4946 - acc: 0.8318Epoch 00104: val_loss improved from 0.36682 to 0.36337, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4939 - acc: 0.8320 - val_loss: 0.3634 - val_acc: 0.8780
Epoch 105/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4893 - acc: 0.8337Epoch 00105: val_loss improved from 0.36337 to 0.35977, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.4885 - acc: 0.8343 - val_loss: 0.3598 - val_acc: 0.8774
Epoch 106/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4792 - acc: 0.8396Epoch 00106: val_loss improved from 0.35977 to 0.35761, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.4780 - acc: 0.8405 - val_loss: 0.3576 - val_acc: 0.8808
Epoch 107/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4771 - acc: 0.8390Epoch 00107: val_loss improved from 0.35761 to 0.34672, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4774 - acc: 0.8392 - val_loss: 0.3467 - val_acc: 0.8808
Epoch 108/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4737 - acc: 0.8418Epoch 00108: val_loss improved from 0.34672 to 0.34326, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4726 - acc: 0.8419 - val_loss: 0.3433 - val_acc: 0.8819
Epoch 109/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4609 - acc: 0.8415Epoch 00109: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.4604 - acc: 0.8422 - val_loss: 0.3555 - val_acc: 0.8791
Epoch 110/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4630 - acc: 0.8442Epoch 00110: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.4619 - acc: 0.8446 - val_loss: 0.3475 - val_acc: 0.8763
Epoch 111/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4637 - acc: 0.8436Epoch 00111: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.4624 - acc: 0.8434 - val_loss: 0.3472 - val_acc: 0.8808
Epoch 112/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4475 - acc: 0.8496Epoch 00112: val_loss improved from 0.34326 to 0.33662, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4487 - acc: 0.8491 - val_loss: 0.3366 - val_acc: 0.8864
Epoch 113/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4575 - acc: 0.8450Epoch 00113: val_loss improved from 0.33662 to 0.33289, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4579 - acc: 0.8451 - val_loss: 0.3329 - val_acc: 0.8852
Epoch 114/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4413 - acc: 0.8536Epoch 00114: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.4413 - acc: 0.8530 - val_loss: 0.3356 - val_acc: 0.8836
Epoch 115/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4394 - acc: 0.8507Epoch 00115: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.4397 - acc: 0.8510 - val_loss: 0.3351 - val_acc: 0.8852
Epoch 116/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4422 - acc: 0.8524Epoch 00116: val_loss improved from 0.33289 to 0.33282, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4442 - acc: 0.8526 - val_loss: 0.3328 - val_acc: 0.8813
Epoch 117/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4395 - acc: 0.8535Epoch 00117: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.4380 - acc: 0.8541 - val_loss: 0.3343 - val_acc: 0.8841
Epoch 118/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4307 - acc: 0.8552Epoch 00118: val_loss improved from 0.33282 to 0.32957, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4325 - acc: 0.8543 - val_loss: 0.3296 - val_acc: 0.8852
Epoch 119/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4254 - acc: 0.8545Epoch 00119: val_loss improved from 0.32957 to 0.31882, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4259 - acc: 0.8544 - val_loss: 0.3188 - val_acc: 0.8914
Epoch 120/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4299 - acc: 0.8557Epoch 00120: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.4329 - acc: 0.8547 - val_loss: 0.3225 - val_acc: 0.8869
Epoch 121/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4242 - acc: 0.8560Epoch 00121: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.4254 - acc: 0.8552 - val_loss: 0.3210 - val_acc: 0.8930
Epoch 122/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4136 - acc: 0.8598Epoch 00122: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.4130 - acc: 0.8601 - val_loss: 0.3198 - val_acc: 0.8886
Epoch 123/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4088 - acc: 0.8621Epoch 00123: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.4094 - acc: 0.8613 - val_loss: 0.3260 - val_acc: 0.8875
Epoch 124/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4037 - acc: 0.8646Epoch 00124: val_loss improved from 0.31882 to 0.31695, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4067 - acc: 0.8636 - val_loss: 0.3170 - val_acc: 0.8886
Epoch 125/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4030 - acc: 0.8626Epoch 00125: val_loss improved from 0.31695 to 0.31539, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4051 - acc: 0.8620 - val_loss: 0.3154 - val_acc: 0.8919
Epoch 126/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4072 - acc: 0.8632Epoch 00126: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.4108 - acc: 0.8626 - val_loss: 0.3271 - val_acc: 0.8891
Epoch 127/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4079 - acc: 0.8641Epoch 00127: val_loss improved from 0.31539 to 0.31202, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.4072 - acc: 0.8639 - val_loss: 0.3120 - val_acc: 0.8969
Epoch 128/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3961 - acc: 0.8681Epoch 00128: val_loss improved from 0.31202 to 0.30665, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.3979 - acc: 0.8675 - val_loss: 0.3066 - val_acc: 0.8953
Epoch 129/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3966 - acc: 0.8668Epoch 00129: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3952 - acc: 0.8677 - val_loss: 0.3128 - val_acc: 0.8942
Epoch 130/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3948 - acc: 0.8671Epoch 00130: val_loss improved from 0.30665 to 0.30327, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.3943 - acc: 0.8672 - val_loss: 0.3033 - val_acc: 0.8942
Epoch 131/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.4018 - acc: 0.8663Epoch 00131: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3994 - acc: 0.8670 - val_loss: 0.3114 - val_acc: 0.8891
Epoch 132/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3841 - acc: 0.8707Epoch 00132: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3846 - acc: 0.8711 - val_loss: 0.3142 - val_acc: 0.8891
Epoch 133/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3908 - acc: 0.8677Epoch 00133: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3917 - acc: 0.8677 - val_loss: 0.3186 - val_acc: 0.8891
Epoch 134/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3911 - acc: 0.8683Epoch 00134: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3899 - acc: 0.8688 - val_loss: 0.3051 - val_acc: 0.8958
Epoch 135/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3820 - acc: 0.8724Epoch 00135: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3832 - acc: 0.8718 - val_loss: 0.3045 - val_acc: 0.8930
Epoch 136/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3833 - acc: 0.8724Epoch 00136: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3817 - acc: 0.8728 - val_loss: 0.3033 - val_acc: 0.8925
Epoch 137/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3714 - acc: 0.8745Epoch 00137: val_loss improved from 0.30327 to 0.30277, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3738 - acc: 0.8737 - val_loss: 0.3028 - val_acc: 0.8936
Epoch 138/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3741 - acc: 0.8717Epoch 00138: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3738 - acc: 0.8712 - val_loss: 0.3038 - val_acc: 0.8964
Epoch 139/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3799 - acc: 0.8680Epoch 00139: val_loss improved from 0.30277 to 0.29369, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.3806 - acc: 0.8680 - val_loss: 0.2937 - val_acc: 0.8969
Epoch 140/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3694 - acc: 0.8746Epoch 00140: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3690 - acc: 0.8751 - val_loss: 0.2944 - val_acc: 0.8997
Epoch 141/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3642 - acc: 0.8772Epoch 00141: val_loss improved from 0.29369 to 0.29298, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3648 - acc: 0.8766 - val_loss: 0.2930 - val_acc: 0.8936
Epoch 142/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3577 - acc: 0.8783Epoch 00142: val_loss improved from 0.29298 to 0.28844, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3587 - acc: 0.8781 - val_loss: 0.2884 - val_acc: 0.9036
Epoch 143/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3643 - acc: 0.8771Epoch 00143: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3632 - acc: 0.8772 - val_loss: 0.2933 - val_acc: 0.8953
Epoch 144/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3616 - acc: 0.8810Epoch 00144: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3632 - acc: 0.8803 - val_loss: 0.2890 - val_acc: 0.9003
Epoch 145/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3499 - acc: 0.8818Epoch 00145: val_loss improved from 0.28844 to 0.28589, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3496 - acc: 0.8817 - val_loss: 0.2859 - val_acc: 0.9008
Epoch 146/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3564 - acc: 0.8769Epoch 00146: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3566 - acc: 0.8771 - val_loss: 0.2883 - val_acc: 0.8997
Epoch 147/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3556 - acc: 0.8818Epoch 00147: val_loss improved from 0.28589 to 0.28526, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.3543 - acc: 0.8822 - val_loss: 0.2853 - val_acc: 0.8997
Epoch 148/250
17664/17885 [============================&gt;.] - ETA: 0s - loss: 0.3493 - acc: 0.8787Epoch 00148: val_loss improved from 0.28526 to 0.28448, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.3487 - acc: 0.8791 - val_loss: 0.2845 - val_acc: 0.9003
Epoch 149/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3561 - acc: 0.8801Epoch 00149: val_loss improved from 0.28448 to 0.28326, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3568 - acc: 0.8803 - val_loss: 0.2833 - val_acc: 0.9042
Epoch 150/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3477 - acc: 0.8823Epoch 00150: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3497 - acc: 0.8814 - val_loss: 0.2841 - val_acc: 0.9036
Epoch 151/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3536 - acc: 0.8802Epoch 00151: val_loss improved from 0.28326 to 0.27851, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3546 - acc: 0.8803 - val_loss: 0.2785 - val_acc: 0.9047
Epoch 152/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3406 - acc: 0.8870Epoch 00152: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3392 - acc: 0.8872 - val_loss: 0.2857 - val_acc: 0.9031
Epoch 153/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3356 - acc: 0.8870Epoch 00153: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3334 - acc: 0.8874 - val_loss: 0.2789 - val_acc: 0.9053
Epoch 154/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3373 - acc: 0.8859Epoch 00154: val_loss improved from 0.27851 to 0.27402, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.3381 - acc: 0.8857 - val_loss: 0.2740 - val_acc: 0.9053
Epoch 155/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3337 - acc: 0.8884Epoch 00155: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3348 - acc: 0.8879 - val_loss: 0.2782 - val_acc: 0.9042
Epoch 156/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3290 - acc: 0.8860Epoch 00156: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3309 - acc: 0.8859 - val_loss: 0.2800 - val_acc: 0.8992
Epoch 157/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3347 - acc: 0.8863Epoch 00157: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3343 - acc: 0.8866 - val_loss: 0.2746 - val_acc: 0.9036
Epoch 158/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3338 - acc: 0.8869Epoch 00158: val_loss improved from 0.27402 to 0.27392, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.3348 - acc: 0.8869 - val_loss: 0.2739 - val_acc: 0.9031
Epoch 159/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3257 - acc: 0.8896Epoch 00159: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3243 - acc: 0.8901 - val_loss: 0.2781 - val_acc: 0.9025
Epoch 160/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3274 - acc: 0.8877Epoch 00160: val_loss improved from 0.27392 to 0.27122, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3268 - acc: 0.8881 - val_loss: 0.2712 - val_acc: 0.9053
Epoch 161/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3247 - acc: 0.8867Epoch 00161: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3254 - acc: 0.8866 - val_loss: 0.2825 - val_acc: 0.8981
Epoch 162/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3220 - acc: 0.8920Epoch 00162: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3218 - acc: 0.8916 - val_loss: 0.2732 - val_acc: 0.9070
Epoch 163/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3183 - acc: 0.8931Epoch 00163: val_loss improved from 0.27122 to 0.26798, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3212 - acc: 0.8918 - val_loss: 0.2680 - val_acc: 0.9064
Epoch 164/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3258 - acc: 0.8913Epoch 00164: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3250 - acc: 0.8912 - val_loss: 0.2705 - val_acc: 0.9064
Epoch 165/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3156 - acc: 0.8943Epoch 00165: val_loss improved from 0.26798 to 0.26778, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.3165 - acc: 0.8939 - val_loss: 0.2678 - val_acc: 0.9053
Epoch 166/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3118 - acc: 0.8932Epoch 00166: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3120 - acc: 0.8930 - val_loss: 0.2786 - val_acc: 0.9025
Epoch 167/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3096 - acc: 0.8942Epoch 00167: val_loss improved from 0.26778 to 0.26493, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.3092 - acc: 0.8945 - val_loss: 0.2649 - val_acc: 0.9064
Epoch 168/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3135 - acc: 0.8931Epoch 00168: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3149 - acc: 0.8927 - val_loss: 0.2696 - val_acc: 0.9070
Epoch 169/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3208 - acc: 0.8917Epoch 00169: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3215 - acc: 0.8918 - val_loss: 0.2658 - val_acc: 0.9053
Epoch 170/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3133 - acc: 0.8918Epoch 00170: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3144 - acc: 0.8916 - val_loss: 0.2676 - val_acc: 0.9064
Epoch 171/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3138 - acc: 0.8947Epoch 00171: val_loss improved from 0.26493 to 0.25936, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3146 - acc: 0.8943 - val_loss: 0.2594 - val_acc: 0.9142
Epoch 172/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3070 - acc: 0.8959Epoch 00172: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3070 - acc: 0.8960 - val_loss: 0.2636 - val_acc: 0.9109
Epoch 173/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3118 - acc: 0.8960Epoch 00173: val_loss improved from 0.25936 to 0.25778, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.3119 - acc: 0.8963 - val_loss: 0.2578 - val_acc: 0.9103
Epoch 174/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3062 - acc: 0.8944Epoch 00174: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3063 - acc: 0.8949 - val_loss: 0.2668 - val_acc: 0.9075
Epoch 175/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3042 - acc: 0.8959Epoch 00175: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.3068 - acc: 0.8951 - val_loss: 0.2622 - val_acc: 0.9114
Epoch 176/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3068 - acc: 0.8967Epoch 00176: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3075 - acc: 0.8966 - val_loss: 0.2663 - val_acc: 0.9097
Epoch 177/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3026 - acc: 0.8976Epoch 00177: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3029 - acc: 0.8976 - val_loss: 0.2653 - val_acc: 0.9103
Epoch 178/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3047 - acc: 0.8993Epoch 00178: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3076 - acc: 0.8980 - val_loss: 0.2618 - val_acc: 0.9109
Epoch 179/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2959 - acc: 0.9025Epoch 00179: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2965 - acc: 0.9023 - val_loss: 0.2656 - val_acc: 0.9058
Epoch 180/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2985 - acc: 0.9001Epoch 00180: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2982 - acc: 0.9009 - val_loss: 0.2578 - val_acc: 0.9109
Epoch 181/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2974 - acc: 0.9000Epoch 00181: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2974 - acc: 0.9003 - val_loss: 0.2606 - val_acc: 0.9114
Epoch 182/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.3031 - acc: 0.8971Epoch 00182: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.3027 - acc: 0.8972 - val_loss: 0.2647 - val_acc: 0.9081
Epoch 183/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2955 - acc: 0.899 - ETA: 0s - loss: 0.2955 - acc: 0.8998Epoch 00183: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2952 - acc: 0.8997 - val_loss: 0.2620 - val_acc: 0.9081
Epoch 184/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2948 - acc: 0.8988Epoch 00184: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2945 - acc: 0.8990 - val_loss: 0.2587 - val_acc: 0.9109
Epoch 185/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2955 - acc: 0.9005Epoch 00185: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2955 - acc: 0.9008 - val_loss: 0.2613 - val_acc: 0.9153
Epoch 186/250
17664/17885 [============================&gt;.] - ETA: 0s - loss: 0.2827 - acc: 0.9048Epoch 00186: val_loss improved from 0.25778 to 0.25404, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.2825 - acc: 0.9047 - val_loss: 0.2540 - val_acc: 0.9081
Epoch 187/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2935 - acc: 0.8998Epoch 00187: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2938 - acc: 0.8999 - val_loss: 0.2552 - val_acc: 0.9081
Epoch 188/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2942 - acc: 0.8985Epoch 00188: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2942 - acc: 0.8984 - val_loss: 0.2555 - val_acc: 0.9097
Epoch 189/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2932 - acc: 0.9006Epoch 00189: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2931 - acc: 0.9004 - val_loss: 0.2554 - val_acc: 0.9131
Epoch 190/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2833 - acc: 0.9034Epoch 00190: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2832 - acc: 0.9033 - val_loss: 0.2587 - val_acc: 0.9114
Epoch 191/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2886 - acc: 0.9019Epoch 00191: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2881 - acc: 0.9016 - val_loss: 0.2609 - val_acc: 0.9064
Epoch 192/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2810 - acc: 0.9041Epoch 00192: val_loss improved from 0.25404 to 0.24994, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.2825 - acc: 0.9036 - val_loss: 0.2499 - val_acc: 0.9142
Epoch 193/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2765 - acc: 0.9065Epoch 00193: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2761 - acc: 0.9067 - val_loss: 0.2560 - val_acc: 0.9097
Epoch 194/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2704 - acc: 0.9046Epoch 00194: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2708 - acc: 0.9048 - val_loss: 0.2529 - val_acc: 0.9159
Epoch 195/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2847 - acc: 0.9039Epoch 00195: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2832 - acc: 0.9039 - val_loss: 0.2522 - val_acc: 0.9148
Epoch 196/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2752 - acc: 0.9037Epoch 00196: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2747 - acc: 0.9039 - val_loss: 0.2551 - val_acc: 0.9131
Epoch 197/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2779 - acc: 0.9059Epoch 00197: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2773 - acc: 0.9062 - val_loss: 0.2579 - val_acc: 0.9081
Epoch 198/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2708 - acc: 0.9099Epoch 00198: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2727 - acc: 0.9091 - val_loss: 0.2553 - val_acc: 0.9103
Epoch 199/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2753 - acc: 0.9074Epoch 00199: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2748 - acc: 0.9075 - val_loss: 0.2564 - val_acc: 0.9086
Epoch 200/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2689 - acc: 0.9075Epoch 00200: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2685 - acc: 0.9074 - val_loss: 0.2580 - val_acc: 0.9142
Epoch 201/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2725 - acc: 0.9062Epoch 00201: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2746 - acc: 0.9056 - val_loss: 0.2548 - val_acc: 0.9136
Epoch 202/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2786 - acc: 0.9074Epoch 00202: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2813 - acc: 0.9066 - val_loss: 0.2560 - val_acc: 0.9131
Epoch 203/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2660 - acc: 0.9089Epoch 00203: val_loss did not improve
17885/17885 [==============================] - 2s 90us/step - loss: 0.2646 - acc: 0.9094 - val_loss: 0.2576 - val_acc: 0.9081
Epoch 204/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2729 - acc: 0.9063Epoch 00204: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2727 - acc: 0.9064 - val_loss: 0.2519 - val_acc: 0.9159
Epoch 205/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2733 - acc: 0.9063Epoch 00205: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2722 - acc: 0.9067 - val_loss: 0.2642 - val_acc: 0.9097
Epoch 206/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2710 - acc: 0.9078Epoch 00206: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2720 - acc: 0.9074 - val_loss: 0.2567 - val_acc: 0.9142
Epoch 207/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2592 - acc: 0.9121Epoch 00207: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2612 - acc: 0.9113 - val_loss: 0.2575 - val_acc: 0.9153
Epoch 208/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2732 - acc: 0.9081Epoch 00208: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2726 - acc: 0.9080 - val_loss: 0.2545 - val_acc: 0.9114
Epoch 209/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2626 - acc: 0.9114Epoch 00209: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2646 - acc: 0.9107 - val_loss: 0.2575 - val_acc: 0.9081
Epoch 210/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2728 - acc: 0.9105Epoch 00210: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2735 - acc: 0.9102 - val_loss: 0.2522 - val_acc: 0.9131
Epoch 211/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2675 - acc: 0.9095Epoch 00211: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2661 - acc: 0.9099 - val_loss: 0.2561 - val_acc: 0.9153
Epoch 212/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2719 - acc: 0.9062Epoch 00212: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2740 - acc: 0.9057 - val_loss: 0.2584 - val_acc: 0.9120
Epoch 213/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2625 - acc: 0.9097Epoch 00213: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2630 - acc: 0.9100 - val_loss: 0.2544 - val_acc: 0.9136
Epoch 214/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2644 - acc: 0.9088Epoch 00214: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2641 - acc: 0.9086 - val_loss: 0.2570 - val_acc: 0.9148
Epoch 215/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2605 - acc: 0.9110Epoch 00215: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2609 - acc: 0.9109 - val_loss: 0.2526 - val_acc: 0.9164
Epoch 216/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2595 - acc: 0.9111Epoch 00216: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2599 - acc: 0.9108 - val_loss: 0.2552 - val_acc: 0.9114
Epoch 217/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2593 - acc: 0.9102Epoch 00217: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2597 - acc: 0.9099 - val_loss: 0.2504 - val_acc: 0.9142
Epoch 218/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2664 - acc: 0.9076Epoch 00218: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2663 - acc: 0.9077 - val_loss: 0.2526 - val_acc: 0.9164
Epoch 219/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2588 - acc: 0.9126Epoch 00219: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2578 - acc: 0.9128 - val_loss: 0.2514 - val_acc: 0.9159
Epoch 220/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2604 - acc: 0.9095Epoch 00220: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2598 - acc: 0.9099 - val_loss: 0.2519 - val_acc: 0.9159
Epoch 221/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2466 - acc: 0.9152Epoch 00221: val_loss did not improve
17885/17885 [==============================] - 2s 90us/step - loss: 0.2464 - acc: 0.9154 - val_loss: 0.2514 - val_acc: 0.9164
Epoch 222/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2605 - acc: 0.9117Epoch 00222: val_loss did not improve
17885/17885 [==============================] - 2s 90us/step - loss: 0.2595 - acc: 0.9119 - val_loss: 0.2514 - val_acc: 0.9153
Epoch 223/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2573 - acc: 0.9125Epoch 00223: val_loss did not improve
17885/17885 [==============================] - 2s 90us/step - loss: 0.2565 - acc: 0.9131 - val_loss: 0.2501 - val_acc: 0.9114
Epoch 224/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2487 - acc: 0.9172Epoch 00224: val_loss did not improve
17885/17885 [==============================] - 2s 89us/step - loss: 0.2495 - acc: 0.9164 - val_loss: 0.2513 - val_acc: 0.9170
Epoch 225/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2554 - acc: 0.9108Epoch 00225: val_loss did not improve
17885/17885 [==============================] - 2s 89us/step - loss: 0.2531 - acc: 0.9117 - val_loss: 0.2510 - val_acc: 0.9131
Epoch 226/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2455 - acc: 0.9148Epoch 00226: val_loss improved from 0.24994 to 0.24582, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 92us/step - loss: 0.2467 - acc: 0.9141 - val_loss: 0.2458 - val_acc: 0.9136
Epoch 227/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2560 - acc: 0.9135Epoch 00227: val_loss did not improve
17885/17885 [==============================] - 2s 90us/step - loss: 0.2541 - acc: 0.9137 - val_loss: 0.2503 - val_acc: 0.9131
Epoch 228/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2504 - acc: 0.9157Epoch 00228: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2494 - acc: 0.9161 - val_loss: 0.2463 - val_acc: 0.9148
Epoch 229/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2537 - acc: 0.9133Epoch 00229: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2535 - acc: 0.9137 - val_loss: 0.2494 - val_acc: 0.9159
Epoch 230/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2470 - acc: 0.9152Epoch 00230: val_loss improved from 0.24582 to 0.24569, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 94us/step - loss: 0.2482 - acc: 0.9148 - val_loss: 0.2457 - val_acc: 0.9187
Epoch 231/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2497 - acc: 0.9145Epoch 00231: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2484 - acc: 0.9150 - val_loss: 0.2490 - val_acc: 0.9187
Epoch 232/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2489 - acc: 0.9146Epoch 00232: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2485 - acc: 0.9147 - val_loss: 0.2472 - val_acc: 0.9114
Epoch 233/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2416 - acc: 0.9172Epoch 00233: val_loss did not improve
17885/17885 [==============================] - 2s 93us/step - loss: 0.2414 - acc: 0.9170 - val_loss: 0.2484 - val_acc: 0.9142
Epoch 234/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2489 - acc: 0.9132Epoch 00234: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2495 - acc: 0.9134 - val_loss: 0.2477 - val_acc: 0.9187
Epoch 235/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2411 - acc: 0.9165Epoch 00235: val_loss improved from 0.24569 to 0.24529, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.2422 - acc: 0.9162 - val_loss: 0.2453 - val_acc: 0.9181
Epoch 236/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2402 - acc: 0.9180Epoch 00236: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2407 - acc: 0.9181 - val_loss: 0.2495 - val_acc: 0.9181
Epoch 237/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2387 - acc: 0.9174Epoch 00237: val_loss improved from 0.24529 to 0.23990, saving model to weights2.hdf5
17885/17885 [==============================] - 2s 93us/step - loss: 0.2397 - acc: 0.9176 - val_loss: 0.2399 - val_acc: 0.9192
Epoch 238/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2431 - acc: 0.9176Epoch 00238: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2423 - acc: 0.9183 - val_loss: 0.2464 - val_acc: 0.9175
Epoch 239/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2354 - acc: 0.9187Epoch 00239: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2359 - acc: 0.9186 - val_loss: 0.2532 - val_acc: 0.9175
Epoch 240/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2399 - acc: 0.9173Epoch 00240: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2399 - acc: 0.9176 - val_loss: 0.2508 - val_acc: 0.9159
Epoch 241/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2360 - acc: 0.9219Epoch 00241: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2368 - acc: 0.9216 - val_loss: 0.2446 - val_acc: 0.9142
Epoch 242/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2442 - acc: 0.9146Epoch 00242: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2446 - acc: 0.9146 - val_loss: 0.2477 - val_acc: 0.9153
Epoch 243/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2360 - acc: 0.9196Epoch 00243: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2350 - acc: 0.9200 - val_loss: 0.2479 - val_acc: 0.9170
Epoch 244/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2360 - acc: 0.9209Epoch 00244: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2342 - acc: 0.9212 - val_loss: 0.2458 - val_acc: 0.9175
Epoch 245/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2350 - acc: 0.9198Epoch 00245: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2350 - acc: 0.9193 - val_loss: 0.2461 - val_acc: 0.9164
Epoch 246/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2385 - acc: 0.9184Epoch 00246: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2388 - acc: 0.9184 - val_loss: 0.2475 - val_acc: 0.9170
Epoch 247/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2356 - acc: 0.9167Epoch 00247: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2356 - acc: 0.9169 - val_loss: 0.2485 - val_acc: 0.9170
Epoch 248/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2306 - acc: 0.9186Epoch 00248: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2311 - acc: 0.9189 - val_loss: 0.2470 - val_acc: 0.9175
Epoch 249/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2328 - acc: 0.9214Epoch 00249: val_loss did not improve
17885/17885 [==============================] - 2s 91us/step - loss: 0.2317 - acc: 0.9219 - val_loss: 0.2513 - val_acc: 0.9192
Epoch 250/250
17152/17885 [===========================&gt;..] - ETA: 0s - loss: 0.2313 - acc: 0.9196Epoch 00250: val_loss did not improve
17885/17885 [==============================] - 2s 92us/step - loss: 0.2293 - acc: 0.9203 - val_loss: 0.2445 - val_acc: 0.9153
3


 trianing 



Train on 17887 samples, validate on 1793 samples
Epoch 1/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 7.1625 - acc: 0.0889Epoch 00001: val_loss improved from inf to 2.48469, saving model to weights3.hdf5
17887/17887 [==============================] - 3s 143us/step - loss: 6.9733 - acc: 0.0886 - val_loss: 2.4847 - val_acc: 0.0853
Epoch 2/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.5219 - acc: 0.0897Epoch 00002: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 2.5205 - acc: 0.0900 - val_loss: 2.4848 - val_acc: 0.0848
Epoch 3/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4946 - acc: 0.0831Epoch 00003: val_loss improved from 2.48469 to 2.48389, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4943 - acc: 0.0841 - val_loss: 2.4839 - val_acc: 0.0926
Epoch 4/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4760 - acc: 0.0988Epoch 00004: val_loss improved from 2.48389 to 2.45359, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4757 - acc: 0.0993 - val_loss: 2.4536 - val_acc: 0.1032
Epoch 5/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4401 - acc: 0.1090Epoch 00005: val_loss improved from 2.45359 to 2.44345, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4403 - acc: 0.1090 - val_loss: 2.4435 - val_acc: 0.1037
Epoch 6/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4329 - acc: 0.1086Epoch 00006: val_loss improved from 2.44345 to 2.44231, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4320 - acc: 0.1092 - val_loss: 2.4423 - val_acc: 0.1032
Epoch 7/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4298 - acc: 0.1099Epoch 00007: val_loss improved from 2.44231 to 2.44166, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4296 - acc: 0.1103 - val_loss: 2.4417 - val_acc: 0.1032
Epoch 8/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4298 - acc: 0.1096Epoch 00008: val_loss improved from 2.44166 to 2.44117, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4291 - acc: 0.1098 - val_loss: 2.4412 - val_acc: 0.1037
Epoch 9/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4262 - acc: 0.1109Epoch 00009: val_loss improved from 2.44117 to 2.44075, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4263 - acc: 0.1103 - val_loss: 2.4408 - val_acc: 0.1037
Epoch 10/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4242 - acc: 0.1110Epoch 00010: val_loss improved from 2.44075 to 2.44045, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 2.4252 - acc: 0.1104 - val_loss: 2.4404 - val_acc: 0.1032
Epoch 11/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4241 - acc: 0.1107Epoch 00011: val_loss improved from 2.44045 to 2.44017, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 95us/step - loss: 2.4240 - acc: 0.1103 - val_loss: 2.4402 - val_acc: 0.1032
Epoch 12/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4237 - acc: 0.1099Epoch 00012: val_loss improved from 2.44017 to 2.44002, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4241 - acc: 0.1100 - val_loss: 2.4400 - val_acc: 0.1032
Epoch 13/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4237 - acc: 0.1095Epoch 00013: val_loss improved from 2.44002 to 2.43994, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 2.4234 - acc: 0.1096 - val_loss: 2.4399 - val_acc: 0.1032
Epoch 14/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4213 - acc: 0.1117Epoch 00014: val_loss improved from 2.43994 to 2.43963, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 2.4224 - acc: 0.1103 - val_loss: 2.4396 - val_acc: 0.1032
Epoch 15/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4219 - acc: 0.1101Epoch 00015: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 2.4220 - acc: 0.1106 - val_loss: 2.4397 - val_acc: 0.1032
Epoch 16/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4211 - acc: 0.1098Epoch 00016: val_loss improved from 2.43963 to 2.43947, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4217 - acc: 0.1099 - val_loss: 2.4395 - val_acc: 0.1032
Epoch 17/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4210 - acc: 0.1105Epoch 00017: val_loss improved from 2.43947 to 2.43937, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4218 - acc: 0.1105 - val_loss: 2.4394 - val_acc: 0.1032
Epoch 18/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4211 - acc: 0.1108Epoch 00018: val_loss improved from 2.43937 to 2.43932, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4218 - acc: 0.1097 - val_loss: 2.4393 - val_acc: 0.1032
Epoch 19/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4215 - acc: 0.1107Epoch 00019: val_loss improved from 2.43932 to 2.43929, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4216 - acc: 0.1102 - val_loss: 2.4393 - val_acc: 0.1032
Epoch 20/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4216 - acc: 0.1086Epoch 00020: val_loss improved from 2.43929 to 2.43927, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 2.4213 - acc: 0.1089 - val_loss: 2.4393 - val_acc: 0.1032
Epoch 21/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4209 - acc: 0.1102Epoch 00021: val_loss improved from 2.43927 to 2.43926, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 2.4210 - acc: 0.1099 - val_loss: 2.4393 - val_acc: 0.1032
Epoch 22/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4211 - acc: 0.1104Epoch 00022: val_loss improved from 2.43926 to 2.43926, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4208 - acc: 0.1106 - val_loss: 2.4393 - val_acc: 0.1032
Epoch 23/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4201 - acc: 0.1097Epoch 00023: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 2.4208 - acc: 0.1096 - val_loss: 2.4393 - val_acc: 0.1032
Epoch 24/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4212 - acc: 0.1088- ETA: 1s - loss: 2.4Epoch 00024: val_loss did not improve
17887/17887 [==============================] - 2s 90us/step - loss: 2.4208 - acc: 0.1090 - val_loss: 2.4393 - val_acc: 0.1032
Epoch 25/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4201 - acc: 0.1099Epoch 00025: val_loss did not improve
17887/17887 [==============================] - 2s 90us/step - loss: 2.4200 - acc: 0.1098 - val_loss: 2.4393 - val_acc: 0.1032
Epoch 26/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4200 - acc: 0.1104Epoch 00026: val_loss did not improve
17887/17887 [==============================] - 2s 90us/step - loss: 2.4205 - acc: 0.1104 - val_loss: 2.4393 - val_acc: 0.1032
Epoch 27/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4199 - acc: 0.1098Epoch 00027: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 2.4205 - acc: 0.1096 - val_loss: 2.4393 - val_acc: 0.1032
Epoch 28/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4191 - acc: 0.1103Epoch 00028: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 2.4200 - acc: 0.1102 - val_loss: 2.4393 - val_acc: 0.1032
Epoch 29/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4211 - acc: 0.1097Epoch 00029: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 2.4208 - acc: 0.1101 - val_loss: 2.4394 - val_acc: 0.1032
Epoch 30/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4198 - acc: 0.1081Epoch 00030: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 2.4203 - acc: 0.1089 - val_loss: 2.4394 - val_acc: 0.1032
Epoch 31/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4132 - acc: 0.1151Epoch 00031: val_loss improved from 2.43926 to 2.41211, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4135 - acc: 0.1145 - val_loss: 2.4121 - val_acc: 0.1115
Epoch 32/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.3953 - acc: 0.1279Epoch 00032: val_loss improved from 2.41211 to 2.39603, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.3949 - acc: 0.1277 - val_loss: 2.3960 - val_acc: 0.1283
Epoch 33/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.3749 - acc: 0.1391Epoch 00033: val_loss improved from 2.39603 to 2.32371, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 2.3739 - acc: 0.1395 - val_loss: 2.3237 - val_acc: 0.1634
Epoch 34/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.3457 - acc: 0.1498Epoch 00034: val_loss improved from 2.32371 to 2.29497, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 2.3456 - acc: 0.1509 - val_loss: 2.2950 - val_acc: 0.1629
Epoch 35/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.3257 - acc: 0.1569Epoch 00035: val_loss improved from 2.29497 to 2.26753, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.3261 - acc: 0.1564 - val_loss: 2.2675 - val_acc: 0.1550
Epoch 36/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.3094 - acc: 0.1637Epoch 00036: val_loss improved from 2.26753 to 2.23087, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 2.3082 - acc: 0.1640 - val_loss: 2.2309 - val_acc: 0.1924
Epoch 37/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.2582 - acc: 0.1862Epoch 00037: val_loss improved from 2.23087 to 2.10897, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.2573 - acc: 0.1865 - val_loss: 2.1090 - val_acc: 0.2499
Epoch 38/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.1782 - acc: 0.2293Epoch 00038: val_loss improved from 2.10897 to 1.98424, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.1753 - acc: 0.2307 - val_loss: 1.9842 - val_acc: 0.3023
Epoch 39/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.0803 - acc: 0.2589Epoch 00039: val_loss improved from 1.98424 to 1.90026, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.0764 - acc: 0.2601 - val_loss: 1.9003 - val_acc: 0.3173
Epoch 40/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.9965 - acc: 0.2859Epoch 00040: val_loss improved from 1.90026 to 1.77865, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.9951 - acc: 0.2872 - val_loss: 1.7787 - val_acc: 0.3748
Epoch 41/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.8929 - acc: 0.3161Epoch 00041: val_loss improved from 1.77865 to 1.66281, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.8886 - acc: 0.3170 - val_loss: 1.6628 - val_acc: 0.4077
Epoch 42/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.8088 - acc: 0.3492Epoch 00042: val_loss improved from 1.66281 to 1.56489, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.8088 - acc: 0.3491 - val_loss: 1.5649 - val_acc: 0.4668
Epoch 43/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.7069 - acc: 0.3814Epoch 00043: val_loss improved from 1.56489 to 1.43653, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.7088 - acc: 0.3811 - val_loss: 1.4365 - val_acc: 0.5276
Epoch 44/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.5948 - acc: 0.4362Epoch 00044: val_loss improved from 1.43653 to 1.28695, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.5924 - acc: 0.4369 - val_loss: 1.2870 - val_acc: 0.5717
Epoch 45/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.5143 - acc: 0.4612Epoch 00045: val_loss improved from 1.28695 to 1.19312, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.5143 - acc: 0.4618 - val_loss: 1.1931 - val_acc: 0.6213
Epoch 46/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.4343 - acc: 0.4915Epoch 00046: val_loss improved from 1.19312 to 1.12009, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 1.4319 - acc: 0.4915 - val_loss: 1.1201 - val_acc: 0.6481
Epoch 47/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.3641 - acc: 0.5213Epoch 00047: val_loss improved from 1.12009 to 1.05083, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.3601 - acc: 0.5228 - val_loss: 1.0508 - val_acc: 0.6637
Epoch 48/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.2886 - acc: 0.5473Epoch 00048: val_loss improved from 1.05083 to 0.98719, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.2897 - acc: 0.5473 - val_loss: 0.9872 - val_acc: 0.6810
Epoch 49/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.2403 - acc: 0.5644Epoch 00049: val_loss improved from 0.98719 to 0.93528, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 92us/step - loss: 1.2397 - acc: 0.5638 - val_loss: 0.9353 - val_acc: 0.6983
Epoch 50/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.1854 - acc: 0.5845Epoch 00050: val_loss improved from 0.93528 to 0.87765, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.1859 - acc: 0.5841 - val_loss: 0.8777 - val_acc: 0.7150
Epoch 51/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.1376 - acc: 0.6030Epoch 00051: val_loss improved from 0.87765 to 0.84408, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.1369 - acc: 0.6033 - val_loss: 0.8441 - val_acc: 0.7323
Epoch 52/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.0972 - acc: 0.6241Epoch 00052: val_loss improved from 0.84408 to 0.80597, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.0954 - acc: 0.6246 - val_loss: 0.8060 - val_acc: 0.7501
Epoch 53/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.0566 - acc: 0.6346Epoch 00053: val_loss improved from 0.80597 to 0.75990, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.0566 - acc: 0.6347 - val_loss: 0.7599 - val_acc: 0.7574
Epoch 54/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.0191 - acc: 0.6477Epoch 00054: val_loss improved from 0.75990 to 0.73805, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.0197 - acc: 0.6476 - val_loss: 0.7380 - val_acc: 0.7691
Epoch 55/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.9885 - acc: 0.6582Epoch 00055: val_loss improved from 0.73805 to 0.70492, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.9875 - acc: 0.6586 - val_loss: 0.7049 - val_acc: 0.7758
Epoch 56/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.9479 - acc: 0.6726Epoch 00056: val_loss improved from 0.70492 to 0.69591, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 92us/step - loss: 0.9504 - acc: 0.6715 - val_loss: 0.6959 - val_acc: 0.7830
Epoch 57/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.9356 - acc: 0.6800Epoch 00057: val_loss improved from 0.69591 to 0.66398, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.9341 - acc: 0.6808 - val_loss: 0.6640 - val_acc: 0.7869
Epoch 58/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.9039 - acc: 0.6900Epoch 00058: val_loss improved from 0.66398 to 0.66040, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.9043 - acc: 0.6897 - val_loss: 0.6604 - val_acc: 0.7987
Epoch 59/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.8859 - acc: 0.6972Epoch 00059: val_loss improved from 0.66040 to 0.62594, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.8856 - acc: 0.6968 - val_loss: 0.6259 - val_acc: 0.8009
Epoch 60/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.8568 - acc: 0.7061Epoch 00060: val_loss improved from 0.62594 to 0.60214, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.8554 - acc: 0.7065 - val_loss: 0.6021 - val_acc: 0.8221
Epoch 61/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.8347 - acc: 0.7116Epoch 00061: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.8358 - acc: 0.7107 - val_loss: 0.6114 - val_acc: 0.8065
Epoch 62/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.8141 - acc: 0.7204- ETA: 0s - loss: 0.8157 - accEpoch 00062: val_loss improved from 0.60214 to 0.58281, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.8152 - acc: 0.7206 - val_loss: 0.5828 - val_acc: 0.8165
Epoch 63/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.7943 - acc: 0.7263Epoch 00063: val_loss improved from 0.58281 to 0.56186, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.7927 - acc: 0.7272 - val_loss: 0.5619 - val_acc: 0.8282
Epoch 64/250
17664/17887 [============================&gt;.] - ETA: 0s - loss: 0.7928 - acc: 0.7301Epoch 00064: val_loss improved from 0.56186 to 0.55509, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.7916 - acc: 0.7301 - val_loss: 0.5551 - val_acc: 0.8299
Epoch 65/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.7703 - acc: 0.7350Epoch 00065: val_loss improved from 0.55509 to 0.54099, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.7702 - acc: 0.7357 - val_loss: 0.5410 - val_acc: 0.8338
Epoch 66/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.7518 - acc: 0.7413Epoch 00066: val_loss improved from 0.54099 to 0.53287, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.7499 - acc: 0.7419 - val_loss: 0.5329 - val_acc: 0.8371
Epoch 67/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.7366 - acc: 0.7487Epoch 00067: val_loss improved from 0.53287 to 0.52501, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.7364 - acc: 0.7490 - val_loss: 0.5250 - val_acc: 0.8371
Epoch 68/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.7288 - acc: 0.7513Epoch 00068: val_loss improved from 0.52501 to 0.51391, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.7269 - acc: 0.7517 - val_loss: 0.5139 - val_acc: 0.8427
Epoch 69/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.7128 - acc: 0.7562Epoch 00069: val_loss improved from 0.51391 to 0.50305, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.7106 - acc: 0.7573 - val_loss: 0.5031 - val_acc: 0.8472
Epoch 70/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6985 - acc: 0.7665Epoch 00070: val_loss improved from 0.50305 to 0.49469, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.6965 - acc: 0.7681 - val_loss: 0.4947 - val_acc: 0.8427
Epoch 71/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6825 - acc: 0.7682Epoch 00071: val_loss improved from 0.49469 to 0.48569, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.6828 - acc: 0.7681 - val_loss: 0.4857 - val_acc: 0.8505
Epoch 72/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6703 - acc: 0.7745Epoch 00072: val_loss improved from 0.48569 to 0.47980, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.6695 - acc: 0.7742 - val_loss: 0.4798 - val_acc: 0.8561
Epoch 73/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6665 - acc: 0.7751Epoch 00073: val_loss improved from 0.47980 to 0.47635, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.6684 - acc: 0.7747 - val_loss: 0.4764 - val_acc: 0.8567
Epoch 74/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6436 - acc: 0.7811Epoch 00074: val_loss improved from 0.47635 to 0.45558, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.6428 - acc: 0.7817 - val_loss: 0.4556 - val_acc: 0.8634
Epoch 75/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6330 - acc: 0.7877Epoch 00075: val_loss improved from 0.45558 to 0.44984, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.6317 - acc: 0.7874 - val_loss: 0.4498 - val_acc: 0.8516
Epoch 76/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6388 - acc: 0.7801Epoch 00076: val_loss improved from 0.44984 to 0.44579, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.6359 - acc: 0.7813 - val_loss: 0.4458 - val_acc: 0.8589
Epoch 77/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6234 - acc: 0.7914Epoch 00077: val_loss improved from 0.44579 to 0.42994, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.6240 - acc: 0.7911 - val_loss: 0.4299 - val_acc: 0.8678
Epoch 78/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6136 - acc: 0.7887Epoch 00078: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.6136 - acc: 0.7891 - val_loss: 0.4456 - val_acc: 0.8622
Epoch 79/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6035 - acc: 0.7965Epoch 00079: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.6045 - acc: 0.7967 - val_loss: 0.4312 - val_acc: 0.8678
Epoch 80/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5961 - acc: 0.7983Epoch 00080: val_loss improved from 0.42994 to 0.42297, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.5930 - acc: 0.7992 - val_loss: 0.4230 - val_acc: 0.8678
Epoch 81/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5796 - acc: 0.8005Epoch 00081: val_loss improved from 0.42297 to 0.42034, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.5799 - acc: 0.8000 - val_loss: 0.4203 - val_acc: 0.8673
Epoch 82/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5716 - acc: 0.8065Epoch 00082: val_loss improved from 0.42034 to 0.41082, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.5723 - acc: 0.8062 - val_loss: 0.4108 - val_acc: 0.8712
Epoch 83/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5676 - acc: 0.8070Epoch 00083: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.5680 - acc: 0.8073 - val_loss: 0.4150 - val_acc: 0.8723
Epoch 84/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5663 - acc: 0.8096Epoch 00084: val_loss improved from 0.41082 to 0.39801, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.5658 - acc: 0.8098 - val_loss: 0.3980 - val_acc: 0.8767
Epoch 85/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5526 - acc: 0.8124Epoch 00085: val_loss improved from 0.39801 to 0.39761, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.5539 - acc: 0.8127 - val_loss: 0.3976 - val_acc: 0.8779
Epoch 86/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5457 - acc: 0.8154Epoch 00086: val_loss improved from 0.39761 to 0.38897, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.5481 - acc: 0.8151 - val_loss: 0.3890 - val_acc: 0.8751
Epoch 87/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5411 - acc: 0.8161Epoch 00087: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.5426 - acc: 0.8150 - val_loss: 0.3908 - val_acc: 0.8823
Epoch 88/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5356 - acc: 0.8191Epoch 00088: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.5364 - acc: 0.8190 - val_loss: 0.3902 - val_acc: 0.8767
Epoch 89/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5316 - acc: 0.8210Epoch 00089: val_loss improved from 0.38897 to 0.38650, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.5303 - acc: 0.8214 - val_loss: 0.3865 - val_acc: 0.8773
Epoch 90/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5341 - acc: 0.8201Epoch 00090: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.5353 - acc: 0.8198 - val_loss: 0.3899 - val_acc: 0.8784
Epoch 91/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5080 - acc: 0.8312Epoch 00091: val_loss improved from 0.38650 to 0.38274, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.5082 - acc: 0.8305 - val_loss: 0.3827 - val_acc: 0.8756
Epoch 92/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5100 - acc: 0.8257Epoch 00092: val_loss improved from 0.38274 to 0.37247, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.5106 - acc: 0.8252 - val_loss: 0.3725 - val_acc: 0.8818
Epoch 93/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5059 - acc: 0.8296Epoch 00093: val_loss improved from 0.37247 to 0.36688, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.5052 - acc: 0.8300 - val_loss: 0.3669 - val_acc: 0.8879
Epoch 94/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5041 - acc: 0.8310Epoch 00094: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.5032 - acc: 0.8315 - val_loss: 0.3670 - val_acc: 0.8851
Epoch 95/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4956 - acc: 0.8326Epoch 00095: val_loss improved from 0.36688 to 0.36364, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.4947 - acc: 0.8328 - val_loss: 0.3636 - val_acc: 0.8868
Epoch 96/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4967 - acc: 0.8291- ETA: 0s - loss: 0.4931 - acc: 0Epoch 00096: val_loss improved from 0.36364 to 0.35872, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.4971 - acc: 0.8286 - val_loss: 0.3587 - val_acc: 0.8896
Epoch 97/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4794 - acc: 0.8377Epoch 00097: val_loss improved from 0.35872 to 0.35853, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.4787 - acc: 0.8380 - val_loss: 0.3585 - val_acc: 0.8879
Epoch 98/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4689 - acc: 0.8447Epoch 00098: val_loss improved from 0.35853 to 0.35747, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 92us/step - loss: 0.4717 - acc: 0.8440 - val_loss: 0.3575 - val_acc: 0.8907
Epoch 99/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4736 - acc: 0.8396Epoch 00099: val_loss improved from 0.35747 to 0.35565, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.4758 - acc: 0.8384 - val_loss: 0.3557 - val_acc: 0.8907
Epoch 100/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4723 - acc: 0.8396Epoch 00100: val_loss improved from 0.35565 to 0.34616, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.4752 - acc: 0.8380 - val_loss: 0.3462 - val_acc: 0.8912
Epoch 101/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4688 - acc: 0.8435Epoch 00101: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.4697 - acc: 0.8428 - val_loss: 0.3590 - val_acc: 0.8885
Epoch 102/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4685 - acc: 0.8409Epoch 00102: val_loss improved from 0.34616 to 0.34526, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4697 - acc: 0.8403 - val_loss: 0.3453 - val_acc: 0.8957
Epoch 103/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4595 - acc: 0.8458Epoch 00103: val_loss improved from 0.34526 to 0.34076, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4569 - acc: 0.8469 - val_loss: 0.3408 - val_acc: 0.8951
Epoch 104/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4581 - acc: 0.8444Epoch 00104: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.4597 - acc: 0.8439 - val_loss: 0.3445 - val_acc: 0.8957
Epoch 105/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4511 - acc: 0.8494Epoch 00105: val_loss improved from 0.34076 to 0.33545, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4488 - acc: 0.8499 - val_loss: 0.3354 - val_acc: 0.8985
Epoch 106/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4404 - acc: 0.8519Epoch 00106: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.4405 - acc: 0.8517 - val_loss: 0.3383 - val_acc: 0.8924
Epoch 107/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4451 - acc: 0.8496Epoch 00107: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.4459 - acc: 0.8496 - val_loss: 0.3398 - val_acc: 0.8946
Epoch 108/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4288 - acc: 0.8535Epoch 00108: val_loss improved from 0.33545 to 0.33245, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4292 - acc: 0.8534 - val_loss: 0.3324 - val_acc: 0.8918
Epoch 109/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4271 - acc: 0.8576Epoch 00109: val_loss improved from 0.33245 to 0.32807, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4254 - acc: 0.8586 - val_loss: 0.3281 - val_acc: 0.8935
Epoch 110/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4382 - acc: 0.8521Epoch 00110: val_loss improved from 0.32807 to 0.32628, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4390 - acc: 0.8518 - val_loss: 0.3263 - val_acc: 0.9007
Epoch 111/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4257 - acc: 0.8561Epoch 00111: val_loss improved from 0.32628 to 0.32330, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4257 - acc: 0.8558 - val_loss: 0.3233 - val_acc: 0.8957
Epoch 112/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4321 - acc: 0.8549Epoch 00112: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.4322 - acc: 0.8555 - val_loss: 0.3317 - val_acc: 0.8991
Epoch 113/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4193 - acc: 0.8569Epoch 00113: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.4184 - acc: 0.8581 - val_loss: 0.3236 - val_acc: 0.8974
Epoch 114/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4204 - acc: 0.8558Epoch 00114: val_loss improved from 0.32330 to 0.32240, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4189 - acc: 0.8569 - val_loss: 0.3224 - val_acc: 0.9007
Epoch 115/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4169 - acc: 0.8598Epoch 00115: val_loss improved from 0.32240 to 0.32098, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4187 - acc: 0.8592 - val_loss: 0.3210 - val_acc: 0.8991
Epoch 116/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4199 - acc: 0.8599Epoch 00116: val_loss improved from 0.32098 to 0.31868, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.4187 - acc: 0.8599 - val_loss: 0.3187 - val_acc: 0.9007
Epoch 117/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4096 - acc: 0.8625Epoch 00117: val_loss improved from 0.31868 to 0.31349, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4111 - acc: 0.8619 - val_loss: 0.3135 - val_acc: 0.8985
Epoch 118/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4068 - acc: 0.8641Epoch 00118: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.4083 - acc: 0.8636 - val_loss: 0.3153 - val_acc: 0.8951
Epoch 119/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4070 - acc: 0.8612Epoch 00119: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.4061 - acc: 0.8613 - val_loss: 0.3150 - val_acc: 0.9030
Epoch 120/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3944 - acc: 0.8653Epoch 00120: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3950 - acc: 0.8648 - val_loss: 0.3184 - val_acc: 0.8963
Epoch 121/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4079 - acc: 0.8628Epoch 00121: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.4081 - acc: 0.8626 - val_loss: 0.3188 - val_acc: 0.8935
Epoch 122/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3939 - acc: 0.8663Epoch 00122: val_loss improved from 0.31349 to 0.30963, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3937 - acc: 0.8664 - val_loss: 0.3096 - val_acc: 0.9052
Epoch 123/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3979 - acc: 0.8671Epoch 00123: val_loss improved from 0.30963 to 0.30766, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3970 - acc: 0.8673 - val_loss: 0.3077 - val_acc: 0.9041
Epoch 124/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3843 - acc: 0.8703Epoch 00124: val_loss improved from 0.30766 to 0.30708, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3839 - acc: 0.8701 - val_loss: 0.3071 - val_acc: 0.9024
Epoch 125/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3862 - acc: 0.8685Epoch 00125: val_loss improved from 0.30708 to 0.30656, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3866 - acc: 0.8682 - val_loss: 0.3066 - val_acc: 0.9046
Epoch 126/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3898 - acc: 0.8668Epoch 00126: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3886 - acc: 0.8676 - val_loss: 0.3076 - val_acc: 0.9035
Epoch 127/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3861 - acc: 0.8703Epoch 00127: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3863 - acc: 0.8703 - val_loss: 0.3113 - val_acc: 0.9013
Epoch 128/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3784 - acc: 0.8743Epoch 00128: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3792 - acc: 0.8742 - val_loss: 0.3176 - val_acc: 0.8985
Epoch 129/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3782 - acc: 0.8705Epoch 00129: val_loss improved from 0.30656 to 0.30618, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3777 - acc: 0.8706 - val_loss: 0.3062 - val_acc: 0.9035
Epoch 130/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3763 - acc: 0.8705Epoch 00130: val_loss improved from 0.30618 to 0.30212, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3753 - acc: 0.8708 - val_loss: 0.3021 - val_acc: 0.9080
Epoch 131/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3757 - acc: 0.8723Epoch 00131: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3753 - acc: 0.8724 - val_loss: 0.3027 - val_acc: 0.9002
Epoch 132/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3623 - acc: 0.8759Epoch 00132: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3639 - acc: 0.8757 - val_loss: 0.3100 - val_acc: 0.8991
Epoch 133/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3756 - acc: 0.8755Epoch 00133: val_loss improved from 0.30212 to 0.29708, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 92us/step - loss: 0.3741 - acc: 0.8755 - val_loss: 0.2971 - val_acc: 0.9052
Epoch 134/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3631 - acc: 0.8762Epoch 00134: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3628 - acc: 0.8761 - val_loss: 0.3029 - val_acc: 0.9074
Epoch 135/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3680 - acc: 0.8730Epoch 00135: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3672 - acc: 0.8734 - val_loss: 0.3005 - val_acc: 0.9080
Epoch 136/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3579 - acc: 0.8785Epoch 00136: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3578 - acc: 0.8786 - val_loss: 0.2996 - val_acc: 0.9057
Epoch 137/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3545 - acc: 0.8791Epoch 00137: val_loss improved from 0.29708 to 0.29704, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3561 - acc: 0.8785 - val_loss: 0.2970 - val_acc: 0.9069
Epoch 138/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3568 - acc: 0.8809Epoch 00138: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3556 - acc: 0.8810 - val_loss: 0.2995 - val_acc: 0.9057
Epoch 139/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3493 - acc: 0.8823Epoch 00139: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3522 - acc: 0.8811 - val_loss: 0.3023 - val_acc: 0.9035
Epoch 140/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3554 - acc: 0.8809Epoch 00140: val_loss improved from 0.29704 to 0.29453, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.3556 - acc: 0.8804 - val_loss: 0.2945 - val_acc: 0.9091
Epoch 141/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3492 - acc: 0.8814Epoch 00141: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3503 - acc: 0.8810 - val_loss: 0.2998 - val_acc: 0.9063
Epoch 142/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3523 - acc: 0.8809Epoch 00142: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3519 - acc: 0.8810 - val_loss: 0.2946 - val_acc: 0.9035
Epoch 143/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3479 - acc: 0.8831Epoch 00143: val_loss improved from 0.29453 to 0.29371, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.3472 - acc: 0.8835 - val_loss: 0.2937 - val_acc: 0.9102
Epoch 144/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3416 - acc: 0.8818Epoch 00144: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3413 - acc: 0.8820 - val_loss: 0.2943 - val_acc: 0.9041
Epoch 145/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3426 - acc: 0.8811Epoch 00145: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3440 - acc: 0.8808 - val_loss: 0.2982 - val_acc: 0.9069
Epoch 146/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3367 - acc: 0.8856Epoch 00146: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3362 - acc: 0.8854 - val_loss: 0.2964 - val_acc: 0.9052
Epoch 147/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3363 - acc: 0.8842Epoch 00147: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3353 - acc: 0.8845 - val_loss: 0.2981 - val_acc: 0.9041
Epoch 148/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3402 - acc: 0.8850Epoch 00148: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3404 - acc: 0.8849 - val_loss: 0.2978 - val_acc: 0.9052
Epoch 149/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3293 - acc: 0.8870Epoch 00149: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3293 - acc: 0.8868 - val_loss: 0.3002 - val_acc: 0.9052
Epoch 150/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3410 - acc: 0.8856Epoch 00150: val_loss improved from 0.29371 to 0.29119, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.3390 - acc: 0.8858 - val_loss: 0.2912 - val_acc: 0.9091
Epoch 151/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3301 - acc: 0.8879Epoch 00151: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3310 - acc: 0.8870 - val_loss: 0.2980 - val_acc: 0.9091
Epoch 152/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3335 - acc: 0.8890Epoch 00152: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3329 - acc: 0.8889 - val_loss: 0.2928 - val_acc: 0.9085
Epoch 153/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3367 - acc: 0.8868Epoch 00153: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3378 - acc: 0.8863 - val_loss: 0.3021 - val_acc: 0.9074
Epoch 154/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3140 - acc: 0.8911Epoch 00154: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3154 - acc: 0.8910 - val_loss: 0.2970 - val_acc: 0.9057
Epoch 155/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3249 - acc: 0.8911Epoch 00155: val_loss improved from 0.29119 to 0.28706, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3268 - acc: 0.8910 - val_loss: 0.2871 - val_acc: 0.9124
Epoch 156/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3222 - acc: 0.8932Epoch 00156: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3207 - acc: 0.8932 - val_loss: 0.2921 - val_acc: 0.9074
Epoch 157/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3154 - acc: 0.8893Epoch 00157: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3173 - acc: 0.8891 - val_loss: 0.2890 - val_acc: 0.9113
Epoch 158/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3206 - acc: 0.8913- ETA: 1s - loss: 0.3117Epoch 00158: val_loss did not improve
17887/17887 [==============================] - 2s 90us/step - loss: 0.3197 - acc: 0.8914 - val_loss: 0.2955 - val_acc: 0.9063
Epoch 159/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3146 - acc: 0.8944Epoch 00159: val_loss did not improve
17887/17887 [==============================] - 2s 90us/step - loss: 0.3147 - acc: 0.8944 - val_loss: 0.2956 - val_acc: 0.9069
Epoch 160/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3150 - acc: 0.8923Epoch 00160: val_loss improved from 0.28706 to 0.28506, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 92us/step - loss: 0.3162 - acc: 0.8912 - val_loss: 0.2851 - val_acc: 0.9136
Epoch 161/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3104 - acc: 0.8948Epoch 00161: val_loss did not improve
17887/17887 [==============================] - 2s 89us/step - loss: 0.3116 - acc: 0.8939 - val_loss: 0.2882 - val_acc: 0.9113
Epoch 162/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3122 - acc: 0.8921Epoch 00162: val_loss did not improve
17887/17887 [==============================] - 2s 90us/step - loss: 0.3110 - acc: 0.8925 - val_loss: 0.2927 - val_acc: 0.9057
Epoch 163/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3105 - acc: 0.8949Epoch 00163: val_loss did not improve
17887/17887 [==============================] - 2s 90us/step - loss: 0.3095 - acc: 0.8951 - val_loss: 0.2893 - val_acc: 0.9069
Epoch 164/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3182 - acc: 0.8923Epoch 00164: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3155 - acc: 0.8930 - val_loss: 0.2945 - val_acc: 0.9057
Epoch 165/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3123 - acc: 0.8916Epoch 00165: val_loss improved from 0.28506 to 0.27997, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3142 - acc: 0.8909 - val_loss: 0.2800 - val_acc: 0.9163
Epoch 166/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3076 - acc: 0.8953Epoch 00166: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3075 - acc: 0.8955 - val_loss: 0.2866 - val_acc: 0.9113
Epoch 167/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3105 - acc: 0.8956Epoch 00167: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3100 - acc: 0.8955 - val_loss: 0.2877 - val_acc: 0.9080
Epoch 168/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2997 - acc: 0.8979Epoch 00168: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3001 - acc: 0.8980 - val_loss: 0.2957 - val_acc: 0.9096
Epoch 169/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3039 - acc: 0.8974Epoch 00169: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3052 - acc: 0.8968 - val_loss: 0.2970 - val_acc: 0.9041
Epoch 170/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3042 - acc: 0.8967Epoch 00170: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3047 - acc: 0.8961 - val_loss: 0.2875 - val_acc: 0.9096
Epoch 171/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2982 - acc: 0.8988Epoch 00171: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2979 - acc: 0.8986 - val_loss: 0.2852 - val_acc: 0.9108
Epoch 172/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3071 - acc: 0.9000Epoch 00172: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3050 - acc: 0.9002 - val_loss: 0.2886 - val_acc: 0.9091
Epoch 173/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2971 - acc: 0.8987Epoch 00173: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2952 - acc: 0.8994 - val_loss: 0.2916 - val_acc: 0.9096
Epoch 174/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2879 - acc: 0.9023Epoch 00174: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2902 - acc: 0.9019 - val_loss: 0.2976 - val_acc: 0.9057
Epoch 175/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2975 - acc: 0.8981Epoch 00175: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2965 - acc: 0.8988 - val_loss: 0.2865 - val_acc: 0.9091
Epoch 176/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3007 - acc: 0.8992Epoch 00176: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3006 - acc: 0.8994 - val_loss: 0.2887 - val_acc: 0.9063
Epoch 177/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2936 - acc: 0.8974Epoch 00177: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2944 - acc: 0.8970 - val_loss: 0.2870 - val_acc: 0.9113
Epoch 178/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2968 - acc: 0.8991Epoch 00178: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2969 - acc: 0.8989 - val_loss: 0.2871 - val_acc: 0.9147
Epoch 179/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2839 - acc: 0.9016Epoch 00179: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2855 - acc: 0.9015 - val_loss: 0.2895 - val_acc: 0.9113
Epoch 180/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2894 - acc: 0.9008Epoch 00180: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2893 - acc: 0.9004 - val_loss: 0.2895 - val_acc: 0.9130
Epoch 181/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2878 - acc: 0.8997Epoch 00181: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2872 - acc: 0.9002 - val_loss: 0.2855 - val_acc: 0.9130
Epoch 182/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2860 - acc: 0.9002Epoch 00182: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2876 - acc: 0.9004 - val_loss: 0.2854 - val_acc: 0.9130
Epoch 183/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2863 - acc: 0.9034Epoch 00183: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2839 - acc: 0.9042 - val_loss: 0.2826 - val_acc: 0.9141
Epoch 184/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2861 - acc: 0.9011Epoch 00184: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2850 - acc: 0.9017 - val_loss: 0.2814 - val_acc: 0.9141
Epoch 185/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2871 - acc: 0.9022Epoch 00185: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2874 - acc: 0.9022 - val_loss: 0.2824 - val_acc: 0.9147
Epoch 186/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2825 - acc: 0.9032Epoch 00186: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2807 - acc: 0.9040 - val_loss: 0.2860 - val_acc: 0.9113
Epoch 187/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2852 - acc: 0.9015Epoch 00187: val_loss improved from 0.27997 to 0.27910, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.2866 - acc: 0.9013 - val_loss: 0.2791 - val_acc: 0.9136
Epoch 188/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2843 - acc: 0.9025Epoch 00188: val_loss improved from 0.27910 to 0.27689, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.2842 - acc: 0.9021 - val_loss: 0.2769 - val_acc: 0.9158
Epoch 189/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2840 - acc: 0.9029Epoch 00189: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2818 - acc: 0.9033 - val_loss: 0.2848 - val_acc: 0.9085
Epoch 190/250
17664/17887 [============================&gt;.] - ETA: 0s - loss: 0.2811 - acc: 0.9046Epoch 00190: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.2814 - acc: 0.9046 - val_loss: 0.2819 - val_acc: 0.9147
Epoch 191/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2766 - acc: 0.9049Epoch 00191: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.2760 - acc: 0.9055 - val_loss: 0.2858 - val_acc: 0.9147
Epoch 192/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2870 - acc: 0.9051Epoch 00192: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2865 - acc: 0.9049 - val_loss: 0.2833 - val_acc: 0.9130
Epoch 193/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2747 - acc: 0.9066Epoch 00193: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2737 - acc: 0.9072 - val_loss: 0.2850 - val_acc: 0.9130
Epoch 194/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2712 - acc: 0.9078Epoch 00194: val_loss improved from 0.27689 to 0.27501, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.2714 - acc: 0.9076 - val_loss: 0.2750 - val_acc: 0.9180
Epoch 195/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2736 - acc: 0.9088Epoch 00195: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2749 - acc: 0.9083 - val_loss: 0.2992 - val_acc: 0.9080
Epoch 196/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2738 - acc: 0.9069Epoch 00196: val_loss improved from 0.27501 to 0.27274, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.2740 - acc: 0.9066 - val_loss: 0.2727 - val_acc: 0.9136
Epoch 197/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2656 - acc: 0.9111Epoch 00197: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2683 - acc: 0.9106 - val_loss: 0.2806 - val_acc: 0.9180
Epoch 198/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2664 - acc: 0.9113Epoch 00198: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2689 - acc: 0.9105 - val_loss: 0.2811 - val_acc: 0.9091
Epoch 199/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2812 - acc: 0.9074Epoch 00199: val_loss improved from 0.27274 to 0.26593, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.2790 - acc: 0.9084 - val_loss: 0.2659 - val_acc: 0.9169
Epoch 200/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2678 - acc: 0.9093Epoch 00200: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2701 - acc: 0.9088 - val_loss: 0.2728 - val_acc: 0.9163
Epoch 201/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2721 - acc: 0.9086Epoch 00201: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2728 - acc: 0.9089 - val_loss: 0.2720 - val_acc: 0.9163
Epoch 202/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2721 - acc: 0.9061Epoch 00202: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2724 - acc: 0.9059 - val_loss: 0.2715 - val_acc: 0.9158
Epoch 203/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2677 - acc: 0.9100Epoch 00203: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2674 - acc: 0.9099 - val_loss: 0.2756 - val_acc: 0.9147
Epoch 204/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2646 - acc: 0.9091Epoch 00204: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2666 - acc: 0.9083 - val_loss: 0.2739 - val_acc: 0.9186
Epoch 205/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2559 - acc: 0.9114Epoch 00205: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2560 - acc: 0.9114 - val_loss: 0.2737 - val_acc: 0.9175
Epoch 206/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2588 - acc: 0.9090Epoch 00206: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2604 - acc: 0.9083 - val_loss: 0.2703 - val_acc: 0.9202
Epoch 207/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2677 - acc: 0.9079Epoch 00207: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2679 - acc: 0.9076 - val_loss: 0.2695 - val_acc: 0.9147
Epoch 208/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2615 - acc: 0.9095Epoch 00208: val_loss did not improve
17887/17887 [==============================] - 2s 90us/step - loss: 0.2642 - acc: 0.9094 - val_loss: 0.2762 - val_acc: 0.9158
Epoch 209/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2513 - acc: 0.9125Epoch 00209: val_loss did not improve
17887/17887 [==============================] - 2s 89us/step - loss: 0.2503 - acc: 0.9133 - val_loss: 0.2772 - val_acc: 0.9180
Epoch 210/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2533 - acc: 0.9141Epoch 00210: val_loss did not improve
17887/17887 [==============================] - 2s 89us/step - loss: 0.2527 - acc: 0.9138 - val_loss: 0.2734 - val_acc: 0.9175
Epoch 211/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2612 - acc: 0.9123Epoch 00211: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2616 - acc: 0.9121 - val_loss: 0.2738 - val_acc: 0.9130
Epoch 212/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2578 - acc: 0.9134Epoch 00212: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2592 - acc: 0.9130 - val_loss: 0.2827 - val_acc: 0.9119
Epoch 213/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2674 - acc: 0.9104Epoch 00213: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2709 - acc: 0.9088 - val_loss: 0.2720 - val_acc: 0.9180
Epoch 214/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2536 - acc: 0.9129Epoch 00214: val_loss improved from 0.26593 to 0.26576, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.2540 - acc: 0.9127 - val_loss: 0.2658 - val_acc: 0.9197
Epoch 215/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2563 - acc: 0.9127Epoch 00215: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2557 - acc: 0.9124 - val_loss: 0.2677 - val_acc: 0.9202
Epoch 216/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2519 - acc: 0.9144Epoch 00216: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2523 - acc: 0.9142 - val_loss: 0.2753 - val_acc: 0.9119
Epoch 217/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2514 - acc: 0.9146Epoch 00217: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2528 - acc: 0.9144 - val_loss: 0.2863 - val_acc: 0.9130
Epoch 218/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2557 - acc: 0.9121Epoch 00218: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2570 - acc: 0.9117 - val_loss: 0.2725 - val_acc: 0.9130
Epoch 219/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2464 - acc: 0.9153Epoch 00219: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2469 - acc: 0.9154 - val_loss: 0.2801 - val_acc: 0.9152
Epoch 220/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2556 - acc: 0.9133Epoch 00220: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2568 - acc: 0.9127 - val_loss: 0.2720 - val_acc: 0.9175
Epoch 221/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2525 - acc: 0.9137Epoch 00221: val_loss improved from 0.26576 to 0.26468, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.2506 - acc: 0.9144 - val_loss: 0.2647 - val_acc: 0.9175
Epoch 222/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2510 - acc: 0.9135Epoch 00222: val_loss improved from 0.26468 to 0.26437, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.2499 - acc: 0.9139 - val_loss: 0.2644 - val_acc: 0.9197
Epoch 223/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2457 - acc: 0.9170Epoch 00223: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2491 - acc: 0.9160 - val_loss: 0.2713 - val_acc: 0.9141
Epoch 224/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2516 - acc: 0.9134Epoch 00224: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.2508 - acc: 0.9133 - val_loss: 0.2678 - val_acc: 0.9197
Epoch 225/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2545 - acc: 0.9121Epoch 00225: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2532 - acc: 0.9127 - val_loss: 0.2671 - val_acc: 0.9191
Epoch 226/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2407 - acc: 0.9151Epoch 00226: val_loss improved from 0.26437 to 0.26281, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.2406 - acc: 0.9150 - val_loss: 0.2628 - val_acc: 0.9219
Epoch 227/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2478 - acc: 0.9165Epoch 00227: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2475 - acc: 0.9165 - val_loss: 0.2715 - val_acc: 0.9169
Epoch 228/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2481 - acc: 0.9149Epoch 00228: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2483 - acc: 0.9146 - val_loss: 0.2672 - val_acc: 0.9186
Epoch 229/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2397 - acc: 0.9180Epoch 00229: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2413 - acc: 0.9177 - val_loss: 0.2755 - val_acc: 0.9119
Epoch 230/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2404 - acc: 0.9175Epoch 00230: val_loss improved from 0.26281 to 0.26078, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.2402 - acc: 0.9180 - val_loss: 0.2608 - val_acc: 0.9163
Epoch 231/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2357 - acc: 0.9195Epoch 00231: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2375 - acc: 0.9190 - val_loss: 0.2673 - val_acc: 0.9152
Epoch 232/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2495 - acc: 0.9138Epoch 00232: val_loss improved from 0.26078 to 0.25975, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.2460 - acc: 0.9152 - val_loss: 0.2598 - val_acc: 0.9214
Epoch 233/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2419 - acc: 0.9169Epoch 00233: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.2413 - acc: 0.9174 - val_loss: 0.2670 - val_acc: 0.9186
Epoch 234/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2372 - acc: 0.9172Epoch 00234: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2373 - acc: 0.9171 - val_loss: 0.2625 - val_acc: 0.9175
Epoch 235/250
17664/17887 [============================&gt;.] - ETA: 0s - loss: 0.2384 - acc: 0.9165Epoch 00235: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.2382 - acc: 0.9165 - val_loss: 0.2692 - val_acc: 0.9175
Epoch 236/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2366 - acc: 0.9193Epoch 00236: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.2367 - acc: 0.9187 - val_loss: 0.2651 - val_acc: 0.9180
Epoch 237/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2455 - acc: 0.9170Epoch 00237: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2466 - acc: 0.9163 - val_loss: 0.2714 - val_acc: 0.9214
Epoch 238/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2405 - acc: 0.9193Epoch 00238: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2401 - acc: 0.9193 - val_loss: 0.2661 - val_acc: 0.9225
Epoch 239/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2379 - acc: 0.9170Epoch 00239: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2377 - acc: 0.9169 - val_loss: 0.2627 - val_acc: 0.9180
Epoch 240/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2358 - acc: 0.9181Epoch 00240: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2362 - acc: 0.9182 - val_loss: 0.2653 - val_acc: 0.9191
Epoch 241/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2400 - acc: 0.9205Epoch 00241: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2389 - acc: 0.9208 - val_loss: 0.2680 - val_acc: 0.9158
Epoch 242/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2303 - acc: 0.9211Epoch 00242: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2289 - acc: 0.9213 - val_loss: 0.2647 - val_acc: 0.9202
Epoch 243/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2366 - acc: 0.9170Epoch 00243: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2372 - acc: 0.9165 - val_loss: 0.2761 - val_acc: 0.9180
Epoch 244/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2383 - acc: 0.9195Epoch 00244: val_loss improved from 0.25975 to 0.25941, saving model to weights3.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.2387 - acc: 0.9196 - val_loss: 0.2594 - val_acc: 0.9197
Epoch 245/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2331 - acc: 0.9203Epoch 00245: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2342 - acc: 0.9202 - val_loss: 0.2612 - val_acc: 0.9191
Epoch 246/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2317 - acc: 0.9220Epoch 00246: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2317 - acc: 0.9223 - val_loss: 0.2674 - val_acc: 0.9158
Epoch 247/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2318 - acc: 0.9205Epoch 00247: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2327 - acc: 0.9199 - val_loss: 0.2600 - val_acc: 0.9191
Epoch 248/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2270 - acc: 0.9232Epoch 00248: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2268 - acc: 0.9233 - val_loss: 0.2624 - val_acc: 0.9191
Epoch 249/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2275 - acc: 0.9199Epoch 00249: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2279 - acc: 0.9199 - val_loss: 0.2729 - val_acc: 0.9163
Epoch 250/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2250 - acc: 0.9224Epoch 00250: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2253 - acc: 0.9224 - val_loss: 0.2673 - val_acc: 0.9214
4


 trianing 



Train on 17887 samples, validate on 1793 samples
Epoch 1/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 5.6031 - acc: 0.0940Epoch 00001: val_loss improved from inf to 2.48183, saving model to weights4.hdf5
17887/17887 [==============================] - 3s 148us/step - loss: 5.4767 - acc: 0.0937 - val_loss: 2.4818 - val_acc: 0.0931
Epoch 2/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4992 - acc: 0.0938Epoch 00002: val_loss improved from 2.48183 to 2.46297, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4985 - acc: 0.0952 - val_loss: 2.4630 - val_acc: 0.1104
Epoch 3/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4589 - acc: 0.1071Epoch 00003: val_loss improved from 2.46297 to 2.42684, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 2.4578 - acc: 0.1068 - val_loss: 2.4268 - val_acc: 0.1115
Epoch 4/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4385 - acc: 0.1074Epoch 00004: val_loss improved from 2.42684 to 2.42444, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4375 - acc: 0.1073 - val_loss: 2.4244 - val_acc: 0.1104
Epoch 5/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4318 - acc: 0.1072Epoch 00005: val_loss improved from 2.42444 to 2.42271, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 2.4318 - acc: 0.1075 - val_loss: 2.4227 - val_acc: 0.1104
Epoch 6/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4330 - acc: 0.1065Epoch 00006: val_loss improved from 2.42271 to 2.42208, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 2.4323 - acc: 0.1076 - val_loss: 2.4221 - val_acc: 0.1104
Epoch 7/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.4207 - acc: 0.1117Epoch 00007: val_loss improved from 2.42208 to 2.40357, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 2.4200 - acc: 0.1139 - val_loss: 2.4036 - val_acc: 0.1428
Epoch 8/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.3761 - acc: 0.1478Epoch 00008: val_loss improved from 2.40357 to 2.27840, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 2.3741 - acc: 0.1485 - val_loss: 2.2784 - val_acc: 0.1885
Epoch 9/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.3199 - acc: 0.1716Epoch 00009: val_loss improved from 2.27840 to 2.15431, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 2.3186 - acc: 0.1715 - val_loss: 2.1543 - val_acc: 0.2220
Epoch 10/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.2144 - acc: 0.2026Epoch 00010: val_loss improved from 2.15431 to 2.01186, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 2.2127 - acc: 0.2043 - val_loss: 2.0119 - val_acc: 0.3291
Epoch 11/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 2.0293 - acc: 0.2864Epoch 00011: val_loss improved from 2.01186 to 1.69823, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 2.0251 - acc: 0.2874 - val_loss: 1.6982 - val_acc: 0.4161
Epoch 12/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.8659 - acc: 0.3307Epoch 00012: val_loss improved from 1.69823 to 1.58440, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.8631 - acc: 0.3314 - val_loss: 1.5844 - val_acc: 0.4640
Epoch 13/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.7658 - acc: 0.3626Epoch 00013: val_loss improved from 1.58440 to 1.48177, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 1.7637 - acc: 0.3636 - val_loss: 1.4818 - val_acc: 0.4869
Epoch 14/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.6619 - acc: 0.4097Epoch 00014: val_loss improved from 1.48177 to 1.33414, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.6563 - acc: 0.4117 - val_loss: 1.3341 - val_acc: 0.5678
Epoch 15/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.5555 - acc: 0.4481Epoch 00015: val_loss improved from 1.33414 to 1.21306, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.5536 - acc: 0.4493 - val_loss: 1.2131 - val_acc: 0.6051
Epoch 16/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.4417 - acc: 0.4876Epoch 00016: val_loss improved from 1.21306 to 1.12862, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.4399 - acc: 0.4883 - val_loss: 1.1286 - val_acc: 0.6341
Epoch 17/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.3602 - acc: 0.5219Epoch 00017: val_loss improved from 1.12862 to 0.99138, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 1.3580 - acc: 0.5231 - val_loss: 0.9914 - val_acc: 0.6726
Epoch 18/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.2878 - acc: 0.5532Epoch 00018: val_loss improved from 0.99138 to 0.94162, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 1.2850 - acc: 0.5540 - val_loss: 0.9416 - val_acc: 0.7011
Epoch 19/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.2164 - acc: 0.5761Epoch 00019: val_loss improved from 0.94162 to 0.87367, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 1.2114 - acc: 0.5776 - val_loss: 0.8737 - val_acc: 0.7117
Epoch 20/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.1639 - acc: 0.5948Epoch 00020: val_loss improved from 0.87367 to 0.80686, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 1.1594 - acc: 0.5961 - val_loss: 0.8069 - val_acc: 0.7429
Epoch 21/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.0966 - acc: 0.6159Epoch 00021: val_loss improved from 0.80686 to 0.76039, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.0956 - acc: 0.6162 - val_loss: 0.7604 - val_acc: 0.7434
Epoch 22/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.0368 - acc: 0.6421Epoch 00022: val_loss improved from 0.76039 to 0.72525, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 1.0350 - acc: 0.6436 - val_loss: 0.7252 - val_acc: 0.7674
Epoch 23/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 1.0067 - acc: 0.6539Epoch 00023: val_loss improved from 0.72525 to 0.68614, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 1.0057 - acc: 0.6546 - val_loss: 0.6861 - val_acc: 0.7741
Epoch 24/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.9630 - acc: 0.6680Epoch 00024: val_loss improved from 0.68614 to 0.64137, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.9619 - acc: 0.6676 - val_loss: 0.6414 - val_acc: 0.7842
Epoch 25/250
17408/17887 [============================&gt;.] - ETA: 0s - loss: 0.9185 - acc: 0.6845Epoch 00025: val_loss improved from 0.64137 to 0.61408, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 95us/step - loss: 0.9185 - acc: 0.6844 - val_loss: 0.6141 - val_acc: 0.7970
Epoch 26/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.9060 - acc: 0.6848Epoch 00026: val_loss improved from 0.61408 to 0.60253, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.9041 - acc: 0.6852 - val_loss: 0.6025 - val_acc: 0.8015
Epoch 27/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.8710 - acc: 0.7013Epoch 00027: val_loss improved from 0.60253 to 0.57522, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.8701 - acc: 0.7022 - val_loss: 0.5752 - val_acc: 0.8070
Epoch 28/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.8393 - acc: 0.7087Epoch 00028: val_loss improved from 0.57522 to 0.54433, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.8396 - acc: 0.7094 - val_loss: 0.5443 - val_acc: 0.8271
Epoch 29/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.8287 - acc: 0.7156Epoch 00029: val_loss improved from 0.54433 to 0.52488, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.8253 - acc: 0.7165 - val_loss: 0.5249 - val_acc: 0.8338
Epoch 30/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.8023 - acc: 0.7258Epoch 00030: val_loss improved from 0.52488 to 0.51448, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.7984 - acc: 0.7268 - val_loss: 0.5145 - val_acc: 0.8260
Epoch 31/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.7813 - acc: 0.7341Epoch 00031: val_loss improved from 0.51448 to 0.50691, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.7823 - acc: 0.7330 - val_loss: 0.5069 - val_acc: 0.8355
Epoch 32/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.7523 - acc: 0.7462Epoch 00032: val_loss improved from 0.50691 to 0.48487, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.7508 - acc: 0.7465 - val_loss: 0.4849 - val_acc: 0.8388
Epoch 33/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.7354 - acc: 0.7465Epoch 00033: val_loss improved from 0.48487 to 0.47895, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.7366 - acc: 0.7466 - val_loss: 0.4789 - val_acc: 0.8371
Epoch 34/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.7144 - acc: 0.7581Epoch 00034: val_loss improved from 0.47895 to 0.44765, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.7157 - acc: 0.7574 - val_loss: 0.4477 - val_acc: 0.8561
Epoch 35/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6958 - acc: 0.7643Epoch 00035: val_loss improved from 0.44765 to 0.44085, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.6982 - acc: 0.7636 - val_loss: 0.4409 - val_acc: 0.8661
Epoch 36/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6943 - acc: 0.7632Epoch 00036: val_loss improved from 0.44085 to 0.43895, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.6940 - acc: 0.7641 - val_loss: 0.4389 - val_acc: 0.8673
Epoch 37/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6751 - acc: 0.7674Epoch 00037: val_loss improved from 0.43895 to 0.42727, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.6761 - acc: 0.7669 - val_loss: 0.4273 - val_acc: 0.8639
Epoch 38/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6731 - acc: 0.7726Epoch 00038: val_loss improved from 0.42727 to 0.41718, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.6745 - acc: 0.7717 - val_loss: 0.4172 - val_acc: 0.8678
Epoch 39/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6481 - acc: 0.7795Epoch 00039: val_loss improved from 0.41718 to 0.40259, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.6478 - acc: 0.7793 - val_loss: 0.4026 - val_acc: 0.8740
Epoch 40/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6360 - acc: 0.7823Epoch 00040: val_loss improved from 0.40259 to 0.38856, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.6333 - acc: 0.7831 - val_loss: 0.3886 - val_acc: 0.8818
Epoch 41/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6223 - acc: 0.7895Epoch 00041: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.6222 - acc: 0.7891 - val_loss: 0.3889 - val_acc: 0.8795
Epoch 42/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6112 - acc: 0.7936Epoch 00042: val_loss improved from 0.38856 to 0.37813, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.6092 - acc: 0.7945 - val_loss: 0.3781 - val_acc: 0.8823
Epoch 43/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.6160 - acc: 0.7905Epoch 00043: val_loss improved from 0.37813 to 0.37628, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.6131 - acc: 0.7914 - val_loss: 0.3763 - val_acc: 0.8862
Epoch 44/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5969 - acc: 0.7986Epoch 00044: val_loss improved from 0.37628 to 0.37595, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.5951 - acc: 0.7989 - val_loss: 0.3760 - val_acc: 0.8801
Epoch 45/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5967 - acc: 0.7971Epoch 00045: val_loss improved from 0.37595 to 0.35563, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.5939 - acc: 0.7987 - val_loss: 0.3556 - val_acc: 0.8912
Epoch 46/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5820 - acc: 0.8042Epoch 00046: val_loss improved from 0.35563 to 0.34705, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.5835 - acc: 0.8042 - val_loss: 0.3470 - val_acc: 0.8951
Epoch 47/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5616 - acc: 0.8112Epoch 00047: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.5612 - acc: 0.8113 - val_loss: 0.3476 - val_acc: 0.8907
Epoch 48/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5575 - acc: 0.8116Epoch 00048: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.5568 - acc: 0.8116 - val_loss: 0.3497 - val_acc: 0.9007
Epoch 49/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5564 - acc: 0.8134Epoch 00049: val_loss improved from 0.34705 to 0.33701, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.5554 - acc: 0.8138 - val_loss: 0.3370 - val_acc: 0.9041
Epoch 50/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5475 - acc: 0.8127Epoch 00050: val_loss improved from 0.33701 to 0.33527, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.5469 - acc: 0.8132 - val_loss: 0.3353 - val_acc: 0.9013
Epoch 51/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5300 - acc: 0.8243Epoch 00051: val_loss improved from 0.33527 to 0.33058, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.5314 - acc: 0.8235 - val_loss: 0.3306 - val_acc: 0.8974
Epoch 52/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5320 - acc: 0.8240Epoch 00052: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.5346 - acc: 0.8229 - val_loss: 0.3328 - val_acc: 0.9013
Epoch 53/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5296 - acc: 0.8231Epoch 00053: val_loss improved from 0.33058 to 0.31631, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.5283 - acc: 0.8229 - val_loss: 0.3163 - val_acc: 0.9074
Epoch 54/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5138 - acc: 0.8278Epoch 00054: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.5168 - acc: 0.8272 - val_loss: 0.3218 - val_acc: 0.9080
Epoch 55/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5102 - acc: 0.8267Epoch 00055: val_loss improved from 0.31631 to 0.31173, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.5086 - acc: 0.8271 - val_loss: 0.3117 - val_acc: 0.9085
Epoch 56/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4997 - acc: 0.8335Epoch 00056: val_loss improved from 0.31173 to 0.31108, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.5020 - acc: 0.8330 - val_loss: 0.3111 - val_acc: 0.9074
Epoch 57/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4954 - acc: 0.8350Epoch 00057: val_loss improved from 0.31108 to 0.30207, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4966 - acc: 0.8343 - val_loss: 0.3021 - val_acc: 0.9108
Epoch 58/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.5017 - acc: 0.8290Epoch 00058: val_loss improved from 0.30207 to 0.29987, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.5016 - acc: 0.8300 - val_loss: 0.2999 - val_acc: 0.9108
Epoch 59/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4826 - acc: 0.8362Epoch 00059: val_loss improved from 0.29987 to 0.29415, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.4841 - acc: 0.8356 - val_loss: 0.2941 - val_acc: 0.9119
Epoch 60/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4825 - acc: 0.8356Epoch 00060: val_loss improved from 0.29415 to 0.28865, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.4845 - acc: 0.8348 - val_loss: 0.2887 - val_acc: 0.9152
Epoch 61/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4746 - acc: 0.8413Epoch 00061: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.4767 - acc: 0.8409 - val_loss: 0.2915 - val_acc: 0.9147
Epoch 62/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4794 - acc: 0.8435Epoch 00062: val_loss improved from 0.28865 to 0.28854, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4769 - acc: 0.8442 - val_loss: 0.2885 - val_acc: 0.9175
Epoch 63/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4610 - acc: 0.8452Epoch 00063: val_loss improved from 0.28854 to 0.28494, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.4647 - acc: 0.8429 - val_loss: 0.2849 - val_acc: 0.9136
Epoch 64/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4619 - acc: 0.8440Epoch 00064: val_loss improved from 0.28494 to 0.28102, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4625 - acc: 0.8437 - val_loss: 0.2810 - val_acc: 0.9169
Epoch 65/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4594 - acc: 0.8456Epoch 00065: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.4608 - acc: 0.8456 - val_loss: 0.2903 - val_acc: 0.9113
Epoch 66/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4518 - acc: 0.8496Epoch 00066: val_loss improved from 0.28102 to 0.27291, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.4508 - acc: 0.8496 - val_loss: 0.2729 - val_acc: 0.9163
Epoch 67/250
17664/17887 [============================&gt;.] - ETA: 0s - loss: 0.4460 - acc: 0.8503Epoch 00067: val_loss improved from 0.27291 to 0.27153, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4460 - acc: 0.8503 - val_loss: 0.2715 - val_acc: 0.9136
Epoch 68/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4408 - acc: 0.8504Epoch 00068: val_loss improved from 0.27153 to 0.27106, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4415 - acc: 0.8502 - val_loss: 0.2711 - val_acc: 0.9136
Epoch 69/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4414 - acc: 0.8484Epoch 00069: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.4432 - acc: 0.8477 - val_loss: 0.2715 - val_acc: 0.9158
Epoch 70/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4314 - acc: 0.8520Epoch 00070: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.4344 - acc: 0.8505 - val_loss: 0.2755 - val_acc: 0.9136
Epoch 71/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4204 - acc: 0.8584Epoch 00071: val_loss improved from 0.27106 to 0.26558, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4255 - acc: 0.8568 - val_loss: 0.2656 - val_acc: 0.9180
Epoch 72/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4278 - acc: 0.8575Epoch 00072: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.4262 - acc: 0.8584 - val_loss: 0.2725 - val_acc: 0.9130
Epoch 73/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4233 - acc: 0.8579Epoch 00073: val_loss improved from 0.26558 to 0.26351, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4241 - acc: 0.8579 - val_loss: 0.2635 - val_acc: 0.9175
Epoch 74/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4191 - acc: 0.8562Epoch 00074: val_loss improved from 0.26351 to 0.25987, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.4200 - acc: 0.8560 - val_loss: 0.2599 - val_acc: 0.9191
Epoch 75/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4105 - acc: 0.8618Epoch 00075: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.4123 - acc: 0.8611 - val_loss: 0.2629 - val_acc: 0.9214
Epoch 76/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4158 - acc: 0.8588Epoch 00076: val_loss improved from 0.25987 to 0.25826, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4164 - acc: 0.8583 - val_loss: 0.2583 - val_acc: 0.9219
Epoch 77/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4234 - acc: 0.8561Epoch 00077: val_loss improved from 0.25826 to 0.25797, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.4198 - acc: 0.8576 - val_loss: 0.2580 - val_acc: 0.9225
Epoch 78/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4074 - acc: 0.8622Epoch 00078: val_loss improved from 0.25797 to 0.25422, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.4062 - acc: 0.8624 - val_loss: 0.2542 - val_acc: 0.9241
Epoch 79/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3995 - acc: 0.8668Epoch 00079: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.4000 - acc: 0.8662 - val_loss: 0.2625 - val_acc: 0.9219
Epoch 80/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3983 - acc: 0.8631Epoch 00080: val_loss improved from 0.25422 to 0.24567, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3978 - acc: 0.8635 - val_loss: 0.2457 - val_acc: 0.9258
Epoch 81/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.4046 - acc: 0.8630Epoch 00081: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.4014 - acc: 0.8644 - val_loss: 0.2472 - val_acc: 0.9269
Epoch 82/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3997 - acc: 0.8654Epoch 00082: val_loss improved from 0.24567 to 0.24427, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.3990 - acc: 0.8658 - val_loss: 0.2443 - val_acc: 0.9236
Epoch 83/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3852 - acc: 0.8696Epoch 00083: val_loss improved from 0.24427 to 0.23996, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.3848 - acc: 0.8692 - val_loss: 0.2400 - val_acc: 0.9275
Epoch 84/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3907 - acc: 0.8678Epoch 00084: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3908 - acc: 0.8676 - val_loss: 0.2459 - val_acc: 0.9264
Epoch 85/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3888 - acc: 0.8694Epoch 00085: val_loss improved from 0.23996 to 0.23979, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3912 - acc: 0.8682 - val_loss: 0.2398 - val_acc: 0.9275
Epoch 86/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3857 - acc: 0.8708Epoch 00086: val_loss improved from 0.23979 to 0.23881, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 95us/step - loss: 0.3848 - acc: 0.8718 - val_loss: 0.2388 - val_acc: 0.9264
Epoch 87/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3814 - acc: 0.8706Epoch 00087: val_loss improved from 0.23881 to 0.23765, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3805 - acc: 0.8706 - val_loss: 0.2376 - val_acc: 0.9269
Epoch 88/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3730 - acc: 0.8741Epoch 00088: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3771 - acc: 0.8729 - val_loss: 0.2427 - val_acc: 0.9269
Epoch 89/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3769 - acc: 0.8728Epoch 00089: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3756 - acc: 0.8730 - val_loss: 0.2412 - val_acc: 0.9264
Epoch 90/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3749 - acc: 0.8712Epoch 00090: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3767 - acc: 0.8705 - val_loss: 0.2446 - val_acc: 0.9241
Epoch 91/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3725 - acc: 0.8739Epoch 00091: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3744 - acc: 0.8742 - val_loss: 0.2397 - val_acc: 0.9286
Epoch 92/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3635 - acc: 0.8769Epoch 00092: val_loss improved from 0.23765 to 0.23676, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3646 - acc: 0.8765 - val_loss: 0.2368 - val_acc: 0.9320
Epoch 93/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3656 - acc: 0.8739Epoch 00093: val_loss improved from 0.23676 to 0.23257, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3660 - acc: 0.8737 - val_loss: 0.2326 - val_acc: 0.9314
Epoch 94/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3640 - acc: 0.8777Epoch 00094: val_loss did not improve
17887/17887 [==============================] - 2s 90us/step - loss: 0.3652 - acc: 0.8768 - val_loss: 0.2336 - val_acc: 0.9303
Epoch 95/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3611 - acc: 0.8800Epoch 00095: val_loss did not improve
17887/17887 [==============================] - 2s 90us/step - loss: 0.3622 - acc: 0.8795 - val_loss: 0.2334 - val_acc: 0.9303
Epoch 96/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3515 - acc: 0.8800Epoch 00096: val_loss improved from 0.23257 to 0.22845, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3510 - acc: 0.8806 - val_loss: 0.2285 - val_acc: 0.9314
Epoch 97/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3568 - acc: 0.8785Epoch 00097: val_loss improved from 0.22845 to 0.22428, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3568 - acc: 0.8790 - val_loss: 0.2243 - val_acc: 0.9303
Epoch 98/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3434 - acc: 0.8824Epoch 00098: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3465 - acc: 0.8814 - val_loss: 0.2323 - val_acc: 0.9303
Epoch 99/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3428 - acc: 0.8858Epoch 00099: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3432 - acc: 0.8853 - val_loss: 0.2256 - val_acc: 0.9314
Epoch 100/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3566 - acc: 0.8794Epoch 00100: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3553 - acc: 0.8795 - val_loss: 0.2307 - val_acc: 0.9286
Epoch 101/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3363 - acc: 0.8856Epoch 00101: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3378 - acc: 0.8848 - val_loss: 0.2249 - val_acc: 0.9303
Epoch 102/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3434 - acc: 0.8831Epoch 00102: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3438 - acc: 0.8830 - val_loss: 0.2267 - val_acc: 0.9353
Epoch 103/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3491 - acc: 0.8801Epoch 00103: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3487 - acc: 0.8799 - val_loss: 0.2293 - val_acc: 0.9308
Epoch 104/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3443 - acc: 0.8848Epoch 00104: val_loss improved from 0.22428 to 0.22346, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3446 - acc: 0.8846 - val_loss: 0.2235 - val_acc: 0.9303
Epoch 105/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3356 - acc: 0.8860Epoch 00105: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3369 - acc: 0.8853 - val_loss: 0.2256 - val_acc: 0.9314
Epoch 106/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3332 - acc: 0.8882Epoch 00106: val_loss improved from 0.22346 to 0.22189, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3292 - acc: 0.8895 - val_loss: 0.2219 - val_acc: 0.9347
Epoch 107/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3322 - acc: 0.8864Epoch 00107: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3301 - acc: 0.8866 - val_loss: 0.2289 - val_acc: 0.9320
Epoch 108/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3289 - acc: 0.8881Epoch 00108: val_loss improved from 0.22189 to 0.22031, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3300 - acc: 0.8876 - val_loss: 0.2203 - val_acc: 0.9375
Epoch 109/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3335 - acc: 0.8860Epoch 00109: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3346 - acc: 0.8859 - val_loss: 0.2278 - val_acc: 0.9297
Epoch 110/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3331 - acc: 0.8868Epoch 00110: val_loss improved from 0.22031 to 0.21781, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.3342 - acc: 0.8861 - val_loss: 0.2178 - val_acc: 0.9375
Epoch 111/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3366 - acc: 0.8870Epoch 00111: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3372 - acc: 0.8867 - val_loss: 0.2271 - val_acc: 0.9370
Epoch 112/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3201 - acc: 0.8920Epoch 00112: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3182 - acc: 0.8929 - val_loss: 0.2213 - val_acc: 0.9359
Epoch 113/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3266 - acc: 0.8902Epoch 00113: val_loss improved from 0.21781 to 0.21641, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3257 - acc: 0.8905 - val_loss: 0.2164 - val_acc: 0.9347
Epoch 114/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3178 - acc: 0.8910Epoch 00114: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3179 - acc: 0.8910 - val_loss: 0.2231 - val_acc: 0.9336
Epoch 115/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3239 - acc: 0.8899Epoch 00115: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3234 - acc: 0.8902 - val_loss: 0.2200 - val_acc: 0.9331
Epoch 116/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3175 - acc: 0.8907Epoch 00116: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3174 - acc: 0.8905 - val_loss: 0.2176 - val_acc: 0.9331
Epoch 117/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3126 - acc: 0.8936Epoch 00117: val_loss improved from 0.21641 to 0.21459, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.3134 - acc: 0.8939 - val_loss: 0.2146 - val_acc: 0.9375
Epoch 118/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3186 - acc: 0.8912Epoch 00118: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3174 - acc: 0.8910 - val_loss: 0.2177 - val_acc: 0.9375
Epoch 119/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3096 - acc: 0.8941Epoch 00119: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3113 - acc: 0.8937 - val_loss: 0.2194 - val_acc: 0.9347
Epoch 120/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3119 - acc: 0.8953Epoch 00120: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3133 - acc: 0.8952 - val_loss: 0.2250 - val_acc: 0.9320
Epoch 121/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3051 - acc: 0.8953Epoch 00121: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3066 - acc: 0.8947 - val_loss: 0.2200 - val_acc: 0.9375
Epoch 122/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3071 - acc: 0.8980Epoch 00122: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3091 - acc: 0.8972 - val_loss: 0.2185 - val_acc: 0.9331
Epoch 123/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3086 - acc: 0.8952Epoch 00123: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3094 - acc: 0.8953 - val_loss: 0.2174 - val_acc: 0.9375
Epoch 124/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3057 - acc: 0.8961Epoch 00124: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3057 - acc: 0.8958 - val_loss: 0.2228 - val_acc: 0.9331
Epoch 125/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3069 - acc: 0.8967Epoch 00125: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.3080 - acc: 0.8963 - val_loss: 0.2146 - val_acc: 0.9347
Epoch 126/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3012 - acc: 0.8983Epoch 00126: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3023 - acc: 0.8978 - val_loss: 0.2175 - val_acc: 0.9353
Epoch 127/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2995 - acc: 0.8974Epoch 00127: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2999 - acc: 0.8970 - val_loss: 0.2151 - val_acc: 0.9353
Epoch 128/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2997 - acc: 0.8966Epoch 00128: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2987 - acc: 0.8970 - val_loss: 0.2182 - val_acc: 0.9375
Epoch 129/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2968 - acc: 0.8991Epoch 00129: val_loss improved from 0.21459 to 0.21357, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.2990 - acc: 0.8986 - val_loss: 0.2136 - val_acc: 0.9375
Epoch 130/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2998 - acc: 0.8998Epoch 00130: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2989 - acc: 0.8996 - val_loss: 0.2139 - val_acc: 0.9364
Epoch 131/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2970 - acc: 0.8980Epoch 00131: val_loss improved from 0.21357 to 0.21219, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.2951 - acc: 0.8988 - val_loss: 0.2122 - val_acc: 0.9403
Epoch 132/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2908 - acc: 0.9003Epoch 00132: val_loss improved from 0.21219 to 0.21150, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.2909 - acc: 0.9000 - val_loss: 0.2115 - val_acc: 0.9387
Epoch 133/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2922 - acc: 0.8988Epoch 00133: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2920 - acc: 0.8990 - val_loss: 0.2190 - val_acc: 0.9353
Epoch 134/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2969 - acc: 0.8991Epoch 00134: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2965 - acc: 0.8992 - val_loss: 0.2117 - val_acc: 0.9375
Epoch 135/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.3006 - acc: 0.8982Epoch 00135: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.3008 - acc: 0.8982 - val_loss: 0.2202 - val_acc: 0.9353
Epoch 136/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2888 - acc: 0.9008Epoch 00136: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2873 - acc: 0.9012 - val_loss: 0.2186 - val_acc: 0.9347
Epoch 137/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2864 - acc: 0.9061Epoch 00137: val_loss improved from 0.21150 to 0.21121, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.2882 - acc: 0.9048 - val_loss: 0.2112 - val_acc: 0.9364
Epoch 138/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2912 - acc: 0.9029Epoch 00138: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2899 - acc: 0.9032 - val_loss: 0.2140 - val_acc: 0.9403
Epoch 139/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2798 - acc: 0.9049Epoch 00139: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2797 - acc: 0.9048 - val_loss: 0.2148 - val_acc: 0.9375
Epoch 140/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2850 - acc: 0.9014Epoch 00140: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.2849 - acc: 0.9017 - val_loss: 0.2141 - val_acc: 0.9370
Epoch 141/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2914 - acc: 0.9018Epoch 00141: val_loss improved from 0.21121 to 0.20785, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.2901 - acc: 0.9022 - val_loss: 0.2078 - val_acc: 0.9392
Epoch 142/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2855 - acc: 0.9020Epoch 00142: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2847 - acc: 0.9019 - val_loss: 0.2217 - val_acc: 0.9325
Epoch 143/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2809 - acc: 0.9031Epoch 00143: val_loss did not improve
17887/17887 [==============================] - 2s 90us/step - loss: 0.2811 - acc: 0.9027 - val_loss: 0.2082 - val_acc: 0.9398
Epoch 144/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2814 - acc: 0.9036Epoch 00144: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2806 - acc: 0.9038 - val_loss: 0.2134 - val_acc: 0.9381
Epoch 145/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2782 - acc: 0.9030Epoch 00145: val_loss improved from 0.20785 to 0.20754, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.2785 - acc: 0.9028 - val_loss: 0.2075 - val_acc: 0.9392
Epoch 146/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2811 - acc: 0.9014Epoch 00146: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2814 - acc: 0.9016 - val_loss: 0.2201 - val_acc: 0.9353
Epoch 147/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2783 - acc: 0.9049Epoch 00147: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2789 - acc: 0.9045 - val_loss: 0.2101 - val_acc: 0.9387
Epoch 148/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2786 - acc: 0.9056Epoch 00148: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2783 - acc: 0.9057 - val_loss: 0.2124 - val_acc: 0.9359
Epoch 149/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2828 - acc: 0.9042Epoch 00149: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2839 - acc: 0.9038 - val_loss: 0.2125 - val_acc: 0.9364
Epoch 150/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2702 - acc: 0.9081Epoch 00150: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2694 - acc: 0.9084 - val_loss: 0.2163 - val_acc: 0.9359
Epoch 151/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2765 - acc: 0.9053Epoch 00151: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2785 - acc: 0.9047 - val_loss: 0.2149 - val_acc: 0.9381
Epoch 152/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2747 - acc: 0.9060Epoch 00152: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2748 - acc: 0.9060 - val_loss: 0.2094 - val_acc: 0.9387
Epoch 153/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2737 - acc: 0.9054Epoch 00153: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2729 - acc: 0.9060 - val_loss: 0.2137 - val_acc: 0.9347
Epoch 154/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2636 - acc: 0.9103Epoch 00154: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2649 - acc: 0.9097 - val_loss: 0.2087 - val_acc: 0.9387
Epoch 155/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2673 - acc: 0.9069Epoch 00155: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2685 - acc: 0.9065 - val_loss: 0.2148 - val_acc: 0.9353
Epoch 156/250
17664/17887 [============================&gt;.] - ETA: 0s - loss: 0.2651 - acc: 0.9092Epoch 00156: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2657 - acc: 0.9090 - val_loss: 0.2146 - val_acc: 0.9387
Epoch 157/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2614 - acc: 0.9109Epoch 00157: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2621 - acc: 0.9104 - val_loss: 0.2137 - val_acc: 0.9381
Epoch 158/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2661 - acc: 0.9086Epoch 00158: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2670 - acc: 0.9088 - val_loss: 0.2165 - val_acc: 0.9353
Epoch 159/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2617 - acc: 0.9119Epoch 00159: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2608 - acc: 0.9119 - val_loss: 0.2144 - val_acc: 0.9387
Epoch 160/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2635 - acc: 0.9110Epoch 00160: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2640 - acc: 0.9106 - val_loss: 0.2165 - val_acc: 0.9364
Epoch 161/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2620 - acc: 0.9115Epoch 00161: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2621 - acc: 0.9119 - val_loss: 0.2159 - val_acc: 0.9347
Epoch 162/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2621 - acc: 0.9099Epoch 00162: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2618 - acc: 0.9102 - val_loss: 0.2158 - val_acc: 0.9370
Epoch 163/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2622 - acc: 0.9099Epoch 00163: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2614 - acc: 0.9101 - val_loss: 0.2136 - val_acc: 0.9392
Epoch 164/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2640 - acc: 0.9103Epoch 00164: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2648 - acc: 0.9095 - val_loss: 0.2155 - val_acc: 0.9331
Epoch 165/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2617 - acc: 0.9107Epoch 00165: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2603 - acc: 0.9112 - val_loss: 0.2171 - val_acc: 0.9409
Epoch 166/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2588 - acc: 0.9107Epoch 00166: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2591 - acc: 0.9107 - val_loss: 0.2154 - val_acc: 0.9403
Epoch 167/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2588 - acc: 0.9100Epoch 00167: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2582 - acc: 0.9098 - val_loss: 0.2098 - val_acc: 0.9403
Epoch 168/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2543 - acc: 0.9133Epoch 00168: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2541 - acc: 0.9131 - val_loss: 0.2113 - val_acc: 0.9375
Epoch 169/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2560 - acc: 0.9111Epoch 00169: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2566 - acc: 0.9111 - val_loss: 0.2151 - val_acc: 0.9364
Epoch 170/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2585 - acc: 0.9141Epoch 00170: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2587 - acc: 0.9142 - val_loss: 0.2174 - val_acc: 0.9398
Epoch 171/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2508 - acc: 0.9132Epoch 00171: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2491 - acc: 0.9135 - val_loss: 0.2158 - val_acc: 0.9387
Epoch 172/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2463 - acc: 0.9152Epoch 00172: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2466 - acc: 0.9152 - val_loss: 0.2158 - val_acc: 0.9370
Epoch 173/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2508 - acc: 0.9144Epoch 00173: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2515 - acc: 0.9142 - val_loss: 0.2189 - val_acc: 0.9359
Epoch 174/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2477 - acc: 0.9175Epoch 00174: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2472 - acc: 0.9176 - val_loss: 0.2158 - val_acc: 0.9381
Epoch 175/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2558 - acc: 0.9136Epoch 00175: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2574 - acc: 0.9131 - val_loss: 0.2101 - val_acc: 0.9370
Epoch 176/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2522 - acc: 0.9146Epoch 00176: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2515 - acc: 0.9150 - val_loss: 0.2141 - val_acc: 0.9381
Epoch 177/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2463 - acc: 0.9156Epoch 00177: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2466 - acc: 0.9156 - val_loss: 0.2166 - val_acc: 0.9370
Epoch 178/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2473 - acc: 0.9146Epoch 00178: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2476 - acc: 0.9144 - val_loss: 0.2121 - val_acc: 0.9353
Epoch 179/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2436 - acc: 0.9188Epoch 00179: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2441 - acc: 0.9184 - val_loss: 0.2116 - val_acc: 0.9375
Epoch 180/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2513 - acc: 0.9135Epoch 00180: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2503 - acc: 0.9141 - val_loss: 0.2102 - val_acc: 0.9403
Epoch 181/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2397 - acc: 0.9162Epoch 00181: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2417 - acc: 0.9159 - val_loss: 0.2135 - val_acc: 0.9359
Epoch 182/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2453 - acc: 0.9148Epoch 00182: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2454 - acc: 0.9144 - val_loss: 0.2190 - val_acc: 0.9347
Epoch 183/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2517 - acc: 0.9137Epoch 00183: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2530 - acc: 0.9136 - val_loss: 0.2105 - val_acc: 0.9375
Epoch 184/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2430 - acc: 0.9167Epoch 00184: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2430 - acc: 0.9169 - val_loss: 0.2103 - val_acc: 0.9364
Epoch 185/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2502 - acc: 0.9147Epoch 00185: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2504 - acc: 0.9142 - val_loss: 0.2125 - val_acc: 0.9364
Epoch 186/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2407 - acc: 0.9162Epoch 00186: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2425 - acc: 0.9155 - val_loss: 0.2094 - val_acc: 0.9331
Epoch 187/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2444 - acc: 0.9178Epoch 00187: val_loss improved from 0.20754 to 0.20585, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 93us/step - loss: 0.2437 - acc: 0.9180 - val_loss: 0.2059 - val_acc: 0.9381
Epoch 188/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2417 - acc: 0.9166Epoch 00188: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2419 - acc: 0.9164 - val_loss: 0.2114 - val_acc: 0.9359
Epoch 189/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2396 - acc: 0.9169Epoch 00189: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2402 - acc: 0.9171 - val_loss: 0.2178 - val_acc: 0.9347
Epoch 190/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2445 - acc: 0.9164Epoch 00190: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2448 - acc: 0.9161 - val_loss: 0.2116 - val_acc: 0.9370
Epoch 191/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2396 - acc: 0.9171Epoch 00191: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2402 - acc: 0.9171 - val_loss: 0.2100 - val_acc: 0.9392
Epoch 192/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2377 - acc: 0.9167Epoch 00192: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2386 - acc: 0.9166 - val_loss: 0.2064 - val_acc: 0.9387
Epoch 193/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2431 - acc: 0.9176Epoch 00193: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2432 - acc: 0.9181 - val_loss: 0.2152 - val_acc: 0.9347
Epoch 194/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2431 - acc: 0.9162Epoch 00194: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2438 - acc: 0.9156 - val_loss: 0.2108 - val_acc: 0.9364
Epoch 195/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2359 - acc: 0.9175Epoch 00195: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.2378 - acc: 0.9172 - val_loss: 0.2092 - val_acc: 0.9359
Epoch 196/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2408 - acc: 0.9171Epoch 00196: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2407 - acc: 0.9172 - val_loss: 0.2079 - val_acc: 0.9375
Epoch 197/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2347 - acc: 0.9214Epoch 00197: val_loss improved from 0.20585 to 0.20564, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 94us/step - loss: 0.2330 - acc: 0.9219 - val_loss: 0.2056 - val_acc: 0.9364
Epoch 198/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2290 - acc: 0.9218Epoch 00198: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2281 - acc: 0.9221 - val_loss: 0.2082 - val_acc: 0.9392
Epoch 199/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2308 - acc: 0.9204Epoch 00199: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2324 - acc: 0.9198 - val_loss: 0.2143 - val_acc: 0.9364
Epoch 200/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2346 - acc: 0.9193Epoch 00200: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2336 - acc: 0.9193 - val_loss: 0.2089 - val_acc: 0.9381
Epoch 201/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2299 - acc: 0.9215Epoch 00201: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2288 - acc: 0.9217 - val_loss: 0.2089 - val_acc: 0.9392
Epoch 202/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2337 - acc: 0.9183- ETA: 1s - loss: 0.24Epoch 00202: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2335 - acc: 0.9184 - val_loss: 0.2089 - val_acc: 0.9359
Epoch 203/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2343 - acc: 0.9195Epoch 00203: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2345 - acc: 0.9194 - val_loss: 0.2176 - val_acc: 0.9370
Epoch 204/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2253 - acc: 0.9220Epoch 00204: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2264 - acc: 0.9214 - val_loss: 0.2121 - val_acc: 0.9359
Epoch 205/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2277 - acc: 0.9209Epoch 00205: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2264 - acc: 0.9216 - val_loss: 0.2149 - val_acc: 0.9370
Epoch 206/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2374 - acc: 0.9188Epoch 00206: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2353 - acc: 0.9195 - val_loss: 0.2168 - val_acc: 0.9353
Epoch 207/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2273 - acc: 0.9212Epoch 00207: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2289 - acc: 0.9209 - val_loss: 0.2062 - val_acc: 0.9347
Epoch 208/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2305 - acc: 0.9179Epoch 00208: val_loss did not improve
17887/17887 [==============================] - 2s 90us/step - loss: 0.2299 - acc: 0.9180 - val_loss: 0.2090 - val_acc: 0.9387
Epoch 209/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2265 - acc: 0.9225Epoch 00209: val_loss did not improve
17887/17887 [==============================] - 2s 90us/step - loss: 0.2261 - acc: 0.9225 - val_loss: 0.2062 - val_acc: 0.9398
Epoch 210/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2211 - acc: 0.9237Epoch 00210: val_loss did not improve
17887/17887 [==============================] - 2s 90us/step - loss: 0.2215 - acc: 0.9234 - val_loss: 0.2155 - val_acc: 0.9359
Epoch 211/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2248 - acc: 0.9236Epoch 00211: val_loss did not improve
17887/17887 [==============================] - 2s 90us/step - loss: 0.2250 - acc: 0.9236 - val_loss: 0.2141 - val_acc: 0.9387
Epoch 212/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2225 - acc: 0.9257Epoch 00212: val_loss improved from 0.20564 to 0.20512, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 92us/step - loss: 0.2240 - acc: 0.9256 - val_loss: 0.2051 - val_acc: 0.9398
Epoch 213/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2191 - acc: 0.9243Epoch 00213: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2193 - acc: 0.9245 - val_loss: 0.2120 - val_acc: 0.9364
Epoch 214/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2221 - acc: 0.9236Epoch 00214: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2200 - acc: 0.9244 - val_loss: 0.2157 - val_acc: 0.9353
Epoch 215/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2319 - acc: 0.9219Epoch 00215: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2331 - acc: 0.9215 - val_loss: 0.2055 - val_acc: 0.9381
Epoch 216/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2177 - acc: 0.9274Epoch 00216: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2201 - acc: 0.9265 - val_loss: 0.2190 - val_acc: 0.9364
Epoch 217/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2227 - acc: 0.9238Epoch 00217: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.2222 - acc: 0.9236 - val_loss: 0.2100 - val_acc: 0.9387
Epoch 218/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2253 - acc: 0.9237Epoch 00218: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.2250 - acc: 0.9235 - val_loss: 0.2076 - val_acc: 0.9381
Epoch 219/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2258 - acc: 0.9226Epoch 00219: val_loss did not improve
17887/17887 [==============================] - 2s 94us/step - loss: 0.2248 - acc: 0.9228 - val_loss: 0.2075 - val_acc: 0.9370
Epoch 220/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2198 - acc: 0.9258Epoch 00220: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.2212 - acc: 0.9255 - val_loss: 0.2128 - val_acc: 0.9347
Epoch 221/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2189 - acc: 0.9241Epoch 00221: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.2192 - acc: 0.9241 - val_loss: 0.2080 - val_acc: 0.9409
Epoch 222/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2233 - acc: 0.9233Epoch 00222: val_loss improved from 0.20512 to 0.20268, saving model to weights4.hdf5
17887/17887 [==============================] - 2s 95us/step - loss: 0.2239 - acc: 0.9231 - val_loss: 0.2027 - val_acc: 0.9403
Epoch 223/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2217 - acc: 0.9255Epoch 00223: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2216 - acc: 0.9256 - val_loss: 0.2136 - val_acc: 0.9370
Epoch 224/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2165 - acc: 0.9257Epoch 00224: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2168 - acc: 0.9258 - val_loss: 0.2145 - val_acc: 0.9375
Epoch 225/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2094 - acc: 0.929 - ETA: 0s - loss: 0.2104 - acc: 0.9288Epoch 00225: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2118 - acc: 0.9280 - val_loss: 0.2152 - val_acc: 0.9392
Epoch 226/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2111 - acc: 0.9276Epoch 00226: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2117 - acc: 0.9279 - val_loss: 0.2154 - val_acc: 0.9375
Epoch 227/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2173 - acc: 0.9241Epoch 00227: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2165 - acc: 0.9247 - val_loss: 0.2231 - val_acc: 0.9342
Epoch 228/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2187 - acc: 0.9232Epoch 00228: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2173 - acc: 0.9234 - val_loss: 0.2134 - val_acc: 0.9398
Epoch 229/250
17408/17887 [============================&gt;.] - ETA: 0s - loss: 0.2171 - acc: 0.9238Epoch 00229: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.2163 - acc: 0.9244 - val_loss: 0.2091 - val_acc: 0.9409
Epoch 230/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2136 - acc: 0.9284Epoch 00230: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2141 - acc: 0.9282 - val_loss: 0.2132 - val_acc: 0.9398
Epoch 231/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2167 - acc: 0.9279Epoch 00231: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2185 - acc: 0.9275 - val_loss: 0.2145 - val_acc: 0.9375
Epoch 232/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2193 - acc: 0.9264Epoch 00232: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2185 - acc: 0.9264 - val_loss: 0.2137 - val_acc: 0.9370
Epoch 233/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2147 - acc: 0.9269Epoch 00233: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2148 - acc: 0.9269 - val_loss: 0.2165 - val_acc: 0.9370
Epoch 234/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2123 - acc: 0.9254Epoch 00234: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2129 - acc: 0.9255 - val_loss: 0.2082 - val_acc: 0.9381
Epoch 235/250
17664/17887 [============================&gt;.] - ETA: 0s - loss: 0.2113 - acc: 0.9266Epoch 00235: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2104 - acc: 0.9268 - val_loss: 0.2150 - val_acc: 0.9364
Epoch 236/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2167 - acc: 0.9270Epoch 00236: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2174 - acc: 0.9269 - val_loss: 0.2090 - val_acc: 0.9381
Epoch 237/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2194 - acc: 0.9267Epoch 00237: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2178 - acc: 0.9272 - val_loss: 0.2093 - val_acc: 0.9381
Epoch 238/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2114 - acc: 0.9276Epoch 00238: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2112 - acc: 0.9278 - val_loss: 0.2104 - val_acc: 0.9398
Epoch 239/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2099 - acc: 0.9274Epoch 00239: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.2101 - acc: 0.9274 - val_loss: 0.2092 - val_acc: 0.9387
Epoch 240/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2090 - acc: 0.9278Epoch 00240: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2082 - acc: 0.9280 - val_loss: 0.2055 - val_acc: 0.9420
Epoch 241/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2075 - acc: 0.9282Epoch 00241: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2077 - acc: 0.9282 - val_loss: 0.2057 - val_acc: 0.9381
Epoch 242/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2095 - acc: 0.9288Epoch 00242: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2087 - acc: 0.9289 - val_loss: 0.2074 - val_acc: 0.9381
Epoch 243/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2067 - acc: 0.9287Epoch 00243: val_loss did not improve
17887/17887 [==============================] - 2s 93us/step - loss: 0.2068 - acc: 0.9283 - val_loss: 0.2173 - val_acc: 0.9381
Epoch 244/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2139 - acc: 0.9257Epoch 00244: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2134 - acc: 0.9255 - val_loss: 0.2172 - val_acc: 0.9398
Epoch 245/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2050 - acc: 0.9271Epoch 00245: val_loss did not improve
17887/17887 [==============================] - 2s 92us/step - loss: 0.2053 - acc: 0.9271 - val_loss: 0.2135 - val_acc: 0.9370
Epoch 246/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2093 - acc: 0.9268Epoch 00246: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2084 - acc: 0.9271 - val_loss: 0.2168 - val_acc: 0.9398
Epoch 247/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2111 - acc: 0.9280Epoch 00247: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2118 - acc: 0.9279 - val_loss: 0.2198 - val_acc: 0.9325
Epoch 248/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2047 - acc: 0.9300Epoch 00248: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2044 - acc: 0.9302 - val_loss: 0.2078 - val_acc: 0.9375
Epoch 249/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2131 - acc: 0.9263- ETA: 1s - loss: 0.215Epoch 00249: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2143 - acc: 0.9254 - val_loss: 0.2147 - val_acc: 0.9347
Epoch 250/250
17152/17887 [===========================&gt;..] - ETA: 0s - loss: 0.2112 - acc: 0.9276Epoch 00250: val_loss did not improve
17887/17887 [==============================] - 2s 91us/step - loss: 0.2104 - acc: 0.9280 - val_loss: 0.2092 - val_acc: 0.9387
5


 trianing 



Train on 17890 samples, validate on 1790 samples
Epoch 1/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 6.6418 - acc: 0.0978Epoch 00001: val_loss improved from inf to 2.44709, saving model to weights5.hdf5
17890/17890 [==============================] - 3s 148us/step - loss: 6.4719 - acc: 0.0977 - val_loss: 2.4471 - val_acc: 0.1089
Epoch 2/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4707 - acc: 0.1086Epoch 00002: val_loss improved from 2.44709 to 2.42482, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4703 - acc: 0.1083 - val_loss: 2.4248 - val_acc: 0.1095
Epoch 3/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4426 - acc: 0.1062Epoch 00003: val_loss improved from 2.42482 to 2.42177, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4425 - acc: 0.1064 - val_loss: 2.4218 - val_acc: 0.1089
Epoch 4/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4361 - acc: 0.1058Epoch 00004: val_loss improved from 2.42177 to 2.42062, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4360 - acc: 0.1057 - val_loss: 2.4206 - val_acc: 0.1117
Epoch 5/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4315 - acc: 0.1082Epoch 00005: val_loss improved from 2.42062 to 2.41977, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 94us/step - loss: 2.4322 - acc: 0.1079 - val_loss: 2.4198 - val_acc: 0.1117
Epoch 6/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4295 - acc: 0.1090Epoch 00006: val_loss improved from 2.41977 to 2.41901, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4299 - acc: 0.1082 - val_loss: 2.4190 - val_acc: 0.1117
Epoch 7/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4293 - acc: 0.1088Epoch 00007: val_loss improved from 2.41901 to 2.41837, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4288 - acc: 0.1088 - val_loss: 2.4184 - val_acc: 0.1117
Epoch 8/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4288 - acc: 0.1075Epoch 00008: val_loss improved from 2.41837 to 2.41764, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4280 - acc: 0.1075 - val_loss: 2.4176 - val_acc: 0.1123
Epoch 9/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4273 - acc: 0.1089Epoch 00009: val_loss improved from 2.41764 to 2.41729, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4271 - acc: 0.1095 - val_loss: 2.4173 - val_acc: 0.1117
Epoch 10/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4266 - acc: 0.1087Epoch 00010: val_loss improved from 2.41729 to 2.41688, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4267 - acc: 0.1085 - val_loss: 2.4169 - val_acc: 0.1117
Epoch 11/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4262 - acc: 0.1084- ETA: 0s - loss: 2.4338 - accEpoch 00011: val_loss improved from 2.41688 to 2.41648, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4262 - acc: 0.1085 - val_loss: 2.4165 - val_acc: 0.1123
Epoch 12/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4262 - acc: 0.1089Epoch 00012: val_loss improved from 2.41648 to 2.41585, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4261 - acc: 0.1086 - val_loss: 2.4159 - val_acc: 0.1123
Epoch 13/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4257 - acc: 0.1087Epoch 00013: val_loss improved from 2.41585 to 2.41560, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4252 - acc: 0.1091 - val_loss: 2.4156 - val_acc: 0.1123
Epoch 14/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4246 - acc: 0.1087Epoch 00014: val_loss improved from 2.41560 to 2.41537, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4254 - acc: 0.1090 - val_loss: 2.4154 - val_acc: 0.1123
Epoch 15/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4253 - acc: 0.1090Epoch 00015: val_loss improved from 2.41537 to 2.41526, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4244 - acc: 0.1089 - val_loss: 2.4153 - val_acc: 0.1123
Epoch 16/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4243 - acc: 0.1088Epoch 00016: val_loss improved from 2.41526 to 2.41493, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4252 - acc: 0.1086 - val_loss: 2.4149 - val_acc: 0.1123
Epoch 17/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4244 - acc: 0.1083Epoch 00017: val_loss improved from 2.41493 to 2.41460, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4243 - acc: 0.1085 - val_loss: 2.4146 - val_acc: 0.1123
Epoch 18/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4249 - acc: 0.1079Epoch 00018: val_loss improved from 2.41460 to 2.41431, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4244 - acc: 0.1087 - val_loss: 2.4143 - val_acc: 0.1123
Epoch 19/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4236 - acc: 0.1087Epoch 00019: val_loss improved from 2.41431 to 2.41417, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4240 - acc: 0.1091 - val_loss: 2.4142 - val_acc: 0.1123
Epoch 20/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4238 - acc: 0.1090Epoch 00020: val_loss improved from 2.41417 to 2.41399, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4248 - acc: 0.1089 - val_loss: 2.4140 - val_acc: 0.1123
Epoch 21/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4245 - acc: 0.1087Epoch 00021: val_loss improved from 2.41399 to 2.41394, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4243 - acc: 0.1085 - val_loss: 2.4139 - val_acc: 0.1123
Epoch 22/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4230 - acc: 0.1088Epoch 00022: val_loss improved from 2.41394 to 2.41378, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4238 - acc: 0.1087 - val_loss: 2.4138 - val_acc: 0.1123
Epoch 23/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4232 - acc: 0.1094Epoch 00023: val_loss improved from 2.41378 to 2.41370, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4244 - acc: 0.1092 - val_loss: 2.4137 - val_acc: 0.1123
Epoch 24/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4234 - acc: 0.1089Epoch 00024: val_loss improved from 2.41370 to 2.41354, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4235 - acc: 0.1083 - val_loss: 2.4135 - val_acc: 0.1123
Epoch 25/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4227 - acc: 0.1095Epoch 00025: val_loss improved from 2.41354 to 2.41336, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4228 - acc: 0.1093 - val_loss: 2.4134 - val_acc: 0.1123
Epoch 26/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4238 - acc: 0.1091Epoch 00026: val_loss improved from 2.41336 to 2.41330, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4232 - acc: 0.1092 - val_loss: 2.4133 - val_acc: 0.1123
Epoch 27/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4225 - acc: 0.1092Epoch 00027: val_loss improved from 2.41330 to 2.41323, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4231 - acc: 0.1090 - val_loss: 2.4132 - val_acc: 0.1123
Epoch 28/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4235 - acc: 0.1101Epoch 00028: val_loss improved from 2.41323 to 2.41316, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4232 - acc: 0.1094 - val_loss: 2.4132 - val_acc: 0.1123
Epoch 29/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4232 - acc: 0.1093Epoch 00029: val_loss improved from 2.41316 to 2.41312, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4233 - acc: 0.1091 - val_loss: 2.4131 - val_acc: 0.1123
Epoch 30/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4229 - acc: 0.1090Epoch 00030: val_loss improved from 2.41312 to 2.41309, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 94us/step - loss: 2.4232 - acc: 0.1088 - val_loss: 2.4131 - val_acc: 0.1123
Epoch 31/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4238 - acc: 0.1098Epoch 00031: val_loss improved from 2.41309 to 2.41306, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4238 - acc: 0.1092 - val_loss: 2.4131 - val_acc: 0.1123
Epoch 32/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4222 - acc: 0.1086Epoch 00032: val_loss improved from 2.41306 to 2.41304, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 91us/step - loss: 2.4229 - acc: 0.1081 - val_loss: 2.4130 - val_acc: 0.1123
Epoch 33/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4231 - acc: 0.1079Epoch 00033: val_loss improved from 2.41304 to 2.41302, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4229 - acc: 0.1079 - val_loss: 2.4130 - val_acc: 0.1123
Epoch 34/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4226 - acc: 0.1090Epoch 00034: val_loss improved from 2.41302 to 2.41300, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4229 - acc: 0.1089 - val_loss: 2.4130 - val_acc: 0.1123
Epoch 35/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4237 - acc: 0.1084Epoch 00035: val_loss improved from 2.41300 to 2.41299, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4233 - acc: 0.1082 - val_loss: 2.4130 - val_acc: 0.1123
Epoch 36/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4242 - acc: 0.1093Epoch 00036: val_loss improved from 2.41299 to 2.41298, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 91us/step - loss: 2.4238 - acc: 0.1093 - val_loss: 2.4130 - val_acc: 0.1123
Epoch 37/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4213 - acc: 0.1093Epoch 00037: val_loss improved from 2.41298 to 2.41296, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4225 - acc: 0.1089 - val_loss: 2.4130 - val_acc: 0.1123
Epoch 38/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4231 - acc: 0.1065Epoch 00038: val_loss improved from 2.41296 to 2.41296, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4229 - acc: 0.1069 - val_loss: 2.4130 - val_acc: 0.1123
Epoch 39/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4224 - acc: 0.1086Epoch 00039: val_loss improved from 2.41296 to 2.41295, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4224 - acc: 0.1088 - val_loss: 2.4129 - val_acc: 0.1123
Epoch 40/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4221 - acc: 0.1096Epoch 00040: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 2.4220 - acc: 0.1102 - val_loss: 2.4133 - val_acc: 0.1123
Epoch 41/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4236 - acc: 0.1090Epoch 00041: val_loss improved from 2.41295 to 2.41293, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 2.4239 - acc: 0.1088 - val_loss: 2.4129 - val_acc: 0.1123
Epoch 42/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4233 - acc: 0.1091Epoch 00042: val_loss improved from 2.41293 to 2.41293, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4225 - acc: 0.1095 - val_loss: 2.4129 - val_acc: 0.1123
Epoch 43/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4173 - acc: 0.1110Epoch 00043: val_loss improved from 2.41293 to 2.39229, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4185 - acc: 0.1110 - val_loss: 2.3923 - val_acc: 0.1212
Epoch 44/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.4036 - acc: 0.1204Epoch 00044: val_loss improved from 2.39229 to 2.38816, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.4029 - acc: 0.1214 - val_loss: 2.3882 - val_acc: 0.1246
Epoch 45/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.3882 - acc: 0.1281Epoch 00045: val_loss improved from 2.38816 to 2.35326, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.3878 - acc: 0.1283 - val_loss: 2.3533 - val_acc: 0.1408
Epoch 46/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.3588 - acc: 0.1386Epoch 00046: val_loss improved from 2.35326 to 2.35262, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.3585 - acc: 0.1386 - val_loss: 2.3526 - val_acc: 0.1508
Epoch 47/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.3285 - acc: 0.1501Epoch 00047: val_loss improved from 2.35262 to 2.30816, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.3299 - acc: 0.1504 - val_loss: 2.3082 - val_acc: 0.1693
Epoch 48/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.2982 - acc: 0.1658Epoch 00048: val_loss improved from 2.30816 to 2.17380, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.2961 - acc: 0.1665 - val_loss: 2.1738 - val_acc: 0.2056
Epoch 49/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.1950 - acc: 0.1869Epoch 00049: val_loss improved from 2.17380 to 2.09562, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.1943 - acc: 0.1876 - val_loss: 2.0956 - val_acc: 0.2223
Epoch 50/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 2.1021 - acc: 0.2235Epoch 00050: val_loss improved from 2.09562 to 1.88725, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 2.1000 - acc: 0.2245 - val_loss: 1.8872 - val_acc: 0.2966
Epoch 51/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 1.9666 - acc: 0.2811Epoch 00051: val_loss improved from 1.88725 to 1.78293, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 1.9650 - acc: 0.2814 - val_loss: 1.7829 - val_acc: 0.3827
Epoch 52/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 1.8748 - acc: 0.3177Epoch 00052: val_loss improved from 1.78293 to 1.65073, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 1.8739 - acc: 0.3186 - val_loss: 1.6507 - val_acc: 0.4508
Epoch 53/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 1.7762 - acc: 0.3600Epoch 00053: val_loss improved from 1.65073 to 1.53427, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 1.7734 - acc: 0.3597 - val_loss: 1.5343 - val_acc: 0.5017
Epoch 54/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 1.6881 - acc: 0.3934Epoch 00054: val_loss improved from 1.53427 to 1.41132, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 1.6832 - acc: 0.3947 - val_loss: 1.4113 - val_acc: 0.5380
Epoch 55/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 1.5801 - acc: 0.4342Epoch 00055: val_loss improved from 1.41132 to 1.33559, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 1.5818 - acc: 0.4342 - val_loss: 1.3356 - val_acc: 0.5687
Epoch 56/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 1.4811 - acc: 0.4721Epoch 00056: val_loss improved from 1.33559 to 1.17307, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 1.4762 - acc: 0.4737 - val_loss: 1.1731 - val_acc: 0.6257
Epoch 57/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 1.3967 - acc: 0.5021Epoch 00057: val_loss improved from 1.17307 to 1.07556, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 1.3915 - acc: 0.5045 - val_loss: 1.0756 - val_acc: 0.6520
Epoch 58/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 1.3168 - acc: 0.5346Epoch 00058: val_loss improved from 1.07556 to 0.99005, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 1.3096 - acc: 0.5381 - val_loss: 0.9900 - val_acc: 0.6838
Epoch 59/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 1.2297 - acc: 0.5689Epoch 00059: val_loss improved from 0.99005 to 0.91270, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 1.2303 - acc: 0.5687 - val_loss: 0.9127 - val_acc: 0.7101
Epoch 60/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 1.1651 - acc: 0.5959Epoch 00060: val_loss improved from 0.91270 to 0.85209, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 1.1633 - acc: 0.5969 - val_loss: 0.8521 - val_acc: 0.7318
Epoch 61/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 1.0978 - acc: 0.6153Epoch 00061: val_loss improved from 0.85209 to 0.81572, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 1.0981 - acc: 0.6155 - val_loss: 0.8157 - val_acc: 0.7318
Epoch 62/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 1.0487 - acc: 0.6392Epoch 00062: val_loss improved from 0.81572 to 0.74407, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 1.0508 - acc: 0.6391 - val_loss: 0.7441 - val_acc: 0.7676
Epoch 63/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 1.0118 - acc: 0.6518Epoch 00063: val_loss improved from 0.74407 to 0.70731, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 1.0079 - acc: 0.6537 - val_loss: 0.7073 - val_acc: 0.7693
Epoch 64/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.9680 - acc: 0.6696Epoch 00064: val_loss improved from 0.70731 to 0.68061, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 0.9678 - acc: 0.6702 - val_loss: 0.6806 - val_acc: 0.7737
Epoch 65/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.9308 - acc: 0.6819Epoch 00065: val_loss improved from 0.68061 to 0.64626, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.9307 - acc: 0.6820 - val_loss: 0.6463 - val_acc: 0.7911
Epoch 66/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.9053 - acc: 0.6880Epoch 00066: val_loss improved from 0.64626 to 0.61756, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 94us/step - loss: 0.9052 - acc: 0.6884 - val_loss: 0.6176 - val_acc: 0.7972
Epoch 67/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.8784 - acc: 0.6976Epoch 00067: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.8788 - acc: 0.6984 - val_loss: 0.6184 - val_acc: 0.7994
Epoch 68/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.8423 - acc: 0.7119Epoch 00068: val_loss improved from 0.61756 to 0.59096, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.8440 - acc: 0.7115 - val_loss: 0.5910 - val_acc: 0.8078
Epoch 69/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.8230 - acc: 0.7196Epoch 00069: val_loss improved from 0.59096 to 0.55803, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 94us/step - loss: 0.8236 - acc: 0.7195 - val_loss: 0.5580 - val_acc: 0.8179
Epoch 70/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.7987 - acc: 0.7249Epoch 00070: val_loss did not improve
17890/17890 [==============================] - 2s 93us/step - loss: 0.7988 - acc: 0.7254 - val_loss: 0.5621 - val_acc: 0.8156
Epoch 71/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.7775 - acc: 0.7339Epoch 00071: val_loss improved from 0.55803 to 0.53080, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 94us/step - loss: 0.7769 - acc: 0.7342 - val_loss: 0.5308 - val_acc: 0.8274
Epoch 72/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.7582 - acc: 0.7418Epoch 00072: val_loss improved from 0.53080 to 0.52013, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 94us/step - loss: 0.7594 - acc: 0.7409 - val_loss: 0.5201 - val_acc: 0.8302
Epoch 73/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.7570 - acc: 0.7425Epoch 00073: val_loss improved from 0.52013 to 0.50940, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 94us/step - loss: 0.7560 - acc: 0.7436 - val_loss: 0.5094 - val_acc: 0.8346
Epoch 74/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.7217 - acc: 0.7568Epoch 00074: val_loss improved from 0.50940 to 0.49320, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 94us/step - loss: 0.7200 - acc: 0.7573 - val_loss: 0.4932 - val_acc: 0.8453
Epoch 75/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.7168 - acc: 0.7569Epoch 00075: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.7181 - acc: 0.7571 - val_loss: 0.5149 - val_acc: 0.8369
Epoch 76/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.6844 - acc: 0.7652Epoch 00076: val_loss improved from 0.49320 to 0.48037, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.6866 - acc: 0.7646 - val_loss: 0.4804 - val_acc: 0.8419
Epoch 77/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.6832 - acc: 0.7666Epoch 00077: val_loss improved from 0.48037 to 0.46526, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 94us/step - loss: 0.6820 - acc: 0.7670 - val_loss: 0.4653 - val_acc: 0.8447
Epoch 78/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.6658 - acc: 0.7738Epoch 00078: val_loss did not improve
17890/17890 [==============================] - 2s 93us/step - loss: 0.6656 - acc: 0.7741 - val_loss: 0.4730 - val_acc: 0.8430
Epoch 79/250
17664/17890 [============================&gt;.] - ETA: 0s - loss: 0.6489 - acc: 0.7812Epoch 00079: val_loss did not improve
17890/17890 [==============================] - 2s 93us/step - loss: 0.6489 - acc: 0.7816 - val_loss: 0.4672 - val_acc: 0.8475
Epoch 80/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.6525 - acc: 0.7807Epoch 00080: val_loss improved from 0.46526 to 0.45454, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 94us/step - loss: 0.6501 - acc: 0.7817 - val_loss: 0.4545 - val_acc: 0.8508
Epoch 81/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.6336 - acc: 0.7879Epoch 00081: val_loss improved from 0.45454 to 0.43293, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.6326 - acc: 0.7876 - val_loss: 0.4329 - val_acc: 0.8575
Epoch 82/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.6258 - acc: 0.7871Epoch 00082: val_loss improved from 0.43293 to 0.43168, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 0.6256 - acc: 0.7876 - val_loss: 0.4317 - val_acc: 0.8581
Epoch 83/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.6180 - acc: 0.7930Epoch 00083: val_loss improved from 0.43168 to 0.43108, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.6167 - acc: 0.7937 - val_loss: 0.4311 - val_acc: 0.8592
Epoch 84/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.6008 - acc: 0.7970Epoch 00084: val_loss improved from 0.43108 to 0.42542, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.5998 - acc: 0.7978 - val_loss: 0.4254 - val_acc: 0.8626
Epoch 85/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.5929 - acc: 0.8022Epoch 00085: val_loss improved from 0.42542 to 0.41298, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.5931 - acc: 0.8017 - val_loss: 0.4130 - val_acc: 0.8693
Epoch 86/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.5879 - acc: 0.8025Epoch 00086: val_loss improved from 0.41298 to 0.40987, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 94us/step - loss: 0.5875 - acc: 0.8026 - val_loss: 0.4099 - val_acc: 0.8665
Epoch 87/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.5855 - acc: 0.8036Epoch 00087: val_loss improved from 0.40987 to 0.40930, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.5852 - acc: 0.8036 - val_loss: 0.4093 - val_acc: 0.8654
Epoch 88/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.5746 - acc: 0.8069Epoch 00088: val_loss improved from 0.40930 to 0.40377, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 94us/step - loss: 0.5729 - acc: 0.8069 - val_loss: 0.4038 - val_acc: 0.8698
Epoch 89/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.5727 - acc: 0.8074Epoch 00089: val_loss improved from 0.40377 to 0.39793, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.5738 - acc: 0.8075 - val_loss: 0.3979 - val_acc: 0.8760
Epoch 90/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.5475 - acc: 0.8168Epoch 00090: val_loss improved from 0.39793 to 0.38182, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.5480 - acc: 0.8163 - val_loss: 0.3818 - val_acc: 0.8754
Epoch 91/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.5393 - acc: 0.8190Epoch 00091: val_loss improved from 0.38182 to 0.38054, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.5414 - acc: 0.8179 - val_loss: 0.3805 - val_acc: 0.8777
Epoch 92/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.5393 - acc: 0.8205Epoch 00092: val_loss improved from 0.38054 to 0.37998, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.5407 - acc: 0.8206 - val_loss: 0.3800 - val_acc: 0.8743
Epoch 93/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.5324 - acc: 0.8210Epoch 00093: val_loss improved from 0.37998 to 0.37273, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.5322 - acc: 0.8210 - val_loss: 0.3727 - val_acc: 0.8799
Epoch 94/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.5331 - acc: 0.8188Epoch 00094: val_loss improved from 0.37273 to 0.36637, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.5321 - acc: 0.8193 - val_loss: 0.3664 - val_acc: 0.8827
Epoch 95/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.5201 - acc: 0.8231Epoch 00095: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.5188 - acc: 0.8236 - val_loss: 0.3731 - val_acc: 0.8782
Epoch 96/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.5185 - acc: 0.8282Epoch 00096: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.5178 - acc: 0.8285 - val_loss: 0.3677 - val_acc: 0.8749
Epoch 97/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4996 - acc: 0.8348Epoch 00097: val_loss improved from 0.36637 to 0.35587, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 0.5014 - acc: 0.8342 - val_loss: 0.3559 - val_acc: 0.8855
Epoch 98/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.5069 - acc: 0.8285Epoch 00098: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.5064 - acc: 0.8285 - val_loss: 0.3689 - val_acc: 0.8777
Epoch 99/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4997 - acc: 0.8306Epoch 00099: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.4984 - acc: 0.8315 - val_loss: 0.3629 - val_acc: 0.8860
Epoch 100/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4972 - acc: 0.8306Epoch 00100: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.4973 - acc: 0.8310 - val_loss: 0.3608 - val_acc: 0.8849
Epoch 101/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.5003 - acc: 0.8317Epoch 00101: val_loss improved from 0.35587 to 0.34863, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 0.4985 - acc: 0.8322 - val_loss: 0.3486 - val_acc: 0.8933
Epoch 102/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4812 - acc: 0.8399Epoch 00102: val_loss improved from 0.34863 to 0.34254, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.4812 - acc: 0.8397 - val_loss: 0.3425 - val_acc: 0.8927
Epoch 103/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4765 - acc: 0.8400Epoch 00103: val_loss improved from 0.34254 to 0.33756, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 0.4746 - acc: 0.8401 - val_loss: 0.3376 - val_acc: 0.8911
Epoch 104/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4682 - acc: 0.8444Epoch 00104: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.4674 - acc: 0.8440 - val_loss: 0.3387 - val_acc: 0.8905
Epoch 105/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4659 - acc: 0.8440Epoch 00105: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.4664 - acc: 0.8438 - val_loss: 0.3531 - val_acc: 0.8894
Epoch 106/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4671 - acc: 0.8442Epoch 00106: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.4662 - acc: 0.8446 - val_loss: 0.3386 - val_acc: 0.8888
Epoch 107/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4576 - acc: 0.8464Epoch 00107: val_loss improved from 0.33756 to 0.33113, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.4578 - acc: 0.8463 - val_loss: 0.3311 - val_acc: 0.8933
Epoch 108/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4493 - acc: 0.8471Epoch 00108: val_loss improved from 0.33113 to 0.32465, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.4522 - acc: 0.8462 - val_loss: 0.3247 - val_acc: 0.8922
Epoch 109/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4487 - acc: 0.8468Epoch 00109: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.4505 - acc: 0.8457 - val_loss: 0.3286 - val_acc: 0.8922
Epoch 110/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4494 - acc: 0.8486Epoch 00110: val_loss improved from 0.32465 to 0.32168, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.4473 - acc: 0.8491 - val_loss: 0.3217 - val_acc: 0.8989
Epoch 111/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4471 - acc: 0.8512Epoch 00111: val_loss improved from 0.32168 to 0.32123, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.4468 - acc: 0.8516 - val_loss: 0.3212 - val_acc: 0.8994
Epoch 112/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4428 - acc: 0.8488Epoch 00112: val_loss improved from 0.32123 to 0.31623, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.4418 - acc: 0.8492 - val_loss: 0.3162 - val_acc: 0.8994
Epoch 113/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4350 - acc: 0.8534Epoch 00113: val_loss improved from 0.31623 to 0.30875, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 94us/step - loss: 0.4346 - acc: 0.8530 - val_loss: 0.3087 - val_acc: 0.9017
Epoch 114/250
17664/17890 [============================&gt;.] - ETA: 0s - loss: 0.4357 - acc: 0.8541Epoch 00114: val_loss did not improve
17890/17890 [==============================] - 2s 93us/step - loss: 0.4354 - acc: 0.8543 - val_loss: 0.3128 - val_acc: 0.9011
Epoch 115/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4297 - acc: 0.8560Epoch 00115: val_loss did not improve
17890/17890 [==============================] - 2s 93us/step - loss: 0.4297 - acc: 0.8561 - val_loss: 0.3190 - val_acc: 0.8989
Epoch 116/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4234 - acc: 0.8582Epoch 00116: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.4246 - acc: 0.8574 - val_loss: 0.3176 - val_acc: 0.9039
Epoch 117/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4129 - acc: 0.8627Epoch 00117: val_loss improved from 0.30875 to 0.30248, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.4126 - acc: 0.8625 - val_loss: 0.3025 - val_acc: 0.9034
Epoch 118/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4179 - acc: 0.8594Epoch 00118: val_loss improved from 0.30248 to 0.29248, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.4172 - acc: 0.8597 - val_loss: 0.2925 - val_acc: 0.9045
Epoch 119/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4132 - acc: 0.8615Epoch 00119: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.4130 - acc: 0.8617 - val_loss: 0.3087 - val_acc: 0.9039
Epoch 120/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4111 - acc: 0.8612Epoch 00120: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.4117 - acc: 0.8614 - val_loss: 0.2962 - val_acc: 0.9039
Epoch 121/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4018 - acc: 0.8635Epoch 00121: val_loss improved from 0.29248 to 0.29012, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.4018 - acc: 0.8635 - val_loss: 0.2901 - val_acc: 0.9045
Epoch 122/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4057 - acc: 0.8626Epoch 00122: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.4058 - acc: 0.8625 - val_loss: 0.2915 - val_acc: 0.9106
Epoch 123/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.4031 - acc: 0.8651- ETA: 0s - loss: 0.4019 - acc: 0.86Epoch 00123: val_loss improved from 0.29012 to 0.28648, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 0.4014 - acc: 0.8656 - val_loss: 0.2865 - val_acc: 0.9101
Epoch 124/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3981 - acc: 0.8659Epoch 00124: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.3981 - acc: 0.8657 - val_loss: 0.2907 - val_acc: 0.9084
Epoch 125/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3950 - acc: 0.8686Epoch 00125: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.3978 - acc: 0.8682 - val_loss: 0.2874 - val_acc: 0.9073
Epoch 126/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3876 - acc: 0.8681Epoch 00126: val_loss improved from 0.28648 to 0.28646, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 0.3870 - acc: 0.8686 - val_loss: 0.2865 - val_acc: 0.9084
Epoch 127/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3939 - acc: 0.8682- ETA: 0s - loss: 0.4057 Epoch 00127: val_loss improved from 0.28646 to 0.28530, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 91us/step - loss: 0.3953 - acc: 0.8678 - val_loss: 0.2853 - val_acc: 0.9084
Epoch 128/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3869 - acc: 0.8689Epoch 00128: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.3867 - acc: 0.8686 - val_loss: 0.2976 - val_acc: 0.9050
Epoch 129/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3752 - acc: 0.8720Epoch 00129: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.3743 - acc: 0.8726 - val_loss: 0.2900 - val_acc: 0.9078
Epoch 130/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3815 - acc: 0.8688Epoch 00130: val_loss improved from 0.28530 to 0.27915, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.3832 - acc: 0.8686 - val_loss: 0.2792 - val_acc: 0.9112
Epoch 131/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3738 - acc: 0.8734Epoch 00131: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.3740 - acc: 0.8737 - val_loss: 0.2893 - val_acc: 0.9078
Epoch 132/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3776 - acc: 0.8729Epoch 00132: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.3789 - acc: 0.8731 - val_loss: 0.2862 - val_acc: 0.9073
Epoch 133/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3812 - acc: 0.8712Epoch 00133: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.3803 - acc: 0.8713 - val_loss: 0.2818 - val_acc: 0.9078
Epoch 134/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3659 - acc: 0.8771Epoch 00134: val_loss improved from 0.27915 to 0.27450, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.3665 - acc: 0.8769 - val_loss: 0.2745 - val_acc: 0.9128
Epoch 135/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3703 - acc: 0.8766Epoch 00135: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.3726 - acc: 0.8756 - val_loss: 0.2791 - val_acc: 0.9045
Epoch 136/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3612 - acc: 0.8770Epoch 00136: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.3598 - acc: 0.8771 - val_loss: 0.2774 - val_acc: 0.9117
Epoch 137/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3602 - acc: 0.8772Epoch 00137: val_loss improved from 0.27450 to 0.27428, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.3601 - acc: 0.8774 - val_loss: 0.2743 - val_acc: 0.9117
Epoch 138/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3678 - acc: 0.8756Epoch 00138: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.3671 - acc: 0.8757 - val_loss: 0.2763 - val_acc: 0.9112
Epoch 139/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3585 - acc: 0.8799Epoch 00139: val_loss improved from 0.27428 to 0.27186, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 0.3594 - acc: 0.8797 - val_loss: 0.2719 - val_acc: 0.9134
Epoch 140/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3572 - acc: 0.8805Epoch 00140: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.3585 - acc: 0.8799 - val_loss: 0.2777 - val_acc: 0.9095
Epoch 141/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3589 - acc: 0.8801Epoch 00141: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.3603 - acc: 0.8793 - val_loss: 0.2726 - val_acc: 0.9140
Epoch 142/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3667 - acc: 0.8764Epoch 00142: val_loss improved from 0.27186 to 0.26305, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.3653 - acc: 0.8767 - val_loss: 0.2631 - val_acc: 0.9117
Epoch 143/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3549 - acc: 0.8801Epoch 00143: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.3553 - acc: 0.8801 - val_loss: 0.2701 - val_acc: 0.9128
Epoch 144/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3521 - acc: 0.8809Epoch 00144: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.3509 - acc: 0.8814 - val_loss: 0.2691 - val_acc: 0.9106
Epoch 145/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3434 - acc: 0.8840Epoch 00145: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.3445 - acc: 0.8835 - val_loss: 0.2788 - val_acc: 0.9123
Epoch 146/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3446 - acc: 0.8856Epoch 00146: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.3466 - acc: 0.8843 - val_loss: 0.2684 - val_acc: 0.9128
Epoch 147/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3463 - acc: 0.8821Epoch 00147: val_loss improved from 0.26305 to 0.26024, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.3470 - acc: 0.8821 - val_loss: 0.2602 - val_acc: 0.9151
Epoch 148/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3509 - acc: 0.8821Epoch 00148: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.3502 - acc: 0.8824 - val_loss: 0.2621 - val_acc: 0.9196
Epoch 149/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3375 - acc: 0.8841Epoch 00149: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.3371 - acc: 0.8843 - val_loss: 0.2637 - val_acc: 0.9162
Epoch 150/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3444 - acc: 0.8832Epoch 00150: val_loss improved from 0.26024 to 0.25565, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 0.3443 - acc: 0.8833 - val_loss: 0.2557 - val_acc: 0.9168
Epoch 151/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3308 - acc: 0.8876Epoch 00151: val_loss improved from 0.25565 to 0.25321, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.3309 - acc: 0.8878 - val_loss: 0.2532 - val_acc: 0.9156
Epoch 152/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3307 - acc: 0.8889Epoch 00152: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.3287 - acc: 0.8892 - val_loss: 0.2655 - val_acc: 0.9106
Epoch 153/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3254 - acc: 0.8899Epoch 00153: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.3275 - acc: 0.8892 - val_loss: 0.2584 - val_acc: 0.9140
Epoch 154/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3396 - acc: 0.8855Epoch 00154: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.3382 - acc: 0.8860 - val_loss: 0.2551 - val_acc: 0.9184
Epoch 155/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3298 - acc: 0.8888Epoch 00155: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.3289 - acc: 0.8889 - val_loss: 0.2651 - val_acc: 0.9117
Epoch 156/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3314 - acc: 0.8851Epoch 00156: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.3320 - acc: 0.8853 - val_loss: 0.2651 - val_acc: 0.9145
Epoch 157/250
17664/17890 [============================&gt;.] - ETA: 0s - loss: 0.3252 - acc: 0.8903Epoch 00157: val_loss improved from 0.25321 to 0.25013, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.3244 - acc: 0.8906 - val_loss: 0.2501 - val_acc: 0.9190
Epoch 158/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3223 - acc: 0.8902Epoch 00158: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.3222 - acc: 0.8901 - val_loss: 0.2532 - val_acc: 0.9151
Epoch 159/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3312 - acc: 0.8879Epoch 00159: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.3302 - acc: 0.8883 - val_loss: 0.2643 - val_acc: 0.9128
Epoch 160/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3227 - acc: 0.8902Epoch 00160: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.3198 - acc: 0.8913 - val_loss: 0.2534 - val_acc: 0.9173
Epoch 161/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3239 - acc: 0.8900Epoch 00161: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.3241 - acc: 0.8901 - val_loss: 0.2637 - val_acc: 0.9123
Epoch 162/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3135 - acc: 0.8937Epoch 00162: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.3152 - acc: 0.8925 - val_loss: 0.2549 - val_acc: 0.9168
Epoch 163/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3234 - acc: 0.8892Epoch 00163: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.3206 - acc: 0.8899 - val_loss: 0.2552 - val_acc: 0.9140
Epoch 164/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3164 - acc: 0.8927Epoch 00164: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.3165 - acc: 0.8930 - val_loss: 0.2540 - val_acc: 0.9173
Epoch 165/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3154 - acc: 0.8908Epoch 00165: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.3150 - acc: 0.8908 - val_loss: 0.2616 - val_acc: 0.9140
Epoch 166/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3128 - acc: 0.8938Epoch 00166: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.3128 - acc: 0.8940 - val_loss: 0.2518 - val_acc: 0.9229
Epoch 167/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3071 - acc: 0.8978Epoch 00167: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.3096 - acc: 0.8966 - val_loss: 0.2503 - val_acc: 0.9196
Epoch 168/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2997 - acc: 0.8961Epoch 00168: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.3027 - acc: 0.8950 - val_loss: 0.2520 - val_acc: 0.9173
Epoch 169/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3047 - acc: 0.9008Epoch 00169: val_loss improved from 0.25013 to 0.24793, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.3055 - acc: 0.9006 - val_loss: 0.2479 - val_acc: 0.9162
Epoch 170/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3069 - acc: 0.8935Epoch 00170: val_loss improved from 0.24793 to 0.24395, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.3067 - acc: 0.8939 - val_loss: 0.2439 - val_acc: 0.9184
Epoch 171/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3117 - acc: 0.8944Epoch 00171: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.3110 - acc: 0.8944 - val_loss: 0.2536 - val_acc: 0.9156
Epoch 172/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3057 - acc: 0.8968Epoch 00172: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.3049 - acc: 0.8970 - val_loss: 0.2465 - val_acc: 0.9196
Epoch 173/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3057 - acc: 0.8974Epoch 00173: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.3066 - acc: 0.8975 - val_loss: 0.2493 - val_acc: 0.9235
Epoch 174/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3039 - acc: 0.8974Epoch 00174: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.3025 - acc: 0.8978 - val_loss: 0.2490 - val_acc: 0.9207
Epoch 175/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3049 - acc: 0.8981Epoch 00175: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.3049 - acc: 0.8981 - val_loss: 0.2493 - val_acc: 0.9196
Epoch 176/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2968 - acc: 0.8957- ETA: 0s - loss: 0.2997 - acc: 0.89Epoch 00176: val_loss did not improve
17890/17890 [==============================] - 2s 89us/step - loss: 0.2956 - acc: 0.8964 - val_loss: 0.2458 - val_acc: 0.9207
Epoch 177/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2892 - acc: 0.9033Epoch 00177: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.2900 - acc: 0.9030 - val_loss: 0.2459 - val_acc: 0.9201
Epoch 178/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.3050 - acc: 0.8974Epoch 00178: val_loss improved from 0.24395 to 0.24003, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.3050 - acc: 0.8969 - val_loss: 0.2400 - val_acc: 0.9207
Epoch 179/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2868 - acc: 0.9000Epoch 00179: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2876 - acc: 0.8993 - val_loss: 0.2424 - val_acc: 0.9190
Epoch 180/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2953 - acc: 0.8982Epoch 00180: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.2958 - acc: 0.8987 - val_loss: 0.2421 - val_acc: 0.9223
Epoch 181/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2906 - acc: 0.9004Epoch 00181: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.2896 - acc: 0.9010 - val_loss: 0.2409 - val_acc: 0.9223
Epoch 182/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2913 - acc: 0.9014Epoch 00182: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.2907 - acc: 0.9015 - val_loss: 0.2412 - val_acc: 0.9218
Epoch 183/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2860 - acc: 0.9012Epoch 00183: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2848 - acc: 0.9016 - val_loss: 0.2436 - val_acc: 0.9207
Epoch 184/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2848 - acc: 0.9025Epoch 00184: val_loss did not improve
17890/17890 [==============================] - 2s 89us/step - loss: 0.2847 - acc: 0.9028 - val_loss: 0.2495 - val_acc: 0.9196
Epoch 185/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2887 - acc: 0.9038Epoch 00185: val_loss improved from 0.24003 to 0.23881, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 0.2898 - acc: 0.9034 - val_loss: 0.2388 - val_acc: 0.9240
Epoch 186/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2862 - acc: 0.9035Epoch 00186: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2859 - acc: 0.9037 - val_loss: 0.2460 - val_acc: 0.9201
Epoch 187/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2868 - acc: 0.9028Epoch 00187: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2875 - acc: 0.9029 - val_loss: 0.2433 - val_acc: 0.9274
Epoch 188/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2834 - acc: 0.9015Epoch 00188: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2823 - acc: 0.9017 - val_loss: 0.2439 - val_acc: 0.9246
Epoch 189/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2846 - acc: 0.9053Epoch 00189: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2852 - acc: 0.9051 - val_loss: 0.2404 - val_acc: 0.9218
Epoch 190/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2819 - acc: 0.9039Epoch 00190: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2821 - acc: 0.9038 - val_loss: 0.2394 - val_acc: 0.9212
Epoch 191/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2812 - acc: 0.9023Epoch 00191: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2801 - acc: 0.9028 - val_loss: 0.2436 - val_acc: 0.9162
Epoch 192/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2834 - acc: 0.9043Epoch 00192: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.2827 - acc: 0.9050 - val_loss: 0.2448 - val_acc: 0.9201
Epoch 193/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2785 - acc: 0.9050Epoch 00193: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.2797 - acc: 0.9045 - val_loss: 0.2420 - val_acc: 0.9201
Epoch 194/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2839 - acc: 0.9025Epoch 00194: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2841 - acc: 0.9025 - val_loss: 0.2435 - val_acc: 0.9196
Epoch 195/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2763 - acc: 0.9098Epoch 00195: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2755 - acc: 0.9101 - val_loss: 0.2414 - val_acc: 0.9251
Epoch 196/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2768 - acc: 0.9044Epoch 00196: val_loss improved from 0.23881 to 0.23478, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 94us/step - loss: 0.2773 - acc: 0.9045 - val_loss: 0.2348 - val_acc: 0.9257
Epoch 197/250
17664/17890 [============================&gt;.] - ETA: 0s - loss: 0.2714 - acc: 0.9059Epoch 00197: val_loss did not improve
17890/17890 [==============================] - 2s 93us/step - loss: 0.2714 - acc: 0.9058 - val_loss: 0.2403 - val_acc: 0.9229
Epoch 198/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2770 - acc: 0.9054Epoch 00198: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.2757 - acc: 0.9059 - val_loss: 0.2390 - val_acc: 0.9246
Epoch 199/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2740 - acc: 0.9050Epoch 00199: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2741 - acc: 0.9050 - val_loss: 0.2361 - val_acc: 0.9223
Epoch 200/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2683 - acc: 0.9092Epoch 00200: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2692 - acc: 0.9093 - val_loss: 0.2429 - val_acc: 0.9229
Epoch 201/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2686 - acc: 0.9086Epoch 00201: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.2669 - acc: 0.9092 - val_loss: 0.2358 - val_acc: 0.9212
Epoch 202/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2658 - acc: 0.9100Epoch 00202: val_loss improved from 0.23478 to 0.23328, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 0.2659 - acc: 0.9098 - val_loss: 0.2333 - val_acc: 0.9201
Epoch 203/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2625 - acc: 0.9110Epoch 00203: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2622 - acc: 0.9114 - val_loss: 0.2359 - val_acc: 0.9229
Epoch 204/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2690 - acc: 0.9074Epoch 00204: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2703 - acc: 0.9070 - val_loss: 0.2354 - val_acc: 0.9201
Epoch 205/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2685 - acc: 0.9096Epoch 00205: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2704 - acc: 0.9087 - val_loss: 0.2363 - val_acc: 0.9223
Epoch 206/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2675 - acc: 0.9082Epoch 00206: val_loss improved from 0.23328 to 0.23014, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.2665 - acc: 0.9088 - val_loss: 0.2301 - val_acc: 0.9212
Epoch 207/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2668 - acc: 0.9106Epoch 00207: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2676 - acc: 0.9101 - val_loss: 0.2378 - val_acc: 0.9246
Epoch 208/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2602 - acc: 0.9106Epoch 00208: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2618 - acc: 0.9103 - val_loss: 0.2323 - val_acc: 0.9263
Epoch 209/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2605 - acc: 0.9130Epoch 00209: val_loss did not improve
17890/17890 [==============================] - 2s 89us/step - loss: 0.2609 - acc: 0.9129 - val_loss: 0.2372 - val_acc: 0.9246
Epoch 210/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2603 - acc: 0.9118Epoch 00210: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.2622 - acc: 0.9106 - val_loss: 0.2399 - val_acc: 0.9223
Epoch 211/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2566 - acc: 0.9110Epoch 00211: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2560 - acc: 0.9111 - val_loss: 0.2318 - val_acc: 0.9240
Epoch 212/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2610 - acc: 0.9104Epoch 00212: val_loss improved from 0.23014 to 0.22956, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.2607 - acc: 0.9104 - val_loss: 0.2296 - val_acc: 0.9274
Epoch 213/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2549 - acc: 0.9148Epoch 00213: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.2558 - acc: 0.9143 - val_loss: 0.2373 - val_acc: 0.9279
Epoch 214/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2530 - acc: 0.9134Epoch 00214: val_loss did not improve
17890/17890 [==============================] - 2s 89us/step - loss: 0.2531 - acc: 0.9132 - val_loss: 0.2396 - val_acc: 0.9291
Epoch 215/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2610 - acc: 0.9115Epoch 00215: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.2613 - acc: 0.9115 - val_loss: 0.2335 - val_acc: 0.9263
Epoch 216/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2528 - acc: 0.9152Epoch 00216: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.2543 - acc: 0.9143 - val_loss: 0.2370 - val_acc: 0.9240
Epoch 217/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2545 - acc: 0.9146Epoch 00217: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2555 - acc: 0.9138 - val_loss: 0.2313 - val_acc: 0.9263
Epoch 218/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2447 - acc: 0.9160Epoch 00218: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2454 - acc: 0.9157 - val_loss: 0.2347 - val_acc: 0.9257
Epoch 219/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2479 - acc: 0.9172Epoch 00219: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.2492 - acc: 0.9167 - val_loss: 0.2355 - val_acc: 0.9190
Epoch 220/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2524 - acc: 0.9149Epoch 00220: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.2539 - acc: 0.9144 - val_loss: 0.2324 - val_acc: 0.9240
Epoch 221/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2442 - acc: 0.9179Epoch 00221: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.2443 - acc: 0.9181 - val_loss: 0.2375 - val_acc: 0.9207
Epoch 222/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2483 - acc: 0.9164Epoch 00222: val_loss improved from 0.22956 to 0.22880, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 92us/step - loss: 0.2494 - acc: 0.9155 - val_loss: 0.2288 - val_acc: 0.9257
Epoch 223/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2502 - acc: 0.9132Epoch 00223: val_loss improved from 0.22880 to 0.22638, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 94us/step - loss: 0.2492 - acc: 0.9133 - val_loss: 0.2264 - val_acc: 0.9223
Epoch 224/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2476 - acc: 0.9162Epoch 00224: val_loss improved from 0.22638 to 0.22505, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 94us/step - loss: 0.2487 - acc: 0.9161 - val_loss: 0.2251 - val_acc: 0.9296
Epoch 225/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2466 - acc: 0.9153Epoch 00225: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2475 - acc: 0.9150 - val_loss: 0.2267 - val_acc: 0.9257
Epoch 226/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2500 - acc: 0.9142Epoch 00226: val_loss improved from 0.22505 to 0.21849, saving model to weights5.hdf5
17890/17890 [==============================] - 2s 93us/step - loss: 0.2505 - acc: 0.9143 - val_loss: 0.2185 - val_acc: 0.9251
Epoch 227/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2456 - acc: 0.9159Epoch 00227: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.2457 - acc: 0.9159 - val_loss: 0.2220 - val_acc: 0.9263
Epoch 228/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2501 - acc: 0.9145Epoch 00228: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.2494 - acc: 0.9150 - val_loss: 0.2208 - val_acc: 0.9263
Epoch 229/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2493 - acc: 0.9153Epoch 00229: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2500 - acc: 0.9151 - val_loss: 0.2297 - val_acc: 0.9291
Epoch 230/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2403 - acc: 0.9160Epoch 00230: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2420 - acc: 0.9159 - val_loss: 0.2263 - val_acc: 0.9318
Epoch 231/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2408 - acc: 0.9184Epoch 00231: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2424 - acc: 0.9180 - val_loss: 0.2250 - val_acc: 0.9291
Epoch 232/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2370 - acc: 0.9193Epoch 00232: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.2368 - acc: 0.9193 - val_loss: 0.2241 - val_acc: 0.9291
Epoch 233/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2490 - acc: 0.9172Epoch 00233: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.2476 - acc: 0.9176 - val_loss: 0.2267 - val_acc: 0.9263
Epoch 234/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2384 - acc: 0.9171Epoch 00234: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.2393 - acc: 0.9170 - val_loss: 0.2292 - val_acc: 0.9263
Epoch 235/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2418 - acc: 0.9163Epoch 00235: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.2403 - acc: 0.9168 - val_loss: 0.2262 - val_acc: 0.9246
Epoch 236/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2371 - acc: 0.9194Epoch 00236: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.2373 - acc: 0.9191 - val_loss: 0.2280 - val_acc: 0.9268
Epoch 237/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2398 - acc: 0.9188Epoch 00237: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.2410 - acc: 0.9180 - val_loss: 0.2324 - val_acc: 0.9263
Epoch 238/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2364 - acc: 0.9184Epoch 00238: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.2369 - acc: 0.9183 - val_loss: 0.2250 - val_acc: 0.9268
Epoch 239/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2335 - acc: 0.9215Epoch 00239: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.2346 - acc: 0.9213 - val_loss: 0.2238 - val_acc: 0.9268
Epoch 240/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2369 - acc: 0.9183- ETA: 0s - loss: 0.2394 -Epoch 00240: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2366 - acc: 0.9179 - val_loss: 0.2248 - val_acc: 0.9296
Epoch 241/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2312 - acc: 0.9219Epoch 00241: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.2313 - acc: 0.9220 - val_loss: 0.2256 - val_acc: 0.9268
Epoch 242/250
17664/17890 [============================&gt;.] - ETA: 0s - loss: 0.2368 - acc: 0.9181Epoch 00242: val_loss did not improve
17890/17890 [==============================] - 2s 92us/step - loss: 0.2372 - acc: 0.9181 - val_loss: 0.2282 - val_acc: 0.9274
Epoch 243/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2383 - acc: 0.9202Epoch 00243: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2405 - acc: 0.9201 - val_loss: 0.2262 - val_acc: 0.9235
Epoch 244/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2285 - acc: 0.9223Epoch 00244: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2294 - acc: 0.9219 - val_loss: 0.2300 - val_acc: 0.9268
Epoch 245/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2265 - acc: 0.9262Epoch 00245: val_loss did not improve
17890/17890 [==============================] - 2s 93us/step - loss: 0.2270 - acc: 0.9257 - val_loss: 0.2274 - val_acc: 0.9291
Epoch 246/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2343 - acc: 0.9208Epoch 00246: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2339 - acc: 0.9210 - val_loss: 0.2289 - val_acc: 0.9263
Epoch 247/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2337 - acc: 0.9201Epoch 00247: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.2354 - acc: 0.9198 - val_loss: 0.2269 - val_acc: 0.9246
Epoch 248/250
17664/17890 [============================&gt;.] - ETA: 0s - loss: 0.2277 - acc: 0.9243Epoch 00248: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2278 - acc: 0.9244 - val_loss: 0.2298 - val_acc: 0.9268
Epoch 249/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2256 - acc: 0.9216Epoch 00249: val_loss did not improve
17890/17890 [==============================] - 2s 90us/step - loss: 0.2284 - acc: 0.9206 - val_loss: 0.2263 - val_acc: 0.9240
Epoch 250/250
17152/17890 [===========================&gt;..] - ETA: 0s - loss: 0.2287 - acc: 0.9240Epoch 00250: val_loss did not improve
17890/17890 [==============================] - 2s 91us/step - loss: 0.2294 - acc: 0.9233 - val_loss: 0.2253 - val_acc: 0.9274
6


 trianing 



Train on 17892 samples, validate on 1788 samples
Epoch 1/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 7.2552 - acc: 0.1010Epoch 00001: val_loss improved from inf to 2.47361, saving model to weights6.hdf5
17892/17892 [==============================] - 3s 161us/step - loss: 7.0598 - acc: 0.1011 - val_loss: 2.4736 - val_acc: 0.1063
Epoch 2/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4814 - acc: 0.1052Epoch 00002: val_loss improved from 2.47361 to 2.42817, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 2.4793 - acc: 0.1052 - val_loss: 2.4282 - val_acc: 0.1057
Epoch 3/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4439 - acc: 0.1058Epoch 00003: val_loss improved from 2.42817 to 2.42645, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 2.4445 - acc: 0.1061 - val_loss: 2.4264 - val_acc: 0.1096
Epoch 4/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4362 - acc: 0.1062Epoch 00004: val_loss improved from 2.42645 to 2.42551, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 2.4357 - acc: 0.1068 - val_loss: 2.4255 - val_acc: 0.1096
Epoch 5/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4345 - acc: 0.1087Epoch 00005: val_loss improved from 2.42551 to 2.42476, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 2.4327 - acc: 0.1093 - val_loss: 2.4248 - val_acc: 0.1096
Epoch 6/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4289 - acc: 0.1089- ETA: 1s - loss: 2.4298Epoch 00006: val_loss improved from 2.42476 to 2.42407, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 2.4296 - acc: 0.1089 - val_loss: 2.4241 - val_acc: 0.1096
Epoch 7/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4282 - acc: 0.1089- ETA: 0s - loss: 2.4279 - acc: 0Epoch 00007: val_loss improved from 2.42407 to 2.42345, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 2.4292 - acc: 0.1088 - val_loss: 2.4234 - val_acc: 0.1096
Epoch 8/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4284 - acc: 0.1089Epoch 00008: val_loss improved from 2.42345 to 2.42291, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 2.4291 - acc: 0.1090 - val_loss: 2.4229 - val_acc: 0.1096
Epoch 9/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4262 - acc: 0.1080Epoch 00009: val_loss improved from 2.42291 to 2.42241, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 95us/step - loss: 2.4266 - acc: 0.1081 - val_loss: 2.4224 - val_acc: 0.1096
Epoch 10/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4266 - acc: 0.1092Epoch 00010: val_loss improved from 2.42241 to 2.42202, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 2.4273 - acc: 0.1085 - val_loss: 2.4220 - val_acc: 0.1096
Epoch 11/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4265 - acc: 0.1093Epoch 00011: val_loss improved from 2.42202 to 2.42165, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 91us/step - loss: 2.4271 - acc: 0.1084 - val_loss: 2.4217 - val_acc: 0.1096
Epoch 12/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4261 - acc: 0.1090Epoch 00012: val_loss improved from 2.42165 to 2.42136, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 91us/step - loss: 2.4255 - acc: 0.1089 - val_loss: 2.4214 - val_acc: 0.1096
Epoch 13/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4262 - acc: 0.1096Epoch 00013: val_loss improved from 2.42136 to 2.42105, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 91us/step - loss: 2.4257 - acc: 0.1097 - val_loss: 2.4211 - val_acc: 0.1096
Epoch 14/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4246 - acc: 0.1082Epoch 00014: val_loss improved from 2.42105 to 2.42083, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 91us/step - loss: 2.4247 - acc: 0.1081 - val_loss: 2.4208 - val_acc: 0.1096
Epoch 15/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4247 - acc: 0.1076Epoch 00015: val_loss improved from 2.42083 to 2.42060, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 2.4248 - acc: 0.1073 - val_loss: 2.4206 - val_acc: 0.1096
Epoch 16/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4236 - acc: 0.1077Epoch 00016: val_loss improved from 2.42060 to 2.42013, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 2.4242 - acc: 0.1075 - val_loss: 2.4201 - val_acc: 0.1102
Epoch 17/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4247 - acc: 0.1088Epoch 00017: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 2.4246 - acc: 0.1085 - val_loss: 2.4203 - val_acc: 0.1102
Epoch 18/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4233 - acc: 0.1110Epoch 00018: val_loss did not improve
17892/17892 [==============================] - 2s 90us/step - loss: 2.4235 - acc: 0.1104 - val_loss: 2.4201 - val_acc: 0.1102
Epoch 19/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4240 - acc: 0.1102Epoch 00019: val_loss improved from 2.42013 to 2.41994, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 2.4249 - acc: 0.1100 - val_loss: 2.4199 - val_acc: 0.1107
Epoch 20/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.4204 - acc: 0.1166Epoch 00020: val_loss improved from 2.41994 to 2.39824, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 2.4202 - acc: 0.1168 - val_loss: 2.3982 - val_acc: 0.1437
Epoch 21/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.3995 - acc: 0.1311Epoch 00021: val_loss improved from 2.39824 to 2.34581, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 2.3998 - acc: 0.1302 - val_loss: 2.3458 - val_acc: 0.1510
Epoch 22/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.3551 - acc: 0.1465Epoch 00022: val_loss improved from 2.34581 to 2.24024, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 2.3529 - acc: 0.1487 - val_loss: 2.2402 - val_acc: 0.1600
Epoch 23/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.2577 - acc: 0.1802Epoch 00023: val_loss improved from 2.24024 to 2.10342, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 2.2548 - acc: 0.1828 - val_loss: 2.1034 - val_acc: 0.2713
Epoch 24/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 2.0940 - acc: 0.2568Epoch 00024: val_loss improved from 2.10342 to 1.83838, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 2.0927 - acc: 0.2575 - val_loss: 1.8384 - val_acc: 0.3937
Epoch 25/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 1.9003 - acc: 0.3258Epoch 00025: val_loss improved from 1.83838 to 1.64870, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 1.8999 - acc: 0.3261 - val_loss: 1.6487 - val_acc: 0.4374
Epoch 26/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 1.7581 - acc: 0.3805Epoch 00026: val_loss improved from 1.64870 to 1.46524, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 1.7525 - acc: 0.3821 - val_loss: 1.4652 - val_acc: 0.5067
Epoch 27/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 1.5982 - acc: 0.4362Epoch 00027: val_loss improved from 1.46524 to 1.23017, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 1.5926 - acc: 0.4384 - val_loss: 1.2302 - val_acc: 0.5984
Epoch 28/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 1.4543 - acc: 0.4924Epoch 00028: val_loss improved from 1.23017 to 1.12658, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 1.4512 - acc: 0.4935 - val_loss: 1.1266 - val_acc: 0.6253
Epoch 29/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 1.3281 - acc: 0.5342Epoch 00029: val_loss improved from 1.12658 to 0.98038, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 1.3265 - acc: 0.5355 - val_loss: 0.9804 - val_acc: 0.6924
Epoch 30/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 1.2412 - acc: 0.5672Epoch 00030: val_loss improved from 0.98038 to 0.89696, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 1.2358 - acc: 0.5684 - val_loss: 0.8970 - val_acc: 0.7097
Epoch 31/250
17664/17892 [============================&gt;.] - ETA: 0s - loss: 1.1638 - acc: 0.5962Epoch 00031: val_loss improved from 0.89696 to 0.86415, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 1.1628 - acc: 0.5962 - val_loss: 0.8641 - val_acc: 0.7181
Epoch 32/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 1.0871 - acc: 0.6266Epoch 00032: val_loss improved from 0.86415 to 0.78495, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 1.0879 - acc: 0.6261 - val_loss: 0.7850 - val_acc: 0.7500
Epoch 33/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 1.0411 - acc: 0.6416Epoch 00033: val_loss improved from 0.78495 to 0.74663, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 1.0412 - acc: 0.6409 - val_loss: 0.7466 - val_acc: 0.7578
Epoch 34/250
17408/17892 [============================&gt;.] - ETA: 0s - loss: 0.9983 - acc: 0.6583Epoch 00034: val_loss improved from 0.74663 to 0.72455, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 95us/step - loss: 0.9975 - acc: 0.6588 - val_loss: 0.7246 - val_acc: 0.7662
Epoch 35/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.9678 - acc: 0.6667Epoch 00035: val_loss improved from 0.72455 to 0.67433, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 95us/step - loss: 0.9670 - acc: 0.6673 - val_loss: 0.6743 - val_acc: 0.7875
Epoch 36/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.9297 - acc: 0.6778Epoch 00036: val_loss improved from 0.67433 to 0.65697, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 0.9279 - acc: 0.6780 - val_loss: 0.6570 - val_acc: 0.7891
Epoch 37/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.8990 - acc: 0.6926Epoch 00037: val_loss improved from 0.65697 to 0.61635, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.8973 - acc: 0.6927 - val_loss: 0.6163 - val_acc: 0.7987
Epoch 38/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.8797 - acc: 0.6988Epoch 00038: val_loss improved from 0.61635 to 0.60672, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 0.8785 - acc: 0.6990 - val_loss: 0.6067 - val_acc: 0.8065
Epoch 39/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.8400 - acc: 0.7130Epoch 00039: val_loss improved from 0.60672 to 0.57848, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.8423 - acc: 0.7115 - val_loss: 0.5785 - val_acc: 0.8210
Epoch 40/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.8091 - acc: 0.7247Epoch 00040: val_loss improved from 0.57848 to 0.56156, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.8113 - acc: 0.7240 - val_loss: 0.5616 - val_acc: 0.8138
Epoch 41/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.7902 - acc: 0.7261Epoch 00041: val_loss improved from 0.56156 to 0.54578, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.7880 - acc: 0.7269 - val_loss: 0.5458 - val_acc: 0.8244
Epoch 42/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.7683 - acc: 0.7406Epoch 00042: val_loss improved from 0.54578 to 0.52331, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 0.7701 - acc: 0.7400 - val_loss: 0.5233 - val_acc: 0.8305
Epoch 43/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.7684 - acc: 0.7401Epoch 00043: val_loss improved from 0.52331 to 0.51217, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.7681 - acc: 0.7404 - val_loss: 0.5122 - val_acc: 0.8328
Epoch 44/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.7478 - acc: 0.7501Epoch 00044: val_loss improved from 0.51217 to 0.49582, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.7457 - acc: 0.7507 - val_loss: 0.4958 - val_acc: 0.8412
Epoch 45/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.7204 - acc: 0.7540Epoch 00045: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.7204 - acc: 0.7532 - val_loss: 0.4964 - val_acc: 0.8423
Epoch 46/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.7140 - acc: 0.7575Epoch 00046: val_loss improved from 0.49582 to 0.48072, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.7131 - acc: 0.7580 - val_loss: 0.4807 - val_acc: 0.8523
Epoch 47/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.7106 - acc: 0.7589Epoch 00047: val_loss improved from 0.48072 to 0.46329, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 0.7093 - acc: 0.7599 - val_loss: 0.4633 - val_acc: 0.8490
Epoch 48/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.6828 - acc: 0.7681Epoch 00048: val_loss improved from 0.46329 to 0.46274, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.6842 - acc: 0.7673 - val_loss: 0.4627 - val_acc: 0.8501
Epoch 49/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.6626 - acc: 0.7734Epoch 00049: val_loss improved from 0.46274 to 0.45021, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.6650 - acc: 0.7726 - val_loss: 0.4502 - val_acc: 0.8512
Epoch 50/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.6484 - acc: 0.7820Epoch 00050: val_loss improved from 0.45021 to 0.43355, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.6492 - acc: 0.7821 - val_loss: 0.4335 - val_acc: 0.8652
Epoch 51/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.6462 - acc: 0.7845Epoch 00051: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.6436 - acc: 0.7849 - val_loss: 0.4443 - val_acc: 0.8568
Epoch 52/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.6329 - acc: 0.7881Epoch 00052: val_loss improved from 0.43355 to 0.42519, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 95us/step - loss: 0.6312 - acc: 0.7887 - val_loss: 0.4252 - val_acc: 0.8652
Epoch 53/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.6266 - acc: 0.7845Epoch 00053: val_loss improved from 0.42519 to 0.42393, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 0.6269 - acc: 0.7849 - val_loss: 0.4239 - val_acc: 0.8591
Epoch 54/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.6128 - acc: 0.7909Epoch 00054: val_loss improved from 0.42393 to 0.41147, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 0.6135 - acc: 0.7908 - val_loss: 0.4115 - val_acc: 0.8742
Epoch 55/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.6100 - acc: 0.7946Epoch 00055: val_loss improved from 0.41147 to 0.40497, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.6085 - acc: 0.7950 - val_loss: 0.4050 - val_acc: 0.8708
Epoch 56/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.5929 - acc: 0.7991Epoch 00056: val_loss improved from 0.40497 to 0.39815, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 0.5927 - acc: 0.7994 - val_loss: 0.3982 - val_acc: 0.8736
Epoch 57/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.5912 - acc: 0.8021Epoch 00057: val_loss improved from 0.39815 to 0.39489, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.5894 - acc: 0.8033 - val_loss: 0.3949 - val_acc: 0.8725
Epoch 58/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.5812 - acc: 0.8029Epoch 00058: val_loss improved from 0.39489 to 0.39036, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.5801 - acc: 0.8031 - val_loss: 0.3904 - val_acc: 0.8736
Epoch 59/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.5769 - acc: 0.8054Epoch 00059: val_loss improved from 0.39036 to 0.38680, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.5781 - acc: 0.8049 - val_loss: 0.3868 - val_acc: 0.8747
Epoch 60/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.5636 - acc: 0.8110Epoch 00060: val_loss improved from 0.38680 to 0.38165, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 0.5643 - acc: 0.8103 - val_loss: 0.3817 - val_acc: 0.8831
Epoch 61/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.5521 - acc: 0.8148Epoch 00061: val_loss improved from 0.38165 to 0.38073, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 0.5527 - acc: 0.8140 - val_loss: 0.3807 - val_acc: 0.8820
Epoch 62/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.5505 - acc: 0.8145Epoch 00062: val_loss improved from 0.38073 to 0.36734, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 0.5483 - acc: 0.8146 - val_loss: 0.3673 - val_acc: 0.8809
Epoch 63/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.5381 - acc: 0.8182- ETA: 1s - loss: 0.541Epoch 00063: val_loss did not improve
17892/17892 [==============================] - 2s 90us/step - loss: 0.5400 - acc: 0.8180 - val_loss: 0.3718 - val_acc: 0.8831
Epoch 64/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.5455 - acc: 0.8123Epoch 00064: val_loss did not improve
17892/17892 [==============================] - 2s 90us/step - loss: 0.5478 - acc: 0.8114 - val_loss: 0.3715 - val_acc: 0.8831
Epoch 65/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.5258 - acc: 0.8259Epoch 00065: val_loss improved from 0.36734 to 0.36351, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 0.5251 - acc: 0.8258 - val_loss: 0.3635 - val_acc: 0.8820
Epoch 66/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.5218 - acc: 0.8245Epoch 00066: val_loss improved from 0.36351 to 0.35524, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 0.5216 - acc: 0.8242 - val_loss: 0.3552 - val_acc: 0.8848
Epoch 67/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.5178 - acc: 0.8281Epoch 00067: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.5163 - acc: 0.8285 - val_loss: 0.3607 - val_acc: 0.8798
Epoch 68/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.5006 - acc: 0.8312Epoch 00068: val_loss improved from 0.35524 to 0.34860, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 0.5008 - acc: 0.8312 - val_loss: 0.3486 - val_acc: 0.8853
Epoch 69/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.5058 - acc: 0.8266Epoch 00069: val_loss did not improve
17892/17892 [==============================] - 2s 90us/step - loss: 0.5049 - acc: 0.8262 - val_loss: 0.3497 - val_acc: 0.8831
Epoch 70/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.5005 - acc: 0.8344Epoch 00070: val_loss improved from 0.34860 to 0.34006, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 91us/step - loss: 0.4997 - acc: 0.8340 - val_loss: 0.3401 - val_acc: 0.8898
Epoch 71/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4880 - acc: 0.8355Epoch 00071: val_loss improved from 0.34006 to 0.33795, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 0.4861 - acc: 0.8358 - val_loss: 0.3380 - val_acc: 0.8898
Epoch 72/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4969 - acc: 0.8331Epoch 00072: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.4948 - acc: 0.8341 - val_loss: 0.3481 - val_acc: 0.8820
Epoch 73/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4730 - acc: 0.8408Epoch 00073: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.4741 - acc: 0.8402 - val_loss: 0.3433 - val_acc: 0.8876
Epoch 74/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4757 - acc: 0.8375Epoch 00074: val_loss improved from 0.33795 to 0.32800, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.4760 - acc: 0.8377 - val_loss: 0.3280 - val_acc: 0.8909
Epoch 75/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4614 - acc: 0.8446Epoch 00075: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.4632 - acc: 0.8440 - val_loss: 0.3325 - val_acc: 0.8926
Epoch 76/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4602 - acc: 0.8430Epoch 00076: val_loss improved from 0.32800 to 0.32781, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.4611 - acc: 0.8430 - val_loss: 0.3278 - val_acc: 0.8904
Epoch 77/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4609 - acc: 0.8438Epoch 00077: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.4604 - acc: 0.8445 - val_loss: 0.3420 - val_acc: 0.8865
Epoch 78/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4507 - acc: 0.8466Epoch 00078: val_loss improved from 0.32781 to 0.31938, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.4517 - acc: 0.8459 - val_loss: 0.3194 - val_acc: 0.8977
Epoch 79/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4525 - acc: 0.8471Epoch 00079: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.4537 - acc: 0.8470 - val_loss: 0.3246 - val_acc: 0.8937
Epoch 80/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4447 - acc: 0.8496Epoch 00080: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.4452 - acc: 0.8493 - val_loss: 0.3321 - val_acc: 0.8949
Epoch 81/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4442 - acc: 0.8534Epoch 00081: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.4421 - acc: 0.8541 - val_loss: 0.3288 - val_acc: 0.8887
Epoch 82/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4358 - acc: 0.8522Epoch 00082: val_loss improved from 0.31938 to 0.31497, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.4376 - acc: 0.8516 - val_loss: 0.3150 - val_acc: 0.8943
Epoch 83/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4348 - acc: 0.8552Epoch 00083: val_loss improved from 0.31497 to 0.31148, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.4375 - acc: 0.8541 - val_loss: 0.3115 - val_acc: 0.9004
Epoch 84/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4397 - acc: 0.8529Epoch 00084: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.4421 - acc: 0.8526 - val_loss: 0.3124 - val_acc: 0.8954
Epoch 85/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4289 - acc: 0.8565Epoch 00085: val_loss improved from 0.31148 to 0.30825, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.4277 - acc: 0.8571 - val_loss: 0.3083 - val_acc: 0.8971
Epoch 86/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4233 - acc: 0.8574Epoch 00086: val_loss improved from 0.30825 to 0.30517, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.4218 - acc: 0.8579 - val_loss: 0.3052 - val_acc: 0.9004
Epoch 87/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4205 - acc: 0.8576Epoch 00087: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.4201 - acc: 0.8574 - val_loss: 0.3110 - val_acc: 0.8949
Epoch 88/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4134 - acc: 0.8615Epoch 00088: val_loss improved from 0.30517 to 0.29864, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.4138 - acc: 0.8618 - val_loss: 0.2986 - val_acc: 0.9010
Epoch 89/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4096 - acc: 0.8600Epoch 00089: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.4119 - acc: 0.8597 - val_loss: 0.3016 - val_acc: 0.9004
Epoch 90/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4156 - acc: 0.8582Epoch 00090: val_loss improved from 0.29864 to 0.29836, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 0.4150 - acc: 0.8579 - val_loss: 0.2984 - val_acc: 0.9004
Epoch 91/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4130 - acc: 0.8626Epoch 00091: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.4134 - acc: 0.8626 - val_loss: 0.3010 - val_acc: 0.9004
Epoch 92/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4145 - acc: 0.8609Epoch 00092: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.4126 - acc: 0.8612 - val_loss: 0.2994 - val_acc: 0.9010
Epoch 93/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.4009 - acc: 0.8649Epoch 00093: val_loss improved from 0.29836 to 0.28923, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.3990 - acc: 0.8655 - val_loss: 0.2892 - val_acc: 0.9066
Epoch 94/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3966 - acc: 0.8649Epoch 00094: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3973 - acc: 0.8646 - val_loss: 0.2970 - val_acc: 0.9010
Epoch 95/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3934 - acc: 0.8608Epoch 00095: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.3958 - acc: 0.8608 - val_loss: 0.2932 - val_acc: 0.8982
Epoch 96/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3894 - acc: 0.8662Epoch 00096: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.3892 - acc: 0.8663 - val_loss: 0.2897 - val_acc: 0.9072
Epoch 97/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3864 - acc: 0.8689Epoch 00097: val_loss improved from 0.28923 to 0.28530, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.3885 - acc: 0.8687 - val_loss: 0.2853 - val_acc: 0.9055
Epoch 98/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3899 - acc: 0.8678Epoch 00098: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.3893 - acc: 0.8679 - val_loss: 0.2889 - val_acc: 0.9027
Epoch 99/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3921 - acc: 0.8686Epoch 00099: val_loss improved from 0.28530 to 0.27915, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 0.3931 - acc: 0.8684 - val_loss: 0.2792 - val_acc: 0.9066
Epoch 100/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3806 - acc: 0.8691Epoch 00100: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.3793 - acc: 0.8693 - val_loss: 0.2830 - val_acc: 0.9010
Epoch 101/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3764 - acc: 0.8732Epoch 00101: val_loss improved from 0.27915 to 0.27747, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 0.3743 - acc: 0.8737 - val_loss: 0.2775 - val_acc: 0.9077
Epoch 102/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3703 - acc: 0.8750Epoch 00102: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.3706 - acc: 0.8750 - val_loss: 0.2825 - val_acc: 0.9004
Epoch 103/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3674 - acc: 0.8771Epoch 00103: val_loss did not improve
17892/17892 [==============================] - 2s 93us/step - loss: 0.3683 - acc: 0.8773 - val_loss: 0.2834 - val_acc: 0.8999
Epoch 104/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3699 - acc: 0.8760Epoch 00104: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.3703 - acc: 0.8758 - val_loss: 0.2864 - val_acc: 0.8954
Epoch 105/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3677 - acc: 0.8755Epoch 00105: val_loss improved from 0.27747 to 0.27488, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.3665 - acc: 0.8760 - val_loss: 0.2749 - val_acc: 0.9088
Epoch 106/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3591 - acc: 0.8777Epoch 00106: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3583 - acc: 0.8779 - val_loss: 0.2751 - val_acc: 0.9060
Epoch 107/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3621 - acc: 0.8788Epoch 00107: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.3601 - acc: 0.8794 - val_loss: 0.2832 - val_acc: 0.9060
Epoch 108/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3503 - acc: 0.8833Epoch 00108: val_loss improved from 0.27488 to 0.26530, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 0.3499 - acc: 0.8832 - val_loss: 0.2653 - val_acc: 0.9105
Epoch 109/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3612 - acc: 0.8772Epoch 00109: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3606 - acc: 0.8772 - val_loss: 0.2680 - val_acc: 0.9088
Epoch 110/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3569 - acc: 0.8787Epoch 00110: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3568 - acc: 0.8785 - val_loss: 0.2660 - val_acc: 0.9100
Epoch 111/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3558 - acc: 0.8786Epoch 00111: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3548 - acc: 0.8788 - val_loss: 0.2701 - val_acc: 0.9044
Epoch 112/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3529 - acc: 0.8813Epoch 00112: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3543 - acc: 0.8806 - val_loss: 0.2725 - val_acc: 0.9111
Epoch 113/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3456 - acc: 0.8837Epoch 00113: val_loss improved from 0.26530 to 0.26363, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.3483 - acc: 0.8830 - val_loss: 0.2636 - val_acc: 0.9128
Epoch 114/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3450 - acc: 0.8834Epoch 00114: val_loss did not improve
17892/17892 [==============================] - 2s 93us/step - loss: 0.3471 - acc: 0.8829 - val_loss: 0.2725 - val_acc: 0.9066
Epoch 115/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3419 - acc: 0.8851Epoch 00115: val_loss improved from 0.26363 to 0.25988, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.3406 - acc: 0.8855 - val_loss: 0.2599 - val_acc: 0.9139
Epoch 116/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3419 - acc: 0.8821Epoch 00116: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.3415 - acc: 0.8826 - val_loss: 0.2615 - val_acc: 0.9178
Epoch 117/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3353 - acc: 0.8869Epoch 00117: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.3345 - acc: 0.8870 - val_loss: 0.2648 - val_acc: 0.9122
Epoch 118/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3382 - acc: 0.8861Epoch 00118: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.3381 - acc: 0.8863 - val_loss: 0.2623 - val_acc: 0.9116
Epoch 119/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3363 - acc: 0.8867Epoch 00119: val_loss improved from 0.25988 to 0.25887, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.3344 - acc: 0.8878 - val_loss: 0.2589 - val_acc: 0.9122
Epoch 120/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3401 - acc: 0.8832Epoch 00120: val_loss improved from 0.25887 to 0.25251, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 0.3404 - acc: 0.8827 - val_loss: 0.2525 - val_acc: 0.9139
Epoch 121/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3271 - acc: 0.8892Epoch 00121: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3293 - acc: 0.8884 - val_loss: 0.2596 - val_acc: 0.9116
Epoch 122/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3248 - acc: 0.8916Epoch 00122: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3257 - acc: 0.8910 - val_loss: 0.2640 - val_acc: 0.9100
Epoch 123/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3248 - acc: 0.8888Epoch 00123: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3239 - acc: 0.8893 - val_loss: 0.2543 - val_acc: 0.9150
Epoch 124/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3347 - acc: 0.8867Epoch 00124: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3328 - acc: 0.8869 - val_loss: 0.2536 - val_acc: 0.9111
Epoch 125/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3304 - acc: 0.8870Epoch 00125: val_loss improved from 0.25251 to 0.24613, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 0.3290 - acc: 0.8877 - val_loss: 0.2461 - val_acc: 0.9189
Epoch 126/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3197 - acc: 0.8899Epoch 00126: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.3213 - acc: 0.8889 - val_loss: 0.2606 - val_acc: 0.9100
Epoch 127/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3246 - acc: 0.8902Epoch 00127: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3253 - acc: 0.8898 - val_loss: 0.2530 - val_acc: 0.9100
Epoch 128/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3285 - acc: 0.8875Epoch 00128: val_loss improved from 0.24613 to 0.24604, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 0.3282 - acc: 0.8873 - val_loss: 0.2460 - val_acc: 0.9139
Epoch 129/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3262 - acc: 0.8892Epoch 00129: val_loss improved from 0.24604 to 0.24550, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 95us/step - loss: 0.3246 - acc: 0.8894 - val_loss: 0.2455 - val_acc: 0.9150
Epoch 130/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3174 - acc: 0.8925Epoch 00130: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.3177 - acc: 0.8920 - val_loss: 0.2548 - val_acc: 0.9094
Epoch 131/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3195 - acc: 0.8920Epoch 00131: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.3200 - acc: 0.8922 - val_loss: 0.2487 - val_acc: 0.9150
Epoch 132/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3151 - acc: 0.8926Epoch 00132: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.3146 - acc: 0.8929 - val_loss: 0.2482 - val_acc: 0.9122
Epoch 133/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3070 - acc: 0.8964Epoch 00133: val_loss improved from 0.24550 to 0.24444, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.3087 - acc: 0.8965 - val_loss: 0.2444 - val_acc: 0.9195
Epoch 134/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3102 - acc: 0.8935Epoch 00134: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.3090 - acc: 0.8945 - val_loss: 0.2473 - val_acc: 0.9189
Epoch 135/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3090 - acc: 0.8943Epoch 00135: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3102 - acc: 0.8941 - val_loss: 0.2513 - val_acc: 0.9116
Epoch 136/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3112 - acc: 0.8951Epoch 00136: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3109 - acc: 0.8955 - val_loss: 0.2523 - val_acc: 0.9128
Epoch 137/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3126 - acc: 0.8958Epoch 00137: val_loss improved from 0.24444 to 0.23832, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.3125 - acc: 0.8959 - val_loss: 0.2383 - val_acc: 0.9128
Epoch 138/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3032 - acc: 0.8939Epoch 00138: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3019 - acc: 0.8946 - val_loss: 0.2467 - val_acc: 0.9122
Epoch 139/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3003 - acc: 0.8977Epoch 00139: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3027 - acc: 0.8973 - val_loss: 0.2454 - val_acc: 0.9139
Epoch 140/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3001 - acc: 0.8965Epoch 00140: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3002 - acc: 0.8965 - val_loss: 0.2468 - val_acc: 0.9128
Epoch 141/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3057 - acc: 0.8979Epoch 00141: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.3053 - acc: 0.8979 - val_loss: 0.2451 - val_acc: 0.9155
Epoch 142/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2941 - acc: 0.8999Epoch 00142: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2957 - acc: 0.8990 - val_loss: 0.2498 - val_acc: 0.9155
Epoch 143/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2942 - acc: 0.8981Epoch 00143: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2940 - acc: 0.8987 - val_loss: 0.2451 - val_acc: 0.9144
Epoch 144/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.3018 - acc: 0.8972Epoch 00144: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.3012 - acc: 0.8976 - val_loss: 0.2466 - val_acc: 0.9150
Epoch 145/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2964 - acc: 0.8975Epoch 00145: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2950 - acc: 0.8980 - val_loss: 0.2460 - val_acc: 0.9139
Epoch 146/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2925 - acc: 0.9004Epoch 00146: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2920 - acc: 0.9005 - val_loss: 0.2419 - val_acc: 0.9183
Epoch 147/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2904 - acc: 0.9017Epoch 00147: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2921 - acc: 0.9011 - val_loss: 0.2467 - val_acc: 0.9133
Epoch 148/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2994 - acc: 0.9008Epoch 00148: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2998 - acc: 0.9004 - val_loss: 0.2413 - val_acc: 0.9167
Epoch 149/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2864 - acc: 0.9040Epoch 00149: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2868 - acc: 0.9035 - val_loss: 0.2554 - val_acc: 0.9116
Epoch 150/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2917 - acc: 0.9014Epoch 00150: val_loss improved from 0.23832 to 0.23743, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.2904 - acc: 0.9019 - val_loss: 0.2374 - val_acc: 0.9183
Epoch 151/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2840 - acc: 0.9043Epoch 00151: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2840 - acc: 0.9041 - val_loss: 0.2428 - val_acc: 0.9172
Epoch 152/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2904 - acc: 0.8987Epoch 00152: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2922 - acc: 0.8983 - val_loss: 0.2375 - val_acc: 0.9150
Epoch 153/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2833 - acc: 0.9029Epoch 00153: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2853 - acc: 0.9025 - val_loss: 0.2439 - val_acc: 0.9144
Epoch 154/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2982 - acc: 0.8994Epoch 00154: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2966 - acc: 0.8996 - val_loss: 0.2398 - val_acc: 0.9200
Epoch 155/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2790 - acc: 0.9058Epoch 00155: val_loss improved from 0.23743 to 0.22930, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.2813 - acc: 0.9048 - val_loss: 0.2293 - val_acc: 0.9195
Epoch 156/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2814 - acc: 0.9040Epoch 00156: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2803 - acc: 0.9045 - val_loss: 0.2317 - val_acc: 0.9195
Epoch 157/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2803 - acc: 0.9018Epoch 00157: val_loss did not improve
17892/17892 [==============================] - 2s 90us/step - loss: 0.2790 - acc: 0.9020 - val_loss: 0.2366 - val_acc: 0.9183
Epoch 158/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2732 - acc: 0.9059Epoch 00158: val_loss did not improve
17892/17892 [==============================] - 2s 90us/step - loss: 0.2748 - acc: 0.9054 - val_loss: 0.2338 - val_acc: 0.9189
Epoch 159/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2792 - acc: 0.9067Epoch 00159: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2774 - acc: 0.9071 - val_loss: 0.2359 - val_acc: 0.9161
Epoch 160/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2762 - acc: 0.9078Epoch 00160: val_loss did not improve
17892/17892 [==============================] - 2s 90us/step - loss: 0.2764 - acc: 0.9079 - val_loss: 0.2351 - val_acc: 0.9150
Epoch 161/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2783 - acc: 0.9072Epoch 00161: val_loss did not improve
17892/17892 [==============================] - 2s 90us/step - loss: 0.2810 - acc: 0.9062 - val_loss: 0.2381 - val_acc: 0.9167
Epoch 162/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2745 - acc: 0.9067Epoch 00162: val_loss improved from 0.22930 to 0.22640, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 0.2754 - acc: 0.9070 - val_loss: 0.2264 - val_acc: 0.9211
Epoch 163/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2754 - acc: 0.9059Epoch 00163: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2752 - acc: 0.9058 - val_loss: 0.2383 - val_acc: 0.9200
Epoch 164/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2771 - acc: 0.9038Epoch 00164: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2750 - acc: 0.9045 - val_loss: 0.2356 - val_acc: 0.9150
Epoch 165/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2757 - acc: 0.9053Epoch 00165: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2772 - acc: 0.9056 - val_loss: 0.2369 - val_acc: 0.9155
Epoch 166/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2661 - acc: 0.9073Epoch 00166: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2695 - acc: 0.9069 - val_loss: 0.2369 - val_acc: 0.9144
Epoch 167/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2629 - acc: 0.9119Epoch 00167: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2625 - acc: 0.9122 - val_loss: 0.2352 - val_acc: 0.9183
Epoch 168/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2691 - acc: 0.9080Epoch 00168: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2690 - acc: 0.9076 - val_loss: 0.2290 - val_acc: 0.9189
Epoch 169/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2775 - acc: 0.9065Epoch 00169: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2771 - acc: 0.9066 - val_loss: 0.2424 - val_acc: 0.9122
Epoch 170/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2637 - acc: 0.9085Epoch 00170: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2624 - acc: 0.9093 - val_loss: 0.2416 - val_acc: 0.9178
Epoch 171/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2677 - acc: 0.9102Epoch 00171: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2668 - acc: 0.9105 - val_loss: 0.2370 - val_acc: 0.9167
Epoch 172/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2612 - acc: 0.9116Epoch 00172: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2624 - acc: 0.9111 - val_loss: 0.2354 - val_acc: 0.9161
Epoch 173/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2654 - acc: 0.9104Epoch 00173: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2645 - acc: 0.9105 - val_loss: 0.2401 - val_acc: 0.9189
Epoch 174/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2664 - acc: 0.9074Epoch 00174: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2678 - acc: 0.9067 - val_loss: 0.2361 - val_acc: 0.9167
Epoch 175/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2617 - acc: 0.9091Epoch 00175: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2635 - acc: 0.9090 - val_loss: 0.2363 - val_acc: 0.9178
Epoch 176/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2623 - acc: 0.9102Epoch 00176: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2626 - acc: 0.9100 - val_loss: 0.2356 - val_acc: 0.9183
Epoch 177/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2606 - acc: 0.9114Epoch 00177: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2634 - acc: 0.9107 - val_loss: 0.2383 - val_acc: 0.9217
Epoch 178/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2638 - acc: 0.9112Epoch 00178: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2643 - acc: 0.9115 - val_loss: 0.2324 - val_acc: 0.9195
Epoch 179/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2634 - acc: 0.9104Epoch 00179: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2638 - acc: 0.9106 - val_loss: 0.2289 - val_acc: 0.9206
Epoch 180/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2597 - acc: 0.9109Epoch 00180: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2604 - acc: 0.9107 - val_loss: 0.2356 - val_acc: 0.9155
Epoch 181/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2560 - acc: 0.9107Epoch 00181: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2552 - acc: 0.9105 - val_loss: 0.2357 - val_acc: 0.9200
Epoch 182/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2549 - acc: 0.9131Epoch 00182: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2544 - acc: 0.9134 - val_loss: 0.2271 - val_acc: 0.9195
Epoch 183/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2525 - acc: 0.9123Epoch 00183: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2534 - acc: 0.9125 - val_loss: 0.2387 - val_acc: 0.9111
Epoch 184/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2411 - acc: 0.9179Epoch 00184: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2436 - acc: 0.9171 - val_loss: 0.2347 - val_acc: 0.9217
Epoch 185/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2463 - acc: 0.9171Epoch 00185: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2462 - acc: 0.9172 - val_loss: 0.2348 - val_acc: 0.9167
Epoch 186/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2521 - acc: 0.9145Epoch 00186: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2517 - acc: 0.9144 - val_loss: 0.2324 - val_acc: 0.9155
Epoch 187/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2562 - acc: 0.9107Epoch 00187: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2568 - acc: 0.9103 - val_loss: 0.2341 - val_acc: 0.9161
Epoch 188/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2450 - acc: 0.9167Epoch 00188: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2467 - acc: 0.9162 - val_loss: 0.2318 - val_acc: 0.9200
Epoch 189/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2500 - acc: 0.9137Epoch 00189: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2479 - acc: 0.9145 - val_loss: 0.2310 - val_acc: 0.9167
Epoch 190/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2379 - acc: 0.9187Epoch 00190: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2398 - acc: 0.9182 - val_loss: 0.2305 - val_acc: 0.9183
Epoch 191/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2463 - acc: 0.9156Epoch 00191: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2464 - acc: 0.9153 - val_loss: 0.2318 - val_acc: 0.9178
Epoch 192/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2445 - acc: 0.9153Epoch 00192: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2461 - acc: 0.9147 - val_loss: 0.2285 - val_acc: 0.9206
Epoch 193/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2461 - acc: 0.9179Epoch 00193: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2454 - acc: 0.9181 - val_loss: 0.2345 - val_acc: 0.9167
Epoch 194/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2413 - acc: 0.9162Epoch 00194: val_loss improved from 0.22640 to 0.22431, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.2431 - acc: 0.9155 - val_loss: 0.2243 - val_acc: 0.9251
Epoch 195/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2471 - acc: 0.9165Epoch 00195: val_loss improved from 0.22431 to 0.22401, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.2459 - acc: 0.9166 - val_loss: 0.2240 - val_acc: 0.9245
Epoch 196/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2354 - acc: 0.9183Epoch 00196: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2343 - acc: 0.9186 - val_loss: 0.2279 - val_acc: 0.9206
Epoch 197/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2481 - acc: 0.9154Epoch 00197: val_loss improved from 0.22401 to 0.22383, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 0.2495 - acc: 0.9148 - val_loss: 0.2238 - val_acc: 0.9217
Epoch 198/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2458 - acc: 0.9153- ETA: 0s - loss: 0.2484 - acc: Epoch 00198: val_loss did not improve
17892/17892 [==============================] - 2s 90us/step - loss: 0.2444 - acc: 0.9159 - val_loss: 0.2302 - val_acc: 0.9167
Epoch 199/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2330 - acc: 0.9177Epoch 00199: val_loss did not improve
17892/17892 [==============================] - 2s 90us/step - loss: 0.2330 - acc: 0.9180 - val_loss: 0.2330 - val_acc: 0.9178
Epoch 200/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2415 - acc: 0.9178Epoch 00200: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2436 - acc: 0.9175 - val_loss: 0.2314 - val_acc: 0.9195
Epoch 201/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2404 - acc: 0.9183Epoch 00201: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2395 - acc: 0.9183 - val_loss: 0.2351 - val_acc: 0.9172
Epoch 202/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2454 - acc: 0.9183Epoch 00202: val_loss did not improve
17892/17892 [==============================] - 2s 93us/step - loss: 0.2452 - acc: 0.9184 - val_loss: 0.2321 - val_acc: 0.9195
Epoch 203/250
17664/17892 [============================&gt;.] - ETA: 0s - loss: 0.2354 - acc: 0.9171Epoch 00203: val_loss improved from 0.22383 to 0.22112, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 95us/step - loss: 0.2353 - acc: 0.9169 - val_loss: 0.2211 - val_acc: 0.9251
Epoch 204/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2336 - acc: 0.9200Epoch 00204: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2341 - acc: 0.9194 - val_loss: 0.2250 - val_acc: 0.9217
Epoch 205/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2309 - acc: 0.9207Epoch 00205: val_loss improved from 0.22112 to 0.21783, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 0.2312 - acc: 0.9202 - val_loss: 0.2178 - val_acc: 0.9251
Epoch 206/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2344 - acc: 0.9191Epoch 00206: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2363 - acc: 0.9186 - val_loss: 0.2261 - val_acc: 0.9228
Epoch 207/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2376 - acc: 0.9201Epoch 00207: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2391 - acc: 0.9193 - val_loss: 0.2270 - val_acc: 0.9211
Epoch 208/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2374 - acc: 0.9194Epoch 00208: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2369 - acc: 0.9197 - val_loss: 0.2232 - val_acc: 0.9223
Epoch 209/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2375 - acc: 0.9176Epoch 00209: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2367 - acc: 0.9177 - val_loss: 0.2224 - val_acc: 0.9239
Epoch 210/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2374 - acc: 0.9181Epoch 00210: val_loss improved from 0.21783 to 0.21742, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 0.2355 - acc: 0.9188 - val_loss: 0.2174 - val_acc: 0.9301
Epoch 211/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2353 - acc: 0.9217Epoch 00211: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2364 - acc: 0.9209 - val_loss: 0.2218 - val_acc: 0.9211
Epoch 212/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2310 - acc: 0.9194Epoch 00212: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2296 - acc: 0.9199 - val_loss: 0.2224 - val_acc: 0.9206
Epoch 213/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2397 - acc: 0.9176Epoch 00213: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2362 - acc: 0.9190 - val_loss: 0.2256 - val_acc: 0.9223
Epoch 214/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2412 - acc: 0.9190Epoch 00214: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2388 - acc: 0.9200 - val_loss: 0.2258 - val_acc: 0.9245
Epoch 215/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2288 - acc: 0.9231Epoch 00215: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2290 - acc: 0.9229 - val_loss: 0.2244 - val_acc: 0.9267
Epoch 216/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2356 - acc: 0.9187Epoch 00216: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2336 - acc: 0.9196 - val_loss: 0.2263 - val_acc: 0.9183
Epoch 217/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2276 - acc: 0.9205Epoch 00217: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2280 - acc: 0.9203 - val_loss: 0.2229 - val_acc: 0.9195
Epoch 218/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2273 - acc: 0.9201Epoch 00218: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2287 - acc: 0.9198 - val_loss: 0.2285 - val_acc: 0.9178
Epoch 219/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2288 - acc: 0.9199Epoch 00219: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2285 - acc: 0.9201 - val_loss: 0.2238 - val_acc: 0.9206
Epoch 220/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2363 - acc: 0.9210Epoch 00220: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2371 - acc: 0.9208 - val_loss: 0.2328 - val_acc: 0.9183
Epoch 221/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2264 - acc: 0.9207Epoch 00221: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2264 - acc: 0.9210 - val_loss: 0.2200 - val_acc: 0.9206
Epoch 222/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2337 - acc: 0.9195Epoch 00222: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2338 - acc: 0.9196 - val_loss: 0.2213 - val_acc: 0.9195
Epoch 223/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2247 - acc: 0.9254- ETA: 0s - loss: 0.2209 - acc: 0Epoch 00223: val_loss improved from 0.21742 to 0.21660, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 0.2252 - acc: 0.9251 - val_loss: 0.2166 - val_acc: 0.9239
Epoch 224/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2239 - acc: 0.9226Epoch 00224: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2232 - acc: 0.9228 - val_loss: 0.2260 - val_acc: 0.9200
Epoch 225/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2258 - acc: 0.9247Epoch 00225: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2265 - acc: 0.9249 - val_loss: 0.2168 - val_acc: 0.9223
Epoch 226/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2184 - acc: 0.9244Epoch 00226: val_loss improved from 0.21660 to 0.21614, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 92us/step - loss: 0.2208 - acc: 0.9234 - val_loss: 0.2161 - val_acc: 0.9234
Epoch 227/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2194 - acc: 0.9237Epoch 00227: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2205 - acc: 0.9231 - val_loss: 0.2204 - val_acc: 0.9223
Epoch 228/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2270 - acc: 0.9220Epoch 00228: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2284 - acc: 0.9216 - val_loss: 0.2180 - val_acc: 0.9217
Epoch 229/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2165 - acc: 0.9276Epoch 00229: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2174 - acc: 0.9277 - val_loss: 0.2229 - val_acc: 0.9217
Epoch 230/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2183 - acc: 0.9254Epoch 00230: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2177 - acc: 0.9256 - val_loss: 0.2191 - val_acc: 0.9223
Epoch 231/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2204 - acc: 0.9260Epoch 00231: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2214 - acc: 0.9254 - val_loss: 0.2202 - val_acc: 0.9234
Epoch 232/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2204 - acc: 0.9260Epoch 00232: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2206 - acc: 0.9261 - val_loss: 0.2278 - val_acc: 0.9172
Epoch 233/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2234 - acc: 0.9227Epoch 00233: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2239 - acc: 0.9229 - val_loss: 0.2228 - val_acc: 0.9256
Epoch 234/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2092 - acc: 0.9284Epoch 00234: val_loss improved from 0.21614 to 0.21501, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 93us/step - loss: 0.2090 - acc: 0.9285 - val_loss: 0.2150 - val_acc: 0.9234
Epoch 235/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2224 - acc: 0.9235Epoch 00235: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2229 - acc: 0.9233 - val_loss: 0.2212 - val_acc: 0.9172
Epoch 236/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2290 - acc: 0.9217Epoch 00236: val_loss improved from 0.21501 to 0.21488, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 0.2299 - acc: 0.9217 - val_loss: 0.2149 - val_acc: 0.9211
Epoch 237/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2184 - acc: 0.9243Epoch 00237: val_loss improved from 0.21488 to 0.20910, saving model to weights6.hdf5
17892/17892 [==============================] - 2s 94us/step - loss: 0.2177 - acc: 0.9247 - val_loss: 0.2091 - val_acc: 0.9284
Epoch 238/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2236 - acc: 0.9236Epoch 00238: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2231 - acc: 0.9239 - val_loss: 0.2259 - val_acc: 0.9195
Epoch 239/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2096 - acc: 0.9286Epoch 00239: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2106 - acc: 0.9280 - val_loss: 0.2143 - val_acc: 0.9217
Epoch 240/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2235 - acc: 0.9261Epoch 00240: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2239 - acc: 0.9256 - val_loss: 0.2135 - val_acc: 0.9228
Epoch 241/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2115 - acc: 0.9269Epoch 00241: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2119 - acc: 0.9267 - val_loss: 0.2130 - val_acc: 0.9284
Epoch 242/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2161 - acc: 0.9250Epoch 00242: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2169 - acc: 0.9246 - val_loss: 0.2238 - val_acc: 0.9189
Epoch 243/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2218 - acc: 0.9251Epoch 00243: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2224 - acc: 0.9246 - val_loss: 0.2100 - val_acc: 0.9284
Epoch 244/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2144 - acc: 0.9269Epoch 00244: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2146 - acc: 0.9270 - val_loss: 0.2119 - val_acc: 0.9284
Epoch 245/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2112 - acc: 0.9275Epoch 00245: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2108 - acc: 0.9275 - val_loss: 0.2225 - val_acc: 0.9223
Epoch 246/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2177 - acc: 0.9268Epoch 00246: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2182 - acc: 0.9263 - val_loss: 0.2133 - val_acc: 0.9262
Epoch 247/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2135 - acc: 0.9264Epoch 00247: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2135 - acc: 0.9266 - val_loss: 0.2199 - val_acc: 0.9251
Epoch 248/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2106 - acc: 0.9275Epoch 00248: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2101 - acc: 0.9275 - val_loss: 0.2131 - val_acc: 0.9251
Epoch 249/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2176 - acc: 0.9249Epoch 00249: val_loss did not improve
17892/17892 [==============================] - 2s 92us/step - loss: 0.2152 - acc: 0.9257 - val_loss: 0.2175 - val_acc: 0.9239
Epoch 250/250
17152/17892 [===========================&gt;..] - ETA: 0s - loss: 0.2197 - acc: 0.9238Epoch 00250: val_loss did not improve
17892/17892 [==============================] - 2s 91us/step - loss: 0.2224 - acc: 0.9230 - val_loss: 0.2169 - val_acc: 0.9211
7


 trianing 



Train on 17894 samples, validate on 1786 samples
Epoch 1/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 7.3804 - acc: 0.1027Epoch 00001: val_loss improved from inf to 2.45048, saving model to weights7.hdf5
17894/17894 [==============================] - 3s 157us/step - loss: 7.1788 - acc: 0.1037 - val_loss: 2.4505 - val_acc: 0.1081
Epoch 2/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4728 - acc: 0.1060Epoch 00002: val_loss improved from 2.45048 to 2.43496, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 2.4714 - acc: 0.1066 - val_loss: 2.4350 - val_acc: 0.1064
Epoch 3/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4403 - acc: 0.1111Epoch 00003: val_loss improved from 2.43496 to 2.43268, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.4404 - acc: 0.1109 - val_loss: 2.4327 - val_acc: 0.1069
Epoch 4/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4321 - acc: 0.1079Epoch 00004: val_loss improved from 2.43268 to 2.43163, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.4329 - acc: 0.1076 - val_loss: 2.4316 - val_acc: 0.1069
Epoch 5/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4310 - acc: 0.1081Epoch 00005: val_loss improved from 2.43163 to 2.43082, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.4300 - acc: 0.1088 - val_loss: 2.4308 - val_acc: 0.1069
Epoch 6/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4283 - acc: 0.1099Epoch 00006: val_loss improved from 2.43082 to 2.43019, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 2.4289 - acc: 0.1093 - val_loss: 2.4302 - val_acc: 0.1069
Epoch 7/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4262 - acc: 0.1094Epoch 00007: val_loss improved from 2.43019 to 2.42968, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.4271 - acc: 0.1095 - val_loss: 2.4297 - val_acc: 0.1069
Epoch 8/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4259 - acc: 0.1092Epoch 00008: val_loss improved from 2.42968 to 2.42928, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.4256 - acc: 0.1090 - val_loss: 2.4293 - val_acc: 0.1069
Epoch 9/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4258 - acc: 0.1089Epoch 00009: val_loss improved from 2.42928 to 2.42895, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.4256 - acc: 0.1085 - val_loss: 2.4289 - val_acc: 0.1069
Epoch 10/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4244 - acc: 0.1097Epoch 00010: val_loss improved from 2.42895 to 2.42868, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.4245 - acc: 0.1094 - val_loss: 2.4287 - val_acc: 0.1069
Epoch 11/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4263 - acc: 0.1099Epoch 00011: val_loss improved from 2.42868 to 2.42840, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 2.4257 - acc: 0.1098 - val_loss: 2.4284 - val_acc: 0.1069
Epoch 12/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4248 - acc: 0.1095Epoch 00012: val_loss improved from 2.42840 to 2.42827, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.4240 - acc: 0.1090 - val_loss: 2.4283 - val_acc: 0.1069
Epoch 13/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4239 - acc: 0.1087Epoch 00013: val_loss improved from 2.42827 to 2.42812, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.4240 - acc: 0.1089 - val_loss: 2.4281 - val_acc: 0.1069
Epoch 14/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4236 - acc: 0.1096Epoch 00014: val_loss improved from 2.42812 to 2.42799, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.4235 - acc: 0.1097 - val_loss: 2.4280 - val_acc: 0.1069
Epoch 15/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4234 - acc: 0.1084Epoch 00015: val_loss improved from 2.42799 to 2.42788, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.4234 - acc: 0.1092 - val_loss: 2.4279 - val_acc: 0.1069
Epoch 16/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4222 - acc: 0.1095Epoch 00016: val_loss improved from 2.42788 to 2.42781, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.4230 - acc: 0.1088 - val_loss: 2.4278 - val_acc: 0.1069
Epoch 17/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4219 - acc: 0.1123Epoch 00017: val_loss improved from 2.42781 to 2.42756, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 2.4221 - acc: 0.1121 - val_loss: 2.4276 - val_acc: 0.1069
Epoch 18/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4171 - acc: 0.1207Epoch 00018: val_loss improved from 2.42756 to 2.38553, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.4162 - acc: 0.1218 - val_loss: 2.3855 - val_acc: 0.1568
Epoch 19/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.3962 - acc: 0.1372Epoch 00019: val_loss improved from 2.38553 to 2.35530, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.3963 - acc: 0.1369 - val_loss: 2.3553 - val_acc: 0.1635
Epoch 20/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.3711 - acc: 0.1490Epoch 00020: val_loss improved from 2.35530 to 2.32872, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.3710 - acc: 0.1492 - val_loss: 2.3287 - val_acc: 0.1870
Epoch 21/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.3483 - acc: 0.1602Epoch 00021: val_loss improved from 2.32872 to 2.26022, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.3484 - acc: 0.1600 - val_loss: 2.2602 - val_acc: 0.2284
Epoch 22/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.2284 - acc: 0.2000Epoch 00022: val_loss improved from 2.26022 to 1.96683, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 2.2237 - acc: 0.2007 - val_loss: 1.9668 - val_acc: 0.3108
Epoch 23/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.0595 - acc: 0.2612Epoch 00023: val_loss improved from 1.96683 to 1.76290, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 2.0547 - acc: 0.2633 - val_loss: 1.7629 - val_acc: 0.3869
Epoch 24/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.8904 - acc: 0.3246Epoch 00024: val_loss improved from 1.76290 to 1.57802, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 1.8857 - acc: 0.3258 - val_loss: 1.5780 - val_acc: 0.4384
Epoch 25/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.7255 - acc: 0.3923Epoch 00025: val_loss improved from 1.57802 to 1.37610, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 1.7219 - acc: 0.3926 - val_loss: 1.3761 - val_acc: 0.5588
Epoch 26/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.5788 - acc: 0.4457Epoch 00026: val_loss improved from 1.37610 to 1.29373, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 1.5771 - acc: 0.4464 - val_loss: 1.2937 - val_acc: 0.5700
Epoch 27/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.4480 - acc: 0.4893Epoch 00027: val_loss improved from 1.29373 to 1.09113, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 1.4436 - acc: 0.4913 - val_loss: 1.0911 - val_acc: 0.6321
Epoch 28/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.3285 - acc: 0.5360Epoch 00028: val_loss improved from 1.09113 to 0.98188, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 1.3262 - acc: 0.5370 - val_loss: 0.9819 - val_acc: 0.6820
Epoch 29/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.2349 - acc: 0.5724Epoch 00029: val_loss improved from 0.98188 to 0.88604, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 1.2305 - acc: 0.5740 - val_loss: 0.8860 - val_acc: 0.7167
Epoch 30/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.1541 - acc: 0.5995Epoch 00030: val_loss improved from 0.88604 to 0.81843, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 1.1499 - acc: 0.6010 - val_loss: 0.8184 - val_acc: 0.7340
Epoch 31/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.0784 - acc: 0.6251Epoch 00031: val_loss improved from 0.81843 to 0.76521, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 1.0785 - acc: 0.6254 - val_loss: 0.7652 - val_acc: 0.7525
Epoch 32/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.0310 - acc: 0.6438Epoch 00032: val_loss improved from 0.76521 to 0.72378, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 1.0304 - acc: 0.6444 - val_loss: 0.7238 - val_acc: 0.7766
Epoch 33/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.9970 - acc: 0.6572Epoch 00033: val_loss improved from 0.72378 to 0.68471, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.9947 - acc: 0.6574 - val_loss: 0.6847 - val_acc: 0.7878
Epoch 34/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.9415 - acc: 0.6760Epoch 00034: val_loss improved from 0.68471 to 0.64500, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 95us/step - loss: 0.9428 - acc: 0.6753 - val_loss: 0.6450 - val_acc: 0.7928
Epoch 35/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.9160 - acc: 0.6906Epoch 00035: val_loss improved from 0.64500 to 0.63792, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.9141 - acc: 0.6909 - val_loss: 0.6379 - val_acc: 0.7923
Epoch 36/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.8744 - acc: 0.6996Epoch 00036: val_loss improved from 0.63792 to 0.58926, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.8770 - acc: 0.6993 - val_loss: 0.5893 - val_acc: 0.8175
Epoch 37/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.8441 - acc: 0.7080Epoch 00037: val_loss improved from 0.58926 to 0.56694, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.8474 - acc: 0.7074 - val_loss: 0.5669 - val_acc: 0.8214
Epoch 38/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.8261 - acc: 0.7216Epoch 00038: val_loss improved from 0.56694 to 0.54505, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.8247 - acc: 0.7216 - val_loss: 0.5451 - val_acc: 0.8303
Epoch 39/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.8006 - acc: 0.7282Epoch 00039: val_loss improved from 0.54505 to 0.53284, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.7980 - acc: 0.7293 - val_loss: 0.5328 - val_acc: 0.8337
Epoch 40/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.7801 - acc: 0.7355Epoch 00040: val_loss improved from 0.53284 to 0.50647, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.7811 - acc: 0.7351 - val_loss: 0.5065 - val_acc: 0.8427
Epoch 41/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.7511 - acc: 0.7436Epoch 00041: val_loss improved from 0.50647 to 0.48928, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.7525 - acc: 0.7434 - val_loss: 0.4893 - val_acc: 0.8404
Epoch 42/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.7451 - acc: 0.7470Epoch 00042: val_loss improved from 0.48928 to 0.46804, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.7445 - acc: 0.7472 - val_loss: 0.4680 - val_acc: 0.8494
Epoch 43/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.7188 - acc: 0.7568Epoch 00043: val_loss improved from 0.46804 to 0.46490, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.7206 - acc: 0.7568 - val_loss: 0.4649 - val_acc: 0.8511
Epoch 44/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.7048 - acc: 0.7625Epoch 00044: val_loss improved from 0.46490 to 0.45973, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.7042 - acc: 0.7633 - val_loss: 0.4597 - val_acc: 0.8527
Epoch 45/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.6984 - acc: 0.7607Epoch 00045: val_loss improved from 0.45973 to 0.42769, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.6974 - acc: 0.7613 - val_loss: 0.4277 - val_acc: 0.8589
Epoch 46/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.6765 - acc: 0.7712Epoch 00046: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.6768 - acc: 0.7708 - val_loss: 0.4356 - val_acc: 0.8527
Epoch 47/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.6618 - acc: 0.7754Epoch 00047: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.6629 - acc: 0.7746 - val_loss: 0.4336 - val_acc: 0.8522
Epoch 48/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.6514 - acc: 0.7790Epoch 00048: val_loss improved from 0.42769 to 0.40872, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.6516 - acc: 0.7788 - val_loss: 0.4087 - val_acc: 0.8651
Epoch 49/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.6444 - acc: 0.7826Epoch 00049: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.6479 - acc: 0.7808 - val_loss: 0.4185 - val_acc: 0.8617
Epoch 50/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.6337 - acc: 0.7872Epoch 00050: val_loss improved from 0.40872 to 0.40530, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.6346 - acc: 0.7868 - val_loss: 0.4053 - val_acc: 0.8645
Epoch 51/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.6236 - acc: 0.7920Epoch 00051: val_loss improved from 0.40530 to 0.38888, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.6228 - acc: 0.7917 - val_loss: 0.3889 - val_acc: 0.8695
Epoch 52/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.6065 - acc: 0.7955Epoch 00052: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.6058 - acc: 0.7960 - val_loss: 0.3911 - val_acc: 0.8729
Epoch 53/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5980 - acc: 0.7989Epoch 00053: val_loss improved from 0.38888 to 0.37982, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.5998 - acc: 0.7990 - val_loss: 0.3798 - val_acc: 0.8729
Epoch 54/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5930 - acc: 0.8001Epoch 00054: val_loss improved from 0.37982 to 0.37656, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.5937 - acc: 0.7997 - val_loss: 0.3766 - val_acc: 0.8718
Epoch 55/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5867 - acc: 0.8035Epoch 00055: val_loss improved from 0.37656 to 0.35869, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.5850 - acc: 0.8038 - val_loss: 0.3587 - val_acc: 0.8785
Epoch 56/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5659 - acc: 0.8105Epoch 00056: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.5651 - acc: 0.8106 - val_loss: 0.3613 - val_acc: 0.8835
Epoch 57/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5692 - acc: 0.8074Epoch 00057: val_loss improved from 0.35869 to 0.35203, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.5663 - acc: 0.8087 - val_loss: 0.3520 - val_acc: 0.8858
Epoch 58/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5601 - acc: 0.8094Epoch 00058: val_loss improved from 0.35203 to 0.34796, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.5597 - acc: 0.8093 - val_loss: 0.3480 - val_acc: 0.8830
Epoch 59/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5454 - acc: 0.8127Epoch 00059: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.5469 - acc: 0.8124 - val_loss: 0.3590 - val_acc: 0.8802
Epoch 60/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5365 - acc: 0.8177Epoch 00060: val_loss improved from 0.34796 to 0.34049, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.5375 - acc: 0.8176 - val_loss: 0.3405 - val_acc: 0.8830
Epoch 61/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5425 - acc: 0.8170Epoch 00061: val_loss improved from 0.34049 to 0.33233, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.5435 - acc: 0.8174 - val_loss: 0.3323 - val_acc: 0.8886
Epoch 62/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5291 - acc: 0.8212Epoch 00062: val_loss improved from 0.33233 to 0.32144, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.5249 - acc: 0.8229 - val_loss: 0.3214 - val_acc: 0.8931
Epoch 63/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5244 - acc: 0.8234Epoch 00063: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.5224 - acc: 0.8241 - val_loss: 0.3260 - val_acc: 0.8953
Epoch 64/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5091 - acc: 0.8292Epoch 00064: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.5081 - acc: 0.8289 - val_loss: 0.3250 - val_acc: 0.8897
Epoch 65/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5146 - acc: 0.8280Epoch 00065: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.5153 - acc: 0.8282 - val_loss: 0.3249 - val_acc: 0.8931
Epoch 66/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5061 - acc: 0.8298Epoch 00066: val_loss improved from 0.32144 to 0.31784, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.5087 - acc: 0.8288 - val_loss: 0.3178 - val_acc: 0.8953
Epoch 67/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4961 - acc: 0.8354Epoch 00067: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.4982 - acc: 0.8345 - val_loss: 0.3185 - val_acc: 0.8947
Epoch 68/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5002 - acc: 0.8306Epoch 00068: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.5015 - acc: 0.8306 - val_loss: 0.3218 - val_acc: 0.8936
Epoch 69/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4920 - acc: 0.8317Epoch 00069: val_loss improved from 0.31784 to 0.30927, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4901 - acc: 0.8327 - val_loss: 0.3093 - val_acc: 0.9059
Epoch 70/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4900 - acc: 0.8355Epoch 00070: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.4925 - acc: 0.8344 - val_loss: 0.3151 - val_acc: 0.8931
Epoch 71/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4829 - acc: 0.8364Epoch 00071: val_loss improved from 0.30927 to 0.29716, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4846 - acc: 0.8363 - val_loss: 0.2972 - val_acc: 0.9026
Epoch 72/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4733 - acc: 0.8367Epoch 00072: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.4740 - acc: 0.8366 - val_loss: 0.3013 - val_acc: 0.8992
Epoch 73/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4674 - acc: 0.8421Epoch 00073: val_loss improved from 0.29716 to 0.29539, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4694 - acc: 0.8411 - val_loss: 0.2954 - val_acc: 0.9020
Epoch 74/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4690 - acc: 0.8440Epoch 00074: val_loss improved from 0.29539 to 0.29346, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4685 - acc: 0.8438 - val_loss: 0.2935 - val_acc: 0.9037
Epoch 75/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4582 - acc: 0.8439Epoch 00075: val_loss improved from 0.29346 to 0.28968, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4580 - acc: 0.8445 - val_loss: 0.2897 - val_acc: 0.9065
Epoch 76/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4561 - acc: 0.8472Epoch 00076: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.4584 - acc: 0.8465 - val_loss: 0.2950 - val_acc: 0.9031
Epoch 77/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.4574 - acc: 0.8474Epoch 00077: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.4568 - acc: 0.8482 - val_loss: 0.2912 - val_acc: 0.9054
Epoch 78/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4490 - acc: 0.8479Epoch 00078: val_loss improved from 0.28968 to 0.28791, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.4492 - acc: 0.8478 - val_loss: 0.2879 - val_acc: 0.9082
Epoch 79/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4421 - acc: 0.8507Epoch 00079: val_loss improved from 0.28791 to 0.28599, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.4425 - acc: 0.8500 - val_loss: 0.2860 - val_acc: 0.9048
Epoch 80/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.4504 - acc: 0.8469Epoch 00080: val_loss improved from 0.28599 to 0.28536, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.4507 - acc: 0.8469 - val_loss: 0.2854 - val_acc: 0.9059
Epoch 81/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4402 - acc: 0.8526Epoch 00081: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.4415 - acc: 0.8515 - val_loss: 0.2890 - val_acc: 0.9071
Epoch 82/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4300 - acc: 0.8527Epoch 00082: val_loss improved from 0.28536 to 0.27859, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.4303 - acc: 0.8525 - val_loss: 0.2786 - val_acc: 0.9082
Epoch 83/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4287 - acc: 0.8568Epoch 00083: val_loss improved from 0.27859 to 0.27612, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4287 - acc: 0.8566 - val_loss: 0.2761 - val_acc: 0.9048
Epoch 84/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4373 - acc: 0.8502Epoch 00084: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.4392 - acc: 0.8505 - val_loss: 0.2804 - val_acc: 0.9043
Epoch 85/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4259 - acc: 0.8566Epoch 00085: val_loss improved from 0.27612 to 0.27007, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4266 - acc: 0.8563 - val_loss: 0.2701 - val_acc: 0.9054
Epoch 86/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4197 - acc: 0.8552Epoch 00086: val_loss improved from 0.27007 to 0.26949, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4190 - acc: 0.8553 - val_loss: 0.2695 - val_acc: 0.9127
Epoch 87/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4167 - acc: 0.8610Epoch 00087: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.4176 - acc: 0.8608 - val_loss: 0.2751 - val_acc: 0.9059
Epoch 88/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4120 - acc: 0.8609Epoch 00088: val_loss improved from 0.26949 to 0.26907, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4129 - acc: 0.8605 - val_loss: 0.2691 - val_acc: 0.9132
Epoch 89/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4106 - acc: 0.8622Epoch 00089: val_loss improved from 0.26907 to 0.26561, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4116 - acc: 0.8622 - val_loss: 0.2656 - val_acc: 0.9059
Epoch 90/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4100 - acc: 0.8616Epoch 00090: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.4089 - acc: 0.8619 - val_loss: 0.2727 - val_acc: 0.9082
Epoch 91/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3998 - acc: 0.8641Epoch 00091: val_loss improved from 0.26561 to 0.26396, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4005 - acc: 0.8639 - val_loss: 0.2640 - val_acc: 0.9087
Epoch 92/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3961 - acc: 0.8670Epoch 00092: val_loss improved from 0.26396 to 0.26200, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.3968 - acc: 0.8671 - val_loss: 0.2620 - val_acc: 0.9171
Epoch 93/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3941 - acc: 0.8662Epoch 00093: val_loss improved from 0.26200 to 0.25820, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.3965 - acc: 0.8658 - val_loss: 0.2582 - val_acc: 0.9155
Epoch 94/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3960 - acc: 0.8685- ETA: 0s - loss: 0.3948 - acc: 0.86Epoch 00094: val_loss improved from 0.25820 to 0.25664, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.3960 - acc: 0.8682 - val_loss: 0.2566 - val_acc: 0.9143
Epoch 95/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3980 - acc: 0.8646- ETA: 0s - loss: 0.3985 - accEpoch 00095: val_loss improved from 0.25664 to 0.25560, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.3982 - acc: 0.8644 - val_loss: 0.2556 - val_acc: 0.9155
Epoch 96/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3910 - acc: 0.8673Epoch 00096: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.3916 - acc: 0.8670 - val_loss: 0.2559 - val_acc: 0.9138
Epoch 97/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3852 - acc: 0.8709Epoch 00097: val_loss did not improve
17894/17894 [==============================] - 2s 89us/step - loss: 0.3859 - acc: 0.8707 - val_loss: 0.2582 - val_acc: 0.9115
Epoch 98/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3877 - acc: 0.8685- ETA: 1s - loss: 0.3876Epoch 00098: val_loss improved from 0.25560 to 0.25487, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.3877 - acc: 0.8685 - val_loss: 0.2549 - val_acc: 0.9121
Epoch 99/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3849 - acc: 0.8702Epoch 00099: val_loss improved from 0.25487 to 0.24751, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.3827 - acc: 0.8711 - val_loss: 0.2475 - val_acc: 0.9155
Epoch 100/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3775 - acc: 0.8703Epoch 00100: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.3779 - acc: 0.8702 - val_loss: 0.2491 - val_acc: 0.9143
Epoch 101/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3769 - acc: 0.8741Epoch 00101: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.3765 - acc: 0.8744 - val_loss: 0.2476 - val_acc: 0.9188
Epoch 102/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3720 - acc: 0.8757Epoch 00102: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3717 - acc: 0.8758 - val_loss: 0.2486 - val_acc: 0.9194
Epoch 103/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3769 - acc: 0.8748Epoch 00103: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3758 - acc: 0.8750 - val_loss: 0.2478 - val_acc: 0.9138
Epoch 104/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3684 - acc: 0.8772Epoch 00104: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.3701 - acc: 0.8765 - val_loss: 0.2492 - val_acc: 0.9205
Epoch 105/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3688 - acc: 0.8744Epoch 00105: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3699 - acc: 0.8738 - val_loss: 0.2520 - val_acc: 0.9155
Epoch 106/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3654 - acc: 0.8767Epoch 00106: val_loss improved from 0.24751 to 0.24679, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.3646 - acc: 0.8772 - val_loss: 0.2468 - val_acc: 0.9143
Epoch 107/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3517 - acc: 0.8794Epoch 00107: val_loss improved from 0.24679 to 0.24289, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.3508 - acc: 0.8800 - val_loss: 0.2429 - val_acc: 0.9188
Epoch 108/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3596 - acc: 0.8759Epoch 00108: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3579 - acc: 0.8763 - val_loss: 0.2437 - val_acc: 0.9177
Epoch 109/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3538 - acc: 0.8788Epoch 00109: val_loss improved from 0.24289 to 0.23938, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.3554 - acc: 0.8785 - val_loss: 0.2394 - val_acc: 0.9183
Epoch 110/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3571 - acc: 0.8785Epoch 00110: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3577 - acc: 0.8786 - val_loss: 0.2487 - val_acc: 0.9188
Epoch 111/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3455 - acc: 0.8822Epoch 00111: val_loss improved from 0.23938 to 0.23868, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.3499 - acc: 0.8807 - val_loss: 0.2387 - val_acc: 0.9160
Epoch 112/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3554 - acc: 0.8819Epoch 00112: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3542 - acc: 0.8821 - val_loss: 0.2446 - val_acc: 0.9149
Epoch 113/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3475 - acc: 0.8835Epoch 00113: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3496 - acc: 0.8829 - val_loss: 0.2466 - val_acc: 0.9149
Epoch 114/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3500 - acc: 0.8818Epoch 00114: val_loss improved from 0.23868 to 0.23785, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.3514 - acc: 0.8812 - val_loss: 0.2378 - val_acc: 0.9171
Epoch 115/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3528 - acc: 0.8816Epoch 00115: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3524 - acc: 0.8811 - val_loss: 0.2404 - val_acc: 0.9155
Epoch 116/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3405 - acc: 0.8857Epoch 00116: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3408 - acc: 0.8853 - val_loss: 0.2447 - val_acc: 0.9143
Epoch 117/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3424 - acc: 0.8832Epoch 00117: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3434 - acc: 0.8825 - val_loss: 0.2424 - val_acc: 0.9188
Epoch 118/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3422 - acc: 0.8821Epoch 00118: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3422 - acc: 0.8828 - val_loss: 0.2431 - val_acc: 0.9171
Epoch 119/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3318 - acc: 0.8839Epoch 00119: val_loss improved from 0.23785 to 0.23639, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.3306 - acc: 0.8849 - val_loss: 0.2364 - val_acc: 0.9183
Epoch 120/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3337 - acc: 0.8871Epoch 00120: val_loss improved from 0.23639 to 0.23572, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.3340 - acc: 0.8866 - val_loss: 0.2357 - val_acc: 0.9239
Epoch 121/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3359 - acc: 0.8871Epoch 00121: val_loss improved from 0.23572 to 0.23419, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.3373 - acc: 0.8865 - val_loss: 0.2342 - val_acc: 0.9211
Epoch 122/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3387 - acc: 0.8879Epoch 00122: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3364 - acc: 0.8884 - val_loss: 0.2360 - val_acc: 0.9216
Epoch 123/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.3333 - acc: 0.8880Epoch 00123: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.3332 - acc: 0.8884 - val_loss: 0.2370 - val_acc: 0.9183
Epoch 124/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3249 - acc: 0.8878Epoch 00124: val_loss improved from 0.23419 to 0.23329, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.3258 - acc: 0.8881 - val_loss: 0.2333 - val_acc: 0.9183
Epoch 125/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3216 - acc: 0.8909Epoch 00125: val_loss improved from 0.23329 to 0.22735, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.3224 - acc: 0.8901 - val_loss: 0.2273 - val_acc: 0.9250
Epoch 126/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3253 - acc: 0.8907Epoch 00126: val_loss improved from 0.22735 to 0.22712, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.3247 - acc: 0.8906 - val_loss: 0.2271 - val_acc: 0.9239
Epoch 127/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3254 - acc: 0.8871Epoch 00127: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3250 - acc: 0.8871 - val_loss: 0.2321 - val_acc: 0.9188
Epoch 128/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3198 - acc: 0.8921Epoch 00128: val_loss improved from 0.22712 to 0.22632, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.3209 - acc: 0.8914 - val_loss: 0.2263 - val_acc: 0.9222
Epoch 129/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3233 - acc: 0.8895Epoch 00129: val_loss improved from 0.22632 to 0.22469, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.3235 - acc: 0.8898 - val_loss: 0.2247 - val_acc: 0.9194
Epoch 130/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3179 - acc: 0.8934Epoch 00130: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.3184 - acc: 0.8930 - val_loss: 0.2294 - val_acc: 0.9211
Epoch 131/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3184 - acc: 0.8901Epoch 00131: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3179 - acc: 0.8904 - val_loss: 0.2263 - val_acc: 0.9211
Epoch 132/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3122 - acc: 0.8956Epoch 00132: val_loss did not improve
17894/17894 [==============================] - 2s 89us/step - loss: 0.3129 - acc: 0.8953 - val_loss: 0.2278 - val_acc: 0.9244
Epoch 133/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3159 - acc: 0.8922Epoch 00133: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3172 - acc: 0.8923 - val_loss: 0.2272 - val_acc: 0.9250
Epoch 134/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3156 - acc: 0.8939Epoch 00134: val_loss improved from 0.22469 to 0.22139, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.3170 - acc: 0.8932 - val_loss: 0.2214 - val_acc: 0.9250
Epoch 135/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3163 - acc: 0.8940Epoch 00135: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3157 - acc: 0.8943 - val_loss: 0.2215 - val_acc: 0.9255
Epoch 136/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3106 - acc: 0.8938Epoch 00136: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3102 - acc: 0.8939 - val_loss: 0.2309 - val_acc: 0.9199
Epoch 137/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3048 - acc: 0.8958Epoch 00137: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3051 - acc: 0.8961 - val_loss: 0.2345 - val_acc: 0.9211
Epoch 138/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3064 - acc: 0.8945Epoch 00138: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3072 - acc: 0.8945 - val_loss: 0.2254 - val_acc: 0.9239
Epoch 139/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3152 - acc: 0.8960Epoch 00139: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3144 - acc: 0.8962 - val_loss: 0.2260 - val_acc: 0.9255
Epoch 140/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3051 - acc: 0.8962Epoch 00140: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3077 - acc: 0.8953 - val_loss: 0.2294 - val_acc: 0.9233
Epoch 141/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3102 - acc: 0.8922Epoch 00141: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3081 - acc: 0.8931 - val_loss: 0.2254 - val_acc: 0.9255
Epoch 142/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3046 - acc: 0.8935Epoch 00142: val_loss improved from 0.22139 to 0.22081, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.3056 - acc: 0.8935 - val_loss: 0.2208 - val_acc: 0.9261
Epoch 143/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2978 - acc: 0.8967Epoch 00143: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2975 - acc: 0.8962 - val_loss: 0.2215 - val_acc: 0.9233
Epoch 144/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2976 - acc: 0.8983Epoch 00144: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2978 - acc: 0.8981 - val_loss: 0.2297 - val_acc: 0.9222
Epoch 145/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2980 - acc: 0.8977Epoch 00145: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2980 - acc: 0.8975 - val_loss: 0.2251 - val_acc: 0.9255
Epoch 146/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3008 - acc: 0.8967Epoch 00146: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3003 - acc: 0.8969 - val_loss: 0.2227 - val_acc: 0.9244
Epoch 147/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3035 - acc: 0.8949Epoch 00147: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3045 - acc: 0.8948 - val_loss: 0.2220 - val_acc: 0.9244
Epoch 148/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2963 - acc: 0.8988Epoch 00148: val_loss improved from 0.22081 to 0.21811, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.2936 - acc: 0.8995 - val_loss: 0.2181 - val_acc: 0.9244
Epoch 149/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2906 - acc: 0.9022Epoch 00149: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2911 - acc: 0.9020 - val_loss: 0.2219 - val_acc: 0.9255
Epoch 150/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2923 - acc: 0.9009Epoch 00150: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2924 - acc: 0.9006 - val_loss: 0.2269 - val_acc: 0.9278
Epoch 151/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2935 - acc: 0.8998Epoch 00151: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2940 - acc: 0.8996 - val_loss: 0.2234 - val_acc: 0.9233
Epoch 152/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2879 - acc: 0.9032Epoch 00152: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2896 - acc: 0.9025 - val_loss: 0.2188 - val_acc: 0.9255
Epoch 153/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2977 - acc: 0.8989Epoch 00153: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2964 - acc: 0.8995 - val_loss: 0.2229 - val_acc: 0.9227
Epoch 154/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2910 - acc: 0.9018Epoch 00154: val_loss improved from 0.21811 to 0.21258, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.2911 - acc: 0.9019 - val_loss: 0.2126 - val_acc: 0.9233
Epoch 155/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2867 - acc: 0.9007Epoch 00155: val_loss improved from 0.21258 to 0.21252, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.2877 - acc: 0.9004 - val_loss: 0.2125 - val_acc: 0.9261
Epoch 156/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2878 - acc: 0.9025Epoch 00156: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2881 - acc: 0.9020 - val_loss: 0.2202 - val_acc: 0.9272
Epoch 157/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2813 - acc: 0.9038Epoch 00157: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2814 - acc: 0.9040 - val_loss: 0.2138 - val_acc: 0.9317
Epoch 158/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2808 - acc: 0.9062Epoch 00158: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2816 - acc: 0.9056 - val_loss: 0.2238 - val_acc: 0.9233
Epoch 159/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2828 - acc: 0.9046Epoch 00159: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2829 - acc: 0.9043 - val_loss: 0.2165 - val_acc: 0.9244
Epoch 160/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2816 - acc: 0.9006Epoch 00160: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2822 - acc: 0.9004 - val_loss: 0.2199 - val_acc: 0.9278
Epoch 161/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2771 - acc: 0.9038Epoch 00161: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2753 - acc: 0.9040 - val_loss: 0.2214 - val_acc: 0.9233
Epoch 162/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2799 - acc: 0.9031Epoch 00162: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2793 - acc: 0.9036 - val_loss: 0.2201 - val_acc: 0.9250
Epoch 163/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2834 - acc: 0.9037Epoch 00163: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2825 - acc: 0.9039 - val_loss: 0.2153 - val_acc: 0.9278
Epoch 164/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2801 - acc: 0.9068Epoch 00164: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2817 - acc: 0.9061 - val_loss: 0.2148 - val_acc: 0.9255
Epoch 165/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2791 - acc: 0.9059Epoch 00165: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2802 - acc: 0.9059 - val_loss: 0.2133 - val_acc: 0.9267
Epoch 166/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2715 - acc: 0.9047Epoch 00166: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2722 - acc: 0.9047 - val_loss: 0.2179 - val_acc: 0.9222
Epoch 167/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2698 - acc: 0.9085Epoch 00167: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2723 - acc: 0.9081 - val_loss: 0.2176 - val_acc: 0.9244
Epoch 168/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2734 - acc: 0.9082Epoch 00168: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2718 - acc: 0.9090 - val_loss: 0.2138 - val_acc: 0.9272
Epoch 169/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2749 - acc: 0.9072Epoch 00169: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2729 - acc: 0.9081 - val_loss: 0.2175 - val_acc: 0.9261
Epoch 170/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2697 - acc: 0.9065Epoch 00170: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2705 - acc: 0.9062 - val_loss: 0.2139 - val_acc: 0.9255
Epoch 171/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2697 - acc: 0.9077Epoch 00171: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2704 - acc: 0.9067 - val_loss: 0.2167 - val_acc: 0.9267
Epoch 172/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2695 - acc: 0.9089Epoch 00172: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2683 - acc: 0.9095 - val_loss: 0.2214 - val_acc: 0.9233
Epoch 173/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2627 - acc: 0.9094Epoch 00173: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2652 - acc: 0.9089 - val_loss: 0.2141 - val_acc: 0.9227
Epoch 174/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2696 - acc: 0.9120Epoch 00174: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2695 - acc: 0.9118 - val_loss: 0.2169 - val_acc: 0.9267
Epoch 175/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2580 - acc: 0.9111Epoch 00175: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2569 - acc: 0.9116 - val_loss: 0.2178 - val_acc: 0.9233
Epoch 176/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2541 - acc: 0.9118Epoch 00176: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2544 - acc: 0.9116 - val_loss: 0.2287 - val_acc: 0.9227
Epoch 177/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2671 - acc: 0.9079Epoch 00177: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2692 - acc: 0.9068 - val_loss: 0.2127 - val_acc: 0.9211
Epoch 178/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2604 - acc: 0.9108Epoch 00178: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2585 - acc: 0.9114 - val_loss: 0.2188 - val_acc: 0.9233
Epoch 179/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2693 - acc: 0.9086Epoch 00179: val_loss improved from 0.21252 to 0.21207, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.2684 - acc: 0.9086 - val_loss: 0.2121 - val_acc: 0.9289
Epoch 180/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2625 - acc: 0.9082Epoch 00180: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2630 - acc: 0.9081 - val_loss: 0.2143 - val_acc: 0.9239
Epoch 181/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2689 - acc: 0.9076Epoch 00181: val_loss improved from 0.21207 to 0.21056, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.2675 - acc: 0.9083 - val_loss: 0.2106 - val_acc: 0.9267
Epoch 182/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2588 - acc: 0.9105Epoch 00182: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2587 - acc: 0.9107 - val_loss: 0.2107 - val_acc: 0.9272
Epoch 183/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2647 - acc: 0.9112Epoch 00183: val_loss improved from 0.21056 to 0.21052, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.2654 - acc: 0.9110 - val_loss: 0.2105 - val_acc: 0.9323
Epoch 184/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2595 - acc: 0.9103Epoch 00184: val_loss improved from 0.21052 to 0.20801, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.2590 - acc: 0.9108 - val_loss: 0.2080 - val_acc: 0.9306
Epoch 185/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2559 - acc: 0.9134Epoch 00185: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2562 - acc: 0.9131 - val_loss: 0.2115 - val_acc: 0.9244
Epoch 186/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2644 - acc: 0.9115Epoch 00186: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2637 - acc: 0.9121 - val_loss: 0.2096 - val_acc: 0.9255
Epoch 187/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2526 - acc: 0.9124Epoch 00187: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2527 - acc: 0.9127 - val_loss: 0.2156 - val_acc: 0.9261
Epoch 188/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2625 - acc: 0.9124Epoch 00188: val_loss improved from 0.20801 to 0.20386, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.2636 - acc: 0.9122 - val_loss: 0.2039 - val_acc: 0.9272
Epoch 189/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2443 - acc: 0.9170Epoch 00189: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2468 - acc: 0.9159 - val_loss: 0.2129 - val_acc: 0.9272
Epoch 190/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2505 - acc: 0.9161Epoch 00190: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2507 - acc: 0.9162 - val_loss: 0.2073 - val_acc: 0.9295
Epoch 191/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2578 - acc: 0.9127Epoch 00191: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2572 - acc: 0.9127 - val_loss: 0.2041 - val_acc: 0.9283
Epoch 192/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2586 - acc: 0.9142Epoch 00192: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2577 - acc: 0.9144 - val_loss: 0.2122 - val_acc: 0.9300
Epoch 193/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2573 - acc: 0.9117Epoch 00193: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2578 - acc: 0.9114 - val_loss: 0.2114 - val_acc: 0.9278
Epoch 194/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.2598 - acc: 0.9126Epoch 00194: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2590 - acc: 0.9128 - val_loss: 0.2128 - val_acc: 0.9272
Epoch 195/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2482 - acc: 0.9142Epoch 00195: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2474 - acc: 0.9143 - val_loss: 0.2098 - val_acc: 0.9250
Epoch 196/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2537 - acc: 0.9117Epoch 00196: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2542 - acc: 0.9115 - val_loss: 0.2125 - val_acc: 0.9250
Epoch 197/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.2482 - acc: 0.9130Epoch 00197: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2472 - acc: 0.9134 - val_loss: 0.2079 - val_acc: 0.9244
Epoch 198/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.2394 - acc: 0.9173Epoch 00198: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2394 - acc: 0.9173 - val_loss: 0.2096 - val_acc: 0.9255
Epoch 199/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2588 - acc: 0.9152Epoch 00199: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2568 - acc: 0.9157 - val_loss: 0.2112 - val_acc: 0.9233
Epoch 200/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2518 - acc: 0.9142Epoch 00200: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2506 - acc: 0.9148 - val_loss: 0.2102 - val_acc: 0.9255
Epoch 201/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2569 - acc: 0.9104Epoch 00201: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2567 - acc: 0.9104 - val_loss: 0.2076 - val_acc: 0.9283
Epoch 202/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2430 - acc: 0.9162Epoch 00202: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2427 - acc: 0.9166 - val_loss: 0.2088 - val_acc: 0.9283
Epoch 203/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2371 - acc: 0.9172Epoch 00203: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2357 - acc: 0.9176 - val_loss: 0.2128 - val_acc: 0.9250
Epoch 204/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2503 - acc: 0.9130Epoch 00204: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2487 - acc: 0.9134 - val_loss: 0.2055 - val_acc: 0.9255
Epoch 205/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2415 - acc: 0.9170Epoch 00205: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2405 - acc: 0.9172 - val_loss: 0.2096 - val_acc: 0.9239
Epoch 206/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2415 - acc: 0.9172Epoch 00206: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2390 - acc: 0.9182 - val_loss: 0.2087 - val_acc: 0.9278
Epoch 207/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2382 - acc: 0.9190Epoch 00207: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2382 - acc: 0.9191 - val_loss: 0.2124 - val_acc: 0.9295
Epoch 208/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2443 - acc: 0.9151Epoch 00208: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2442 - acc: 0.9153 - val_loss: 0.2114 - val_acc: 0.9267
Epoch 209/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2423 - acc: 0.9171Epoch 00209: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2430 - acc: 0.9167 - val_loss: 0.2096 - val_acc: 0.9267
Epoch 210/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2357 - acc: 0.9191Epoch 00210: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2370 - acc: 0.9180 - val_loss: 0.2125 - val_acc: 0.9278
Epoch 211/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2366 - acc: 0.9196Epoch 00211: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2374 - acc: 0.9197 - val_loss: 0.2153 - val_acc: 0.9278
Epoch 212/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2358 - acc: 0.9189Epoch 00212: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2363 - acc: 0.9187 - val_loss: 0.2133 - val_acc: 0.9283
Epoch 213/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2370 - acc: 0.9164Epoch 00213: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2359 - acc: 0.9169 - val_loss: 0.2101 - val_acc: 0.9272
Epoch 214/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2426 - acc: 0.9181Epoch 00214: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2428 - acc: 0.9181 - val_loss: 0.2111 - val_acc: 0.9267
Epoch 215/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2369 - acc: 0.9190Epoch 00215: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2376 - acc: 0.9186 - val_loss: 0.2082 - val_acc: 0.9295
Epoch 216/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2376 - acc: 0.9203Epoch 00216: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2377 - acc: 0.9197 - val_loss: 0.2064 - val_acc: 0.9272
Epoch 217/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2424 - acc: 0.9170Epoch 00217: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2413 - acc: 0.9175 - val_loss: 0.2097 - val_acc: 0.9239
Epoch 218/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2329 - acc: 0.9203Epoch 00218: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2320 - acc: 0.9205 - val_loss: 0.2067 - val_acc: 0.9267
Epoch 219/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2357 - acc: 0.9188Epoch 00219: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2357 - acc: 0.9189 - val_loss: 0.2093 - val_acc: 0.9278
Epoch 220/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2299 - acc: 0.9225Epoch 00220: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2294 - acc: 0.9228 - val_loss: 0.2086 - val_acc: 0.9267
Epoch 221/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.2329 - acc: 0.9204Epoch 00221: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2327 - acc: 0.9202 - val_loss: 0.2114 - val_acc: 0.9278
Epoch 222/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2319 - acc: 0.9194Epoch 00222: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2317 - acc: 0.9191 - val_loss: 0.2055 - val_acc: 0.9345
Epoch 223/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2266 - acc: 0.9221Epoch 00223: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2280 - acc: 0.9217 - val_loss: 0.2193 - val_acc: 0.9255
Epoch 224/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2297 - acc: 0.9211Epoch 00224: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2298 - acc: 0.9209 - val_loss: 0.2095 - val_acc: 0.9283
Epoch 225/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2278 - acc: 0.9225Epoch 00225: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2282 - acc: 0.9225 - val_loss: 0.2104 - val_acc: 0.9306
Epoch 226/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2312 - acc: 0.9227Epoch 00226: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2289 - acc: 0.9238 - val_loss: 0.2063 - val_acc: 0.9278
Epoch 227/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2281 - acc: 0.9221Epoch 00227: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2272 - acc: 0.9223 - val_loss: 0.2057 - val_acc: 0.9289
Epoch 228/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2238 - acc: 0.9250Epoch 00228: val_loss improved from 0.20386 to 0.20067, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.2236 - acc: 0.9252 - val_loss: 0.2007 - val_acc: 0.9283
Epoch 229/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2331 - acc: 0.9194Epoch 00229: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2322 - acc: 0.9198 - val_loss: 0.2050 - val_acc: 0.9306
Epoch 230/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2221 - acc: 0.9244Epoch 00230: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2234 - acc: 0.9234 - val_loss: 0.2062 - val_acc: 0.9300
Epoch 231/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2320 - acc: 0.9204Epoch 00231: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2314 - acc: 0.9204 - val_loss: 0.2136 - val_acc: 0.9272
Epoch 232/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2314 - acc: 0.9192Epoch 00232: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2316 - acc: 0.9193 - val_loss: 0.2053 - val_acc: 0.9283
Epoch 233/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2233 - acc: 0.9220Epoch 00233: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2236 - acc: 0.9221 - val_loss: 0.2069 - val_acc: 0.9255
Epoch 234/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2282 - acc: 0.9225Epoch 00234: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2278 - acc: 0.9226 - val_loss: 0.2143 - val_acc: 0.9283
Epoch 235/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2322 - acc: 0.9204Epoch 00235: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2316 - acc: 0.9202 - val_loss: 0.2118 - val_acc: 0.9295
Epoch 236/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2280 - acc: 0.9222Epoch 00236: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2269 - acc: 0.9227 - val_loss: 0.2102 - val_acc: 0.9306
Epoch 237/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2249 - acc: 0.9241Epoch 00237: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2259 - acc: 0.9240 - val_loss: 0.2084 - val_acc: 0.9278
Epoch 238/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2246 - acc: 0.9223Epoch 00238: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2249 - acc: 0.9224 - val_loss: 0.2088 - val_acc: 0.9272
Epoch 239/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2249 - acc: 0.9244Epoch 00239: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2235 - acc: 0.9244 - val_loss: 0.2031 - val_acc: 0.9283
Epoch 240/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2253 - acc: 0.9221Epoch 00240: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2224 - acc: 0.9232 - val_loss: 0.2117 - val_acc: 0.9250
Epoch 241/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2173 - acc: 0.9256Epoch 00241: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2175 - acc: 0.9256 - val_loss: 0.2026 - val_acc: 0.9272
Epoch 242/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2206 - acc: 0.9246Epoch 00242: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2188 - acc: 0.9253 - val_loss: 0.2067 - val_acc: 0.9306
Epoch 243/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2214 - acc: 0.9244Epoch 00243: val_loss improved from 0.20067 to 0.19948, saving model to weights7.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.2189 - acc: 0.9248 - val_loss: 0.1995 - val_acc: 0.9261
Epoch 244/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2279 - acc: 0.9202Epoch 00244: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2283 - acc: 0.9201 - val_loss: 0.2015 - val_acc: 0.9272
Epoch 245/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2268 - acc: 0.9209Epoch 00245: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2246 - acc: 0.9218 - val_loss: 0.2031 - val_acc: 0.9289
Epoch 246/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2198 - acc: 0.9260Epoch 00246: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2195 - acc: 0.9265 - val_loss: 0.2059 - val_acc: 0.9261
Epoch 247/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2197 - acc: 0.9247Epoch 00247: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2212 - acc: 0.9242 - val_loss: 0.2026 - val_acc: 0.9283
Epoch 248/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.2165 - acc: 0.9278Epoch 00248: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2167 - acc: 0.9280 - val_loss: 0.2032 - val_acc: 0.9261
Epoch 249/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2229 - acc: 0.9251Epoch 00249: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2220 - acc: 0.9247 - val_loss: 0.2074 - val_acc: 0.9300
Epoch 250/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2138 - acc: 0.9269Epoch 00250: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2156 - acc: 0.9260 - val_loss: 0.2170 - val_acc: 0.9233
8


 trianing 



Train on 17894 samples, validate on 1786 samples
Epoch 1/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 5.7059 - acc: 0.0908Epoch 00001: val_loss improved from inf to 2.48459, saving model to weights8.hdf5
17894/17894 [==============================] - 3s 162us/step - loss: 5.5748 - acc: 0.0909 - val_loss: 2.4846 - val_acc: 0.0834
Epoch 2/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.5069 - acc: 0.0890- ETA: 0s - loss: 2.5174 Epoch 00002: val_loss improved from 2.48459 to 2.47300, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 2.5059 - acc: 0.0891 - val_loss: 2.4730 - val_acc: 0.1075
Epoch 3/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4742 - acc: 0.1023Epoch 00003: val_loss improved from 2.47300 to 2.43757, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.4740 - acc: 0.1024 - val_loss: 2.4376 - val_acc: 0.1075
Epoch 4/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4452 - acc: 0.1051Epoch 00004: val_loss improved from 2.43757 to 2.42579, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 95us/step - loss: 2.4434 - acc: 0.1060 - val_loss: 2.4258 - val_acc: 0.1081
Epoch 5/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4370 - acc: 0.1065Epoch 00005: val_loss improved from 2.42579 to 2.42391, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 2.4368 - acc: 0.1069 - val_loss: 2.4239 - val_acc: 0.1109
Epoch 6/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4302 - acc: 0.1111Epoch 00006: val_loss improved from 2.42391 to 2.42201, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.4310 - acc: 0.1109 - val_loss: 2.4220 - val_acc: 0.1125
Epoch 7/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.4246 - acc: 0.1191Epoch 00007: val_loss improved from 2.42201 to 2.40328, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.4258 - acc: 0.1181 - val_loss: 2.4033 - val_acc: 0.1366
Epoch 8/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.3913 - acc: 0.1374Epoch 00008: val_loss improved from 2.40328 to 2.27120, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.3902 - acc: 0.1378 - val_loss: 2.2712 - val_acc: 0.1898
Epoch 9/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.2938 - acc: 0.1719Epoch 00009: val_loss improved from 2.27120 to 2.06297, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.2885 - acc: 0.1742 - val_loss: 2.0630 - val_acc: 0.2570
Epoch 10/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 2.1145 - acc: 0.2353Epoch 00010: val_loss improved from 2.06297 to 1.84046, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 2.1124 - acc: 0.2369 - val_loss: 1.8405 - val_acc: 0.3253
Epoch 11/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.9724 - acc: 0.2843Epoch 00011: val_loss improved from 1.84046 to 1.69409, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 1.9712 - acc: 0.2845 - val_loss: 1.6941 - val_acc: 0.4362
Epoch 12/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.8383 - acc: 0.3434Epoch 00012: val_loss improved from 1.69409 to 1.56344, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 1.8353 - acc: 0.3435 - val_loss: 1.5634 - val_acc: 0.4569
Epoch 13/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.7020 - acc: 0.3962Epoch 00013: val_loss improved from 1.56344 to 1.35226, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 1.6988 - acc: 0.3977 - val_loss: 1.3523 - val_acc: 0.5571
Epoch 14/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.5685 - acc: 0.4444Epoch 00014: val_loss improved from 1.35226 to 1.21142, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 1.5665 - acc: 0.4452 - val_loss: 1.2114 - val_acc: 0.6120
Epoch 15/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.4476 - acc: 0.4897Epoch 00015: val_loss improved from 1.21142 to 1.11626, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 1.4465 - acc: 0.4907 - val_loss: 1.1163 - val_acc: 0.6271
Epoch 16/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.3357 - acc: 0.5306Epoch 00016: val_loss improved from 1.11626 to 1.02239, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 1.3364 - acc: 0.5296 - val_loss: 1.0224 - val_acc: 0.6680
Epoch 17/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.2504 - acc: 0.5629Epoch 00017: val_loss improved from 1.02239 to 0.90353, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 1.2504 - acc: 0.5631 - val_loss: 0.9035 - val_acc: 0.7178
Epoch 18/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.1659 - acc: 0.5922Epoch 00018: val_loss improved from 0.90353 to 0.84435, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 1.1665 - acc: 0.5925 - val_loss: 0.8443 - val_acc: 0.7279
Epoch 19/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.0912 - acc: 0.6175Epoch 00019: val_loss improved from 0.84435 to 0.77480, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 1.0893 - acc: 0.6174 - val_loss: 0.7748 - val_acc: 0.7480
Epoch 20/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.0410 - acc: 0.6405Epoch 00020: val_loss improved from 0.77480 to 0.74762, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 1.0428 - acc: 0.6390 - val_loss: 0.7476 - val_acc: 0.7581
Epoch 21/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 1.0091 - acc: 0.6513Epoch 00021: val_loss improved from 0.74762 to 0.68817, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 1.0067 - acc: 0.6522 - val_loss: 0.6882 - val_acc: 0.7749
Epoch 22/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.9626 - acc: 0.6683Epoch 00022: val_loss improved from 0.68817 to 0.66801, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.9630 - acc: 0.6685 - val_loss: 0.6680 - val_acc: 0.7872
Epoch 23/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.9314 - acc: 0.6802Epoch 00023: val_loss improved from 0.66801 to 0.63361, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.9298 - acc: 0.6811 - val_loss: 0.6336 - val_acc: 0.7912
Epoch 24/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.9031 - acc: 0.6902Epoch 00024: val_loss improved from 0.63361 to 0.60287, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.8993 - acc: 0.6916 - val_loss: 0.6029 - val_acc: 0.8057
Epoch 25/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.8612 - acc: 0.7041Epoch 00025: val_loss improved from 0.60287 to 0.58539, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.8617 - acc: 0.7042 - val_loss: 0.5854 - val_acc: 0.8068
Epoch 26/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.8238 - acc: 0.7148Epoch 00026: val_loss improved from 0.58539 to 0.56950, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.8244 - acc: 0.7147 - val_loss: 0.5695 - val_acc: 0.8180
Epoch 27/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.8127 - acc: 0.7208Epoch 00027: val_loss improved from 0.56950 to 0.53761, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.8106 - acc: 0.7220 - val_loss: 0.5376 - val_acc: 0.8309
Epoch 28/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.7834 - acc: 0.7286Epoch 00028: val_loss improved from 0.53761 to 0.52574, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.7847 - acc: 0.7285 - val_loss: 0.5257 - val_acc: 0.8298
Epoch 29/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.7540 - acc: 0.7428Epoch 00029: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.7549 - acc: 0.7424 - val_loss: 0.5385 - val_acc: 0.8242
Epoch 30/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.7459 - acc: 0.7483Epoch 00030: val_loss improved from 0.52574 to 0.49487, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.7461 - acc: 0.7482 - val_loss: 0.4949 - val_acc: 0.8399
Epoch 31/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.7163 - acc: 0.7569Epoch 00031: val_loss improved from 0.49487 to 0.48689, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.7168 - acc: 0.7566 - val_loss: 0.4869 - val_acc: 0.8337
Epoch 32/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.7219 - acc: 0.7552Epoch 00032: val_loss improved from 0.48689 to 0.46676, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.7220 - acc: 0.7554 - val_loss: 0.4668 - val_acc: 0.8438
Epoch 33/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.6948 - acc: 0.7624Epoch 00033: val_loss improved from 0.46676 to 0.46404, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.6973 - acc: 0.7605 - val_loss: 0.4640 - val_acc: 0.8460
Epoch 34/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.6905 - acc: 0.7662Epoch 00034: val_loss improved from 0.46404 to 0.44944, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.6897 - acc: 0.7674 - val_loss: 0.4494 - val_acc: 0.8483
Epoch 35/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.6729 - acc: 0.7740Epoch 00035: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.6748 - acc: 0.7731 - val_loss: 0.4523 - val_acc: 0.8488
Epoch 36/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.6563 - acc: 0.7783Epoch 00036: val_loss improved from 0.44944 to 0.43423, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.6557 - acc: 0.7786 - val_loss: 0.4342 - val_acc: 0.8583
Epoch 37/250
17408/17894 [============================&gt;.] - ETA: 0s - loss: 0.6373 - acc: 0.7833Epoch 00037: val_loss improved from 0.43423 to 0.42163, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.6383 - acc: 0.7825 - val_loss: 0.4216 - val_acc: 0.8623
Epoch 38/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.6314 - acc: 0.7793Epoch 00038: val_loss improved from 0.42163 to 0.41235, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.6313 - acc: 0.7794 - val_loss: 0.4124 - val_acc: 0.8634
Epoch 39/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.6233 - acc: 0.7886Epoch 00039: val_loss improved from 0.41235 to 0.40932, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.6220 - acc: 0.7891 - val_loss: 0.4093 - val_acc: 0.8617
Epoch 40/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.6124 - acc: 0.7923Epoch 00040: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.6131 - acc: 0.7919 - val_loss: 0.4123 - val_acc: 0.8623
Epoch 41/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.6079 - acc: 0.7963Epoch 00041: val_loss improved from 0.40932 to 0.40071, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.6062 - acc: 0.7971 - val_loss: 0.4007 - val_acc: 0.8600
Epoch 42/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5977 - acc: 0.7978Epoch 00042: val_loss improved from 0.40071 to 0.39449, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.5974 - acc: 0.7978 - val_loss: 0.3945 - val_acc: 0.8684
Epoch 43/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5873 - acc: 0.8010Epoch 00043: val_loss improved from 0.39449 to 0.39156, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 91us/step - loss: 0.5892 - acc: 0.8003 - val_loss: 0.3916 - val_acc: 0.8673
Epoch 44/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.5723 - acc: 0.8063Epoch 00044: val_loss improved from 0.39156 to 0.37805, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 95us/step - loss: 0.5713 - acc: 0.8064 - val_loss: 0.3780 - val_acc: 0.8774
Epoch 45/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5687 - acc: 0.8059Epoch 00045: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.5672 - acc: 0.8064 - val_loss: 0.3822 - val_acc: 0.8723
Epoch 46/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5575 - acc: 0.8108Epoch 00046: val_loss improved from 0.37805 to 0.37741, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.5568 - acc: 0.8113 - val_loss: 0.3774 - val_acc: 0.8729
Epoch 47/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5484 - acc: 0.8142Epoch 00047: val_loss improved from 0.37741 to 0.36352, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.5479 - acc: 0.8149 - val_loss: 0.3635 - val_acc: 0.8802
Epoch 48/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5442 - acc: 0.8168Epoch 00048: val_loss improved from 0.36352 to 0.35686, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.5422 - acc: 0.8178 - val_loss: 0.3569 - val_acc: 0.8819
Epoch 49/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5428 - acc: 0.8180Epoch 00049: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.5440 - acc: 0.8171 - val_loss: 0.3625 - val_acc: 0.8740
Epoch 50/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5326 - acc: 0.8198Epoch 00050: val_loss improved from 0.35686 to 0.34951, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.5317 - acc: 0.8203 - val_loss: 0.3495 - val_acc: 0.8819
Epoch 51/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5186 - acc: 0.8268Epoch 00051: val_loss improved from 0.34951 to 0.34094, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.5155 - acc: 0.8278 - val_loss: 0.3409 - val_acc: 0.8886
Epoch 52/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5199 - acc: 0.8251Epoch 00052: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.5184 - acc: 0.8256 - val_loss: 0.3465 - val_acc: 0.8830
Epoch 53/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5166 - acc: 0.8268Epoch 00053: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.5162 - acc: 0.8268 - val_loss: 0.3438 - val_acc: 0.8830
Epoch 54/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5065 - acc: 0.8306Epoch 00054: val_loss improved from 0.34094 to 0.33148, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 91us/step - loss: 0.5056 - acc: 0.8308 - val_loss: 0.3315 - val_acc: 0.8886
Epoch 55/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5066 - acc: 0.8302Epoch 00055: val_loss improved from 0.33148 to 0.32846, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 91us/step - loss: 0.5062 - acc: 0.8303 - val_loss: 0.3285 - val_acc: 0.8970
Epoch 56/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.5054 - acc: 0.8313Epoch 00056: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.5066 - acc: 0.8309 - val_loss: 0.3348 - val_acc: 0.8931
Epoch 57/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4835 - acc: 0.8356Epoch 00057: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.4824 - acc: 0.8359 - val_loss: 0.3321 - val_acc: 0.8897
Epoch 58/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4720 - acc: 0.8413Epoch 00058: val_loss improved from 0.32846 to 0.31808, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4701 - acc: 0.8425 - val_loss: 0.3181 - val_acc: 0.8931
Epoch 59/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4777 - acc: 0.8418Epoch 00059: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.4795 - acc: 0.8413 - val_loss: 0.3251 - val_acc: 0.8936
Epoch 60/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4771 - acc: 0.8405Epoch 00060: val_loss improved from 0.31808 to 0.31570, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.4780 - acc: 0.8410 - val_loss: 0.3157 - val_acc: 0.8919
Epoch 61/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4709 - acc: 0.8449Epoch 00061: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.4695 - acc: 0.8445 - val_loss: 0.3159 - val_acc: 0.8981
Epoch 62/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4663 - acc: 0.8424Epoch 00062: val_loss improved from 0.31570 to 0.30552, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4645 - acc: 0.8431 - val_loss: 0.3055 - val_acc: 0.8981
Epoch 63/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4602 - acc: 0.8433Epoch 00063: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.4590 - acc: 0.8439 - val_loss: 0.3124 - val_acc: 0.8981
Epoch 64/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4551 - acc: 0.8465Epoch 00064: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.4559 - acc: 0.8456 - val_loss: 0.3102 - val_acc: 0.9009
Epoch 65/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4460 - acc: 0.8495Epoch 00065: val_loss improved from 0.30552 to 0.30468, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.4467 - acc: 0.8492 - val_loss: 0.3047 - val_acc: 0.9026
Epoch 66/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4370 - acc: 0.8551Epoch 00066: val_loss improved from 0.30468 to 0.29837, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.4375 - acc: 0.8546 - val_loss: 0.2984 - val_acc: 0.9076
Epoch 67/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4402 - acc: 0.8518Epoch 00067: val_loss improved from 0.29837 to 0.29610, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4405 - acc: 0.8520 - val_loss: 0.2961 - val_acc: 0.9065
Epoch 68/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4349 - acc: 0.8535Epoch 00068: val_loss improved from 0.29610 to 0.29479, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.4343 - acc: 0.8533 - val_loss: 0.2948 - val_acc: 0.9026
Epoch 69/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4375 - acc: 0.8541Epoch 00069: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.4368 - acc: 0.8543 - val_loss: 0.2951 - val_acc: 0.9054
Epoch 70/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4365 - acc: 0.8510Epoch 00070: val_loss improved from 0.29479 to 0.29314, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4352 - acc: 0.8513 - val_loss: 0.2931 - val_acc: 0.9071
Epoch 71/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4239 - acc: 0.8552Epoch 00071: val_loss improved from 0.29314 to 0.28912, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4238 - acc: 0.8554 - val_loss: 0.2891 - val_acc: 0.9059
Epoch 72/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4289 - acc: 0.8584Epoch 00072: val_loss improved from 0.28912 to 0.28351, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4275 - acc: 0.8587 - val_loss: 0.2835 - val_acc: 0.9093
Epoch 73/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4267 - acc: 0.8544Epoch 00073: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.4273 - acc: 0.8544 - val_loss: 0.2846 - val_acc: 0.9104
Epoch 74/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4157 - acc: 0.8619Epoch 00074: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.4177 - acc: 0.8619 - val_loss: 0.2844 - val_acc: 0.9065
Epoch 75/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4191 - acc: 0.8574Epoch 00075: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.4186 - acc: 0.8574 - val_loss: 0.2871 - val_acc: 0.9087
Epoch 76/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4075 - acc: 0.8620Epoch 00076: val_loss improved from 0.28351 to 0.28159, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.4060 - acc: 0.8625 - val_loss: 0.2816 - val_acc: 0.9121
Epoch 77/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4082 - acc: 0.8642Epoch 00077: val_loss improved from 0.28159 to 0.27654, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 91us/step - loss: 0.4085 - acc: 0.8635 - val_loss: 0.2765 - val_acc: 0.9099
Epoch 78/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4056 - acc: 0.8647Epoch 00078: val_loss improved from 0.27654 to 0.27076, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.4058 - acc: 0.8644 - val_loss: 0.2708 - val_acc: 0.9127
Epoch 79/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3956 - acc: 0.8684Epoch 00079: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3949 - acc: 0.8691 - val_loss: 0.2740 - val_acc: 0.9171
Epoch 80/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.4069 - acc: 0.8633Epoch 00080: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.4040 - acc: 0.8643 - val_loss: 0.2717 - val_acc: 0.9143
Epoch 81/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.3916 - acc: 0.8697Epoch 00081: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3920 - acc: 0.8694 - val_loss: 0.2708 - val_acc: 0.9143
Epoch 82/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3884 - acc: 0.8670Epoch 00082: val_loss improved from 0.27076 to 0.26483, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.3874 - acc: 0.8674 - val_loss: 0.2648 - val_acc: 0.9171
Epoch 83/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3868 - acc: 0.8691Epoch 00083: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3858 - acc: 0.8698 - val_loss: 0.2707 - val_acc: 0.9115
Epoch 84/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3908 - acc: 0.8686- ETA: 0s - loss: 0.3843 Epoch 00084: val_loss improved from 0.26483 to 0.26194, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.3893 - acc: 0.8690 - val_loss: 0.2619 - val_acc: 0.9205
Epoch 85/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.3852 - acc: 0.8722Epoch 00085: val_loss improved from 0.26194 to 0.25817, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.3849 - acc: 0.8720 - val_loss: 0.2582 - val_acc: 0.9188
Epoch 86/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3878 - acc: 0.8691Epoch 00086: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3894 - acc: 0.8681 - val_loss: 0.2591 - val_acc: 0.9177
Epoch 87/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3779 - acc: 0.8737Epoch 00087: val_loss improved from 0.25817 to 0.25566, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.3786 - acc: 0.8735 - val_loss: 0.2557 - val_acc: 0.9160
Epoch 88/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3715 - acc: 0.8735Epoch 00088: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3727 - acc: 0.8735 - val_loss: 0.2564 - val_acc: 0.9171
Epoch 89/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3756 - acc: 0.8742Epoch 00089: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.3752 - acc: 0.8743 - val_loss: 0.2573 - val_acc: 0.9143
Epoch 90/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3741 - acc: 0.8758Epoch 00090: val_loss improved from 0.25566 to 0.24865, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.3725 - acc: 0.8761 - val_loss: 0.2487 - val_acc: 0.9188
Epoch 91/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3588 - acc: 0.8809Epoch 00091: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.3614 - acc: 0.8798 - val_loss: 0.2515 - val_acc: 0.9160
Epoch 92/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3629 - acc: 0.8798Epoch 00092: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.3625 - acc: 0.8799 - val_loss: 0.2495 - val_acc: 0.9194
Epoch 93/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3614 - acc: 0.8774Epoch 00093: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3608 - acc: 0.8777 - val_loss: 0.2537 - val_acc: 0.9171
Epoch 94/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3682 - acc: 0.8760Epoch 00094: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3678 - acc: 0.8758 - val_loss: 0.2488 - val_acc: 0.9216
Epoch 95/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3517 - acc: 0.8819Epoch 00095: val_loss improved from 0.24865 to 0.24769, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.3541 - acc: 0.8816 - val_loss: 0.2477 - val_acc: 0.9216
Epoch 96/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3599 - acc: 0.8789Epoch 00096: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3605 - acc: 0.8786 - val_loss: 0.2483 - val_acc: 0.9211
Epoch 97/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3506 - acc: 0.8821Epoch 00097: val_loss improved from 0.24769 to 0.24566, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.3497 - acc: 0.8822 - val_loss: 0.2457 - val_acc: 0.9261
Epoch 98/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3512 - acc: 0.8809Epoch 00098: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3529 - acc: 0.8806 - val_loss: 0.2552 - val_acc: 0.9149
Epoch 99/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3590 - acc: 0.8824Epoch 00099: val_loss improved from 0.24566 to 0.23935, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.3573 - acc: 0.8830 - val_loss: 0.2393 - val_acc: 0.9267
Epoch 100/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3485 - acc: 0.8809Epoch 00100: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3468 - acc: 0.8814 - val_loss: 0.2437 - val_acc: 0.9227
Epoch 101/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3372 - acc: 0.8849Epoch 00101: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.3401 - acc: 0.8842 - val_loss: 0.2400 - val_acc: 0.9233
Epoch 102/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3409 - acc: 0.8867Epoch 00102: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3432 - acc: 0.8863 - val_loss: 0.2460 - val_acc: 0.9222
Epoch 103/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3422 - acc: 0.8864Epoch 00103: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3418 - acc: 0.8865 - val_loss: 0.2439 - val_acc: 0.9239
Epoch 104/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3409 - acc: 0.8843Epoch 00104: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.3379 - acc: 0.8852 - val_loss: 0.2435 - val_acc: 0.9199
Epoch 105/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3335 - acc: 0.8860Epoch 00105: val_loss improved from 0.23935 to 0.23675, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.3340 - acc: 0.8858 - val_loss: 0.2368 - val_acc: 0.9233
Epoch 106/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3343 - acc: 0.8872Epoch 00106: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3350 - acc: 0.8870 - val_loss: 0.2370 - val_acc: 0.9244
Epoch 107/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.3267 - acc: 0.8896Epoch 00107: val_loss improved from 0.23675 to 0.23112, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 96us/step - loss: 0.3281 - acc: 0.8891 - val_loss: 0.2311 - val_acc: 0.9244
Epoch 108/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3326 - acc: 0.8850Epoch 00108: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.3323 - acc: 0.8854 - val_loss: 0.2430 - val_acc: 0.9211
Epoch 109/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.3252 - acc: 0.8873Epoch 00109: val_loss did not improve
17894/17894 [==============================] - 2s 94us/step - loss: 0.3252 - acc: 0.8874 - val_loss: 0.2385 - val_acc: 0.9267
Epoch 110/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3220 - acc: 0.8887Epoch 00110: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.3209 - acc: 0.8890 - val_loss: 0.2385 - val_acc: 0.9216
Epoch 111/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3214 - acc: 0.8908Epoch 00111: val_loss improved from 0.23112 to 0.22870, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 95us/step - loss: 0.3208 - acc: 0.8905 - val_loss: 0.2287 - val_acc: 0.9244
Epoch 112/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3205 - acc: 0.8907Epoch 00112: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3227 - acc: 0.8900 - val_loss: 0.2317 - val_acc: 0.9205
Epoch 113/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3221 - acc: 0.8900Epoch 00113: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3199 - acc: 0.8908 - val_loss: 0.2347 - val_acc: 0.9222
Epoch 114/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3236 - acc: 0.8893Epoch 00114: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3241 - acc: 0.8895 - val_loss: 0.2336 - val_acc: 0.9227
Epoch 115/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3178 - acc: 0.8915Epoch 00115: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.3188 - acc: 0.8915 - val_loss: 0.2341 - val_acc: 0.9233
Epoch 116/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3172 - acc: 0.8944Epoch 00116: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.3162 - acc: 0.8948 - val_loss: 0.2345 - val_acc: 0.9244
Epoch 117/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3099 - acc: 0.8950Epoch 00117: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3088 - acc: 0.8956 - val_loss: 0.2300 - val_acc: 0.9255
Epoch 118/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3200 - acc: 0.8902Epoch 00118: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3192 - acc: 0.8904 - val_loss: 0.2288 - val_acc: 0.9278
Epoch 119/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3139 - acc: 0.8925Epoch 00119: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3146 - acc: 0.8925 - val_loss: 0.2311 - val_acc: 0.9227
Epoch 120/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3167 - acc: 0.8916Epoch 00120: val_loss improved from 0.22870 to 0.22674, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.3173 - acc: 0.8918 - val_loss: 0.2267 - val_acc: 0.9255
Epoch 121/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3117 - acc: 0.8966Epoch 00121: val_loss improved from 0.22674 to 0.22231, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.3118 - acc: 0.8962 - val_loss: 0.2223 - val_acc: 0.9283
Epoch 122/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3102 - acc: 0.8936Epoch 00122: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3101 - acc: 0.8940 - val_loss: 0.2259 - val_acc: 0.9233
Epoch 123/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3107 - acc: 0.8944Epoch 00123: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3100 - acc: 0.8948 - val_loss: 0.2263 - val_acc: 0.9278
Epoch 124/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3076 - acc: 0.8951Epoch 00124: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.3081 - acc: 0.8947 - val_loss: 0.2230 - val_acc: 0.9233
Epoch 125/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3041 - acc: 0.8948Epoch 00125: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3048 - acc: 0.8946 - val_loss: 0.2273 - val_acc: 0.9250
Epoch 126/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2979 - acc: 0.8970Epoch 00126: val_loss improved from 0.22231 to 0.21890, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.2985 - acc: 0.8971 - val_loss: 0.2189 - val_acc: 0.9255
Epoch 127/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2998 - acc: 0.8988Epoch 00127: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2999 - acc: 0.8988 - val_loss: 0.2190 - val_acc: 0.9295
Epoch 128/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2956 - acc: 0.9005Epoch 00128: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2966 - acc: 0.9002 - val_loss: 0.2234 - val_acc: 0.9295
Epoch 129/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.3023 - acc: 0.8975Epoch 00129: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.3010 - acc: 0.8975 - val_loss: 0.2273 - val_acc: 0.9261
Epoch 130/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2956 - acc: 0.9006Epoch 00130: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2959 - acc: 0.9000 - val_loss: 0.2198 - val_acc: 0.9278
Epoch 131/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2958 - acc: 0.8996Epoch 00131: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2975 - acc: 0.8990 - val_loss: 0.2210 - val_acc: 0.9272
Epoch 132/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2978 - acc: 0.8967Epoch 00132: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2967 - acc: 0.8974 - val_loss: 0.2210 - val_acc: 0.9272
Epoch 133/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2973 - acc: 0.9008Epoch 00133: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2973 - acc: 0.9006 - val_loss: 0.2195 - val_acc: 0.9289
Epoch 134/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2954 - acc: 0.8996Epoch 00134: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2938 - acc: 0.8999 - val_loss: 0.2192 - val_acc: 0.9300
Epoch 135/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2984 - acc: 0.8983Epoch 00135: val_loss did not improve
17894/17894 [==============================] - 2s 94us/step - loss: 0.2978 - acc: 0.8983 - val_loss: 0.2202 - val_acc: 0.9255
Epoch 136/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2910 - acc: 0.9008Epoch 00136: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2900 - acc: 0.9013 - val_loss: 0.2296 - val_acc: 0.9261
Epoch 137/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2811 - acc: 0.9040Epoch 00137: val_loss improved from 0.21890 to 0.21413, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 95us/step - loss: 0.2800 - acc: 0.9038 - val_loss: 0.2141 - val_acc: 0.9278
Epoch 138/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2905 - acc: 0.9022Epoch 00138: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2912 - acc: 0.9023 - val_loss: 0.2245 - val_acc: 0.9227
Epoch 139/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2843 - acc: 0.9045Epoch 00139: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2835 - acc: 0.9051 - val_loss: 0.2179 - val_acc: 0.9283
Epoch 140/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2805 - acc: 0.9043Epoch 00140: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2805 - acc: 0.9047 - val_loss: 0.2180 - val_acc: 0.9278
Epoch 141/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2827 - acc: 0.9029Epoch 00141: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2825 - acc: 0.9025 - val_loss: 0.2146 - val_acc: 0.9295
Epoch 142/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2868 - acc: 0.9039Epoch 00142: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2865 - acc: 0.9039 - val_loss: 0.2169 - val_acc: 0.9317
Epoch 143/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2850 - acc: 0.9046Epoch 00143: val_loss improved from 0.21413 to 0.21127, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.2843 - acc: 0.9050 - val_loss: 0.2113 - val_acc: 0.9300
Epoch 144/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2764 - acc: 0.9067Epoch 00144: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2769 - acc: 0.9064 - val_loss: 0.2193 - val_acc: 0.9283
Epoch 145/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2788 - acc: 0.9067Epoch 00145: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2790 - acc: 0.9068 - val_loss: 0.2201 - val_acc: 0.9317
Epoch 146/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2821 - acc: 0.9064Epoch 00146: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2831 - acc: 0.9054 - val_loss: 0.2179 - val_acc: 0.9255
Epoch 147/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2688 - acc: 0.9076Epoch 00147: val_loss improved from 0.21127 to 0.21056, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.2678 - acc: 0.9077 - val_loss: 0.2106 - val_acc: 0.9306
Epoch 148/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2762 - acc: 0.9053Epoch 00148: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2751 - acc: 0.9054 - val_loss: 0.2111 - val_acc: 0.9356
Epoch 149/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2691 - acc: 0.9079Epoch 00149: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2717 - acc: 0.9071 - val_loss: 0.2122 - val_acc: 0.9323
Epoch 150/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2803 - acc: 0.9044Epoch 00150: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2791 - acc: 0.9051 - val_loss: 0.2144 - val_acc: 0.9311
Epoch 151/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2733 - acc: 0.9071Epoch 00151: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2721 - acc: 0.9074 - val_loss: 0.2182 - val_acc: 0.9283
Epoch 152/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2731 - acc: 0.9067Epoch 00152: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2715 - acc: 0.9072 - val_loss: 0.2174 - val_acc: 0.9367
Epoch 153/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2694 - acc: 0.9072Epoch 00153: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2697 - acc: 0.9072 - val_loss: 0.2131 - val_acc: 0.9311
Epoch 154/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2727 - acc: 0.9085Epoch 00154: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2709 - acc: 0.9088 - val_loss: 0.2108 - val_acc: 0.9317
Epoch 155/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2761 - acc: 0.9069Epoch 00155: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2740 - acc: 0.9072 - val_loss: 0.2112 - val_acc: 0.9351
Epoch 156/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2651 - acc: 0.9078Epoch 00156: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2644 - acc: 0.9079 - val_loss: 0.2133 - val_acc: 0.9334
Epoch 157/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2695 - acc: 0.9053Epoch 00157: val_loss improved from 0.21056 to 0.20976, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.2689 - acc: 0.9056 - val_loss: 0.2098 - val_acc: 0.9323
Epoch 158/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2624 - acc: 0.9120Epoch 00158: val_loss improved from 0.20976 to 0.20913, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.2639 - acc: 0.9116 - val_loss: 0.2091 - val_acc: 0.9306
Epoch 159/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2690 - acc: 0.9080Epoch 00159: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2689 - acc: 0.9078 - val_loss: 0.2091 - val_acc: 0.9334
Epoch 160/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2652 - acc: 0.9095Epoch 00160: val_loss improved from 0.20913 to 0.20519, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.2657 - acc: 0.9096 - val_loss: 0.2052 - val_acc: 0.9323
Epoch 161/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2579 - acc: 0.9129Epoch 00161: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2584 - acc: 0.9124 - val_loss: 0.2143 - val_acc: 0.9311
Epoch 162/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2572 - acc: 0.9103Epoch 00162: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2586 - acc: 0.9097 - val_loss: 0.2093 - val_acc: 0.9334
Epoch 163/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2618 - acc: 0.9145Epoch 00163: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2631 - acc: 0.9140 - val_loss: 0.2133 - val_acc: 0.9295
Epoch 164/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2561 - acc: 0.9102Epoch 00164: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2570 - acc: 0.9105 - val_loss: 0.2094 - val_acc: 0.9334
Epoch 165/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2615 - acc: 0.9109Epoch 00165: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2609 - acc: 0.9111 - val_loss: 0.2086 - val_acc: 0.9317
Epoch 166/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2617 - acc: 0.9125Epoch 00166: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2622 - acc: 0.9129 - val_loss: 0.2067 - val_acc: 0.9295
Epoch 167/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2557 - acc: 0.9121Epoch 00167: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2570 - acc: 0.9117 - val_loss: 0.2094 - val_acc: 0.9306
Epoch 168/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2505 - acc: 0.9150Epoch 00168: val_loss improved from 0.20519 to 0.20486, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.2532 - acc: 0.9140 - val_loss: 0.2049 - val_acc: 0.9311
Epoch 169/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2518 - acc: 0.9152Epoch 00169: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2516 - acc: 0.9154 - val_loss: 0.2128 - val_acc: 0.9334
Epoch 170/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2606 - acc: 0.9105Epoch 00170: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2601 - acc: 0.9106 - val_loss: 0.2077 - val_acc: 0.9306
Epoch 171/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2598 - acc: 0.9126Epoch 00171: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2598 - acc: 0.9121 - val_loss: 0.2056 - val_acc: 0.9328
Epoch 172/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2547 - acc: 0.9127Epoch 00172: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2549 - acc: 0.9129 - val_loss: 0.2111 - val_acc: 0.9300
Epoch 173/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2596 - acc: 0.9117Epoch 00173: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2599 - acc: 0.9113 - val_loss: 0.2122 - val_acc: 0.9306
Epoch 174/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2512 - acc: 0.9137Epoch 00174: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2508 - acc: 0.9138 - val_loss: 0.2079 - val_acc: 0.9317
Epoch 175/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2566 - acc: 0.9151- ETA: 0s - loss: 0.2536 - acc: 0.91Epoch 00175: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2579 - acc: 0.9143 - val_loss: 0.2049 - val_acc: 0.9351
Epoch 176/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2530 - acc: 0.9129Epoch 00176: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2542 - acc: 0.9130 - val_loss: 0.2078 - val_acc: 0.9317
Epoch 177/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2406 - acc: 0.9177Epoch 00177: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2417 - acc: 0.9174 - val_loss: 0.2106 - val_acc: 0.9278
Epoch 178/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2502 - acc: 0.9156Epoch 00178: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2521 - acc: 0.9147 - val_loss: 0.2070 - val_acc: 0.9345
Epoch 179/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2477 - acc: 0.9175Epoch 00179: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2485 - acc: 0.9172 - val_loss: 0.2073 - val_acc: 0.9306
Epoch 180/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2429 - acc: 0.9173Epoch 00180: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2434 - acc: 0.9171 - val_loss: 0.2114 - val_acc: 0.9295
Epoch 181/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2474 - acc: 0.9160Epoch 00181: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2484 - acc: 0.9157 - val_loss: 0.2055 - val_acc: 0.9323
Epoch 182/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2437 - acc: 0.9159- ETA: 0s - loss: 0.2364 -Epoch 00182: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2448 - acc: 0.9153 - val_loss: 0.2088 - val_acc: 0.9351
Epoch 183/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2450 - acc: 0.9182Epoch 00183: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2450 - acc: 0.9182 - val_loss: 0.2114 - val_acc: 0.9328
Epoch 184/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2459 - acc: 0.9172Epoch 00184: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2462 - acc: 0.9166 - val_loss: 0.2061 - val_acc: 0.9356
Epoch 185/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2406 - acc: 0.9159Epoch 00185: val_loss improved from 0.20486 to 0.20483, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.2422 - acc: 0.9155 - val_loss: 0.2048 - val_acc: 0.9362
Epoch 186/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2503 - acc: 0.9163- ETA: 0s - loss: 0.2478 - accEpoch 00186: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2504 - acc: 0.9161 - val_loss: 0.2097 - val_acc: 0.9317
Epoch 187/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2450 - acc: 0.9166Epoch 00187: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2438 - acc: 0.9170 - val_loss: 0.2061 - val_acc: 0.9345
Epoch 188/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2415 - acc: 0.9149Epoch 00188: val_loss improved from 0.20483 to 0.20436, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.2413 - acc: 0.9150 - val_loss: 0.2044 - val_acc: 0.9351
Epoch 189/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2451 - acc: 0.9183Epoch 00189: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2453 - acc: 0.9180 - val_loss: 0.2085 - val_acc: 0.9323
Epoch 190/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2316 - acc: 0.9216Epoch 00190: val_loss improved from 0.20436 to 0.20391, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.2326 - acc: 0.9212 - val_loss: 0.2039 - val_acc: 0.9311
Epoch 191/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2332 - acc: 0.9201Epoch 00191: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2352 - acc: 0.9200 - val_loss: 0.2056 - val_acc: 0.9306
Epoch 192/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2404 - acc: 0.9159- ETA: 0s - loss: 0.2334 - accEpoch 00192: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2391 - acc: 0.9163 - val_loss: 0.2059 - val_acc: 0.9323
Epoch 193/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2419 - acc: 0.9183Epoch 00193: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2405 - acc: 0.9181 - val_loss: 0.2062 - val_acc: 0.9306
Epoch 194/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2334 - acc: 0.9224Epoch 00194: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2336 - acc: 0.9224 - val_loss: 0.2074 - val_acc: 0.9311
Epoch 195/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2338 - acc: 0.9199Epoch 00195: val_loss improved from 0.20391 to 0.19895, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 93us/step - loss: 0.2347 - acc: 0.9195 - val_loss: 0.1989 - val_acc: 0.9373
Epoch 196/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2425 - acc: 0.9172Epoch 00196: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2423 - acc: 0.9170 - val_loss: 0.2014 - val_acc: 0.9373
Epoch 197/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2372 - acc: 0.9202Epoch 00197: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2369 - acc: 0.9204 - val_loss: 0.1992 - val_acc: 0.9378
Epoch 198/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2355 - acc: 0.9205Epoch 00198: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2371 - acc: 0.9204 - val_loss: 0.2042 - val_acc: 0.9351
Epoch 199/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2368 - acc: 0.9202Epoch 00199: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2362 - acc: 0.9201 - val_loss: 0.2021 - val_acc: 0.9373
Epoch 200/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2342 - acc: 0.9205Epoch 00200: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2359 - acc: 0.9199 - val_loss: 0.2083 - val_acc: 0.9317
Epoch 201/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2302 - acc: 0.9191Epoch 00201: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2290 - acc: 0.9198 - val_loss: 0.2103 - val_acc: 0.9323
Epoch 202/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2299 - acc: 0.9193Epoch 00202: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2294 - acc: 0.9197 - val_loss: 0.2024 - val_acc: 0.9311
Epoch 203/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2330 - acc: 0.9197Epoch 00203: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2352 - acc: 0.9190 - val_loss: 0.2033 - val_acc: 0.9351
Epoch 204/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2360 - acc: 0.9192Epoch 00204: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2339 - acc: 0.9201 - val_loss: 0.2084 - val_acc: 0.9362
Epoch 205/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2253 - acc: 0.9232Epoch 00205: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2265 - acc: 0.9230 - val_loss: 0.2066 - val_acc: 0.9345
Epoch 206/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2274 - acc: 0.9210Epoch 00206: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2277 - acc: 0.9207 - val_loss: 0.2066 - val_acc: 0.9345
Epoch 207/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2301 - acc: 0.9211- ETA: 1s - loss: 0.2Epoch 00207: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2299 - acc: 0.9215 - val_loss: 0.2018 - val_acc: 0.9362
Epoch 208/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2269 - acc: 0.9207Epoch 00208: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2277 - acc: 0.9207 - val_loss: 0.2052 - val_acc: 0.9378
Epoch 209/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2187 - acc: 0.9267Epoch 00209: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2178 - acc: 0.9269 - val_loss: 0.2028 - val_acc: 0.9356
Epoch 210/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2269 - acc: 0.9222Epoch 00210: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2272 - acc: 0.9222 - val_loss: 0.2051 - val_acc: 0.9362
Epoch 211/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2222 - acc: 0.9243Epoch 00211: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2217 - acc: 0.9244 - val_loss: 0.2049 - val_acc: 0.9373
Epoch 212/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2329 - acc: 0.9194Epoch 00212: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2307 - acc: 0.9203 - val_loss: 0.2087 - val_acc: 0.9311
Epoch 213/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2219 - acc: 0.9241Epoch 00213: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2219 - acc: 0.9242 - val_loss: 0.2051 - val_acc: 0.9362
Epoch 214/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2224 - acc: 0.9239Epoch 00214: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2212 - acc: 0.9242 - val_loss: 0.2075 - val_acc: 0.9345
Epoch 215/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2291 - acc: 0.9238Epoch 00215: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2294 - acc: 0.9234 - val_loss: 0.2073 - val_acc: 0.9317
Epoch 216/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2218 - acc: 0.9248Epoch 00216: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2208 - acc: 0.9254 - val_loss: 0.2020 - val_acc: 0.9356
Epoch 217/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2218 - acc: 0.9227Epoch 00217: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2249 - acc: 0.9218 - val_loss: 0.2047 - val_acc: 0.9356
Epoch 218/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2213 - acc: 0.9232Epoch 00218: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2220 - acc: 0.9231 - val_loss: 0.2073 - val_acc: 0.9356
Epoch 219/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2198 - acc: 0.9222Epoch 00219: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2197 - acc: 0.9222 - val_loss: 0.2107 - val_acc: 0.9328
Epoch 220/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2217 - acc: 0.9254Epoch 00220: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2198 - acc: 0.9257 - val_loss: 0.2086 - val_acc: 0.9345
Epoch 221/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2194 - acc: 0.9261- ETA: 1s - loss: 0.2242Epoch 00221: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2197 - acc: 0.9261 - val_loss: 0.2061 - val_acc: 0.9323
Epoch 222/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2213 - acc: 0.9248Epoch 00222: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2187 - acc: 0.9256 - val_loss: 0.2114 - val_acc: 0.9339
Epoch 223/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2186 - acc: 0.9239Epoch 00223: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2184 - acc: 0.9241 - val_loss: 0.2079 - val_acc: 0.9378
Epoch 224/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2165 - acc: 0.9265Epoch 00224: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2185 - acc: 0.9256 - val_loss: 0.2049 - val_acc: 0.9351
Epoch 225/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2134 - acc: 0.9263Epoch 00225: val_loss did not improve
17894/17894 [==============================] - 2s 89us/step - loss: 0.2135 - acc: 0.9261 - val_loss: 0.2077 - val_acc: 0.9367
Epoch 226/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2233 - acc: 0.9236Epoch 00226: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2228 - acc: 0.9232 - val_loss: 0.2038 - val_acc: 0.9328
Epoch 227/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2150 - acc: 0.9263Epoch 00227: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2155 - acc: 0.9260 - val_loss: 0.2082 - val_acc: 0.9317
Epoch 228/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2136 - acc: 0.9271Epoch 00228: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2153 - acc: 0.9267 - val_loss: 0.2051 - val_acc: 0.9373
Epoch 229/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2172 - acc: 0.9253Epoch 00229: val_loss improved from 0.19895 to 0.19858, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 94us/step - loss: 0.2167 - acc: 0.9253 - val_loss: 0.1986 - val_acc: 0.9384
Epoch 230/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2228 - acc: 0.9242Epoch 00230: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2236 - acc: 0.9242 - val_loss: 0.2056 - val_acc: 0.9378
Epoch 231/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2181 - acc: 0.9243Epoch 00231: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2182 - acc: 0.9240 - val_loss: 0.2077 - val_acc: 0.9351
Epoch 232/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2163 - acc: 0.9267Epoch 00232: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2163 - acc: 0.9268 - val_loss: 0.2035 - val_acc: 0.9384
Epoch 233/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2152 - acc: 0.9267Epoch 00233: val_loss improved from 0.19858 to 0.19742, saving model to weights8.hdf5
17894/17894 [==============================] - 2s 92us/step - loss: 0.2144 - acc: 0.9268 - val_loss: 0.1974 - val_acc: 0.9373
Epoch 234/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2125 - acc: 0.9278Epoch 00234: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2136 - acc: 0.9275 - val_loss: 0.2056 - val_acc: 0.9345
Epoch 235/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2138 - acc: 0.9260Epoch 00235: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2139 - acc: 0.9257 - val_loss: 0.2094 - val_acc: 0.9334
Epoch 236/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2066 - acc: 0.9311Epoch 00236: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2068 - acc: 0.9309 - val_loss: 0.2037 - val_acc: 0.9390
Epoch 237/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2105 - acc: 0.9286Epoch 00237: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2099 - acc: 0.9284 - val_loss: 0.2076 - val_acc: 0.9406
Epoch 238/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2127 - acc: 0.9279Epoch 00238: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2116 - acc: 0.9279 - val_loss: 0.2036 - val_acc: 0.9345
Epoch 239/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2118 - acc: 0.9281Epoch 00239: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2122 - acc: 0.9283 - val_loss: 0.2037 - val_acc: 0.9378
Epoch 240/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2235 - acc: 0.9275Epoch 00240: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2228 - acc: 0.9277 - val_loss: 0.2006 - val_acc: 0.9390
Epoch 241/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2160 - acc: 0.9261Epoch 00241: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2153 - acc: 0.9267 - val_loss: 0.2110 - val_acc: 0.9351
Epoch 242/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2135 - acc: 0.9248Epoch 00242: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2123 - acc: 0.9257 - val_loss: 0.2076 - val_acc: 0.9339
Epoch 243/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2068 - acc: 0.9281Epoch 00243: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2077 - acc: 0.9278 - val_loss: 0.1992 - val_acc: 0.9395
Epoch 244/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2123 - acc: 0.9285Epoch 00244: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2126 - acc: 0.9282 - val_loss: 0.2047 - val_acc: 0.9373
Epoch 245/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2004 - acc: 0.9301Epoch 00245: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.1984 - acc: 0.9308 - val_loss: 0.2060 - val_acc: 0.9367
Epoch 246/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2070 - acc: 0.9285Epoch 00246: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2076 - acc: 0.9281 - val_loss: 0.2034 - val_acc: 0.9384
Epoch 247/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2046 - acc: 0.9313Epoch 00247: val_loss did not improve
17894/17894 [==============================] - 2s 90us/step - loss: 0.2046 - acc: 0.9313 - val_loss: 0.2016 - val_acc: 0.9373
Epoch 248/250
17664/17894 [============================&gt;.] - ETA: 0s - loss: 0.2152 - acc: 0.9261Epoch 00248: val_loss did not improve
17894/17894 [==============================] - 2s 91us/step - loss: 0.2142 - acc: 0.9266 - val_loss: 0.2010 - val_acc: 0.9401
Epoch 249/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2069 - acc: 0.9293Epoch 00249: val_loss did not improve
17894/17894 [==============================] - 2s 93us/step - loss: 0.2058 - acc: 0.9294 - val_loss: 0.2041 - val_acc: 0.9378
Epoch 250/250
17152/17894 [===========================&gt;..] - ETA: 0s - loss: 0.2044 - acc: 0.9306Epoch 00250: val_loss did not improve
17894/17894 [==============================] - 2s 92us/step - loss: 0.2061 - acc: 0.9300 - val_loss: 0.2111 - val_acc: 0.9334
9


 trianing 



Train on 17895 samples, validate on 1785 samples
Epoch 1/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 6.9904 - acc: 0.1023Epoch 00001: val_loss improved from inf to 2.46741, saving model to weights9.hdf5
17895/17895 [==============================] - 3s 165us/step - loss: 6.8071 - acc: 0.1030 - val_loss: 2.4674 - val_acc: 0.1092
Epoch 2/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4788 - acc: 0.1107Epoch 00002: val_loss improved from 2.46741 to 2.43533, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 2.4772 - acc: 0.1105 - val_loss: 2.4353 - val_acc: 0.1076
Epoch 3/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4404 - acc: 0.1097Epoch 00003: val_loss improved from 2.43533 to 2.43209, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 94us/step - loss: 2.4399 - acc: 0.1100 - val_loss: 2.4321 - val_acc: 0.1076
Epoch 4/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4346 - acc: 0.1087Epoch 00004: val_loss improved from 2.43209 to 2.43100, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 2.4345 - acc: 0.1091 - val_loss: 2.4310 - val_acc: 0.1076
Epoch 5/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4301 - acc: 0.1100Epoch 00005: val_loss improved from 2.43100 to 2.42674, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.4304 - acc: 0.1100 - val_loss: 2.4267 - val_acc: 0.1143
Epoch 6/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4205 - acc: 0.1184Epoch 00006: val_loss improved from 2.42674 to 2.41280, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.4210 - acc: 0.1171 - val_loss: 2.4128 - val_acc: 0.1165
Epoch 7/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4083 - acc: 0.1183Epoch 00007: val_loss improved from 2.41280 to 2.40688, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 2.4088 - acc: 0.1178 - val_loss: 2.4069 - val_acc: 0.1165
Epoch 8/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4050 - acc: 0.1181- ETA: 0s - loss: 2.4089 - acEpoch 00008: val_loss improved from 2.40688 to 2.40461, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.4048 - acc: 0.1179 - val_loss: 2.4046 - val_acc: 0.1165
Epoch 9/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4002 - acc: 0.1196Epoch 00009: val_loss improved from 2.40461 to 2.40432, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.4013 - acc: 0.1196 - val_loss: 2.4043 - val_acc: 0.1165
Epoch 10/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4011 - acc: 0.1181Epoch 00010: val_loss improved from 2.40432 to 2.40228, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 2.4006 - acc: 0.1189 - val_loss: 2.4023 - val_acc: 0.1165
Epoch 11/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.3960 - acc: 0.1199Epoch 00011: val_loss improved from 2.40228 to 2.40056, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 2.3979 - acc: 0.1196 - val_loss: 2.4006 - val_acc: 0.1182
Epoch 12/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.3950 - acc: 0.1210Epoch 00012: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 2.3956 - acc: 0.1199 - val_loss: 2.4013 - val_acc: 0.1176
Epoch 13/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.3950 - acc: 0.1198Epoch 00013: val_loss improved from 2.40056 to 2.39964, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 2.3961 - acc: 0.1194 - val_loss: 2.3996 - val_acc: 0.1171
Epoch 14/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.3933 - acc: 0.1221Epoch 00014: val_loss improved from 2.39964 to 2.39936, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.3950 - acc: 0.1211 - val_loss: 2.3994 - val_acc: 0.1182
Epoch 15/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.3886 - acc: 0.1245Epoch 00015: val_loss improved from 2.39936 to 2.39413, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.3896 - acc: 0.1236 - val_loss: 2.3941 - val_acc: 0.1188
Epoch 16/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.3810 - acc: 0.1308Epoch 00016: val_loss improved from 2.39413 to 2.37764, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.3825 - acc: 0.1301 - val_loss: 2.3776 - val_acc: 0.1333
Epoch 17/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.3583 - acc: 0.1465Epoch 00017: val_loss improved from 2.37764 to 2.30221, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.3580 - acc: 0.1454 - val_loss: 2.3022 - val_acc: 0.1658
Epoch 18/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.3263 - acc: 0.1551Epoch 00018: val_loss improved from 2.30221 to 2.27478, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.3270 - acc: 0.1550 - val_loss: 2.2748 - val_acc: 0.1731
Epoch 19/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.2940 - acc: 0.1628Epoch 00019: val_loss improved from 2.27478 to 2.17406, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 2.2935 - acc: 0.1626 - val_loss: 2.1741 - val_acc: 0.1910
Epoch 20/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.2457 - acc: 0.1960Epoch 00020: val_loss improved from 2.17406 to 2.12224, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 2.2444 - acc: 0.1975 - val_loss: 2.1222 - val_acc: 0.2571
Epoch 21/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.1777 - acc: 0.2270Epoch 00021: val_loss improved from 2.12224 to 2.01285, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 2.1762 - acc: 0.2273 - val_loss: 2.0129 - val_acc: 0.2919
Epoch 22/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.0462 - acc: 0.2731Epoch 00022: val_loss improved from 2.01285 to 1.68630, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 2.0406 - acc: 0.2744 - val_loss: 1.6863 - val_acc: 0.4314
Epoch 23/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.8446 - acc: 0.3491Epoch 00023: val_loss improved from 1.68630 to 1.49948, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 1.8431 - acc: 0.3496 - val_loss: 1.4995 - val_acc: 0.4801
Epoch 24/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.7009 - acc: 0.4013Epoch 00024: val_loss improved from 1.49948 to 1.34156, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 1.6985 - acc: 0.4014 - val_loss: 1.3416 - val_acc: 0.5535
Epoch 25/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.5670 - acc: 0.4464Epoch 00025: val_loss improved from 1.34156 to 1.24160, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 95us/step - loss: 1.5656 - acc: 0.4471 - val_loss: 1.2416 - val_acc: 0.5938
Epoch 26/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.4866 - acc: 0.4785Epoch 00026: val_loss improved from 1.24160 to 1.16538, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 95us/step - loss: 1.4859 - acc: 0.4786 - val_loss: 1.1654 - val_acc: 0.6235
Epoch 27/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.3956 - acc: 0.5115Epoch 00027: val_loss improved from 1.16538 to 1.06715, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 1.3964 - acc: 0.5110 - val_loss: 1.0672 - val_acc: 0.6695
Epoch 28/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.3186 - acc: 0.5455Epoch 00028: val_loss improved from 1.06715 to 0.96226, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 1.3173 - acc: 0.5455 - val_loss: 0.9623 - val_acc: 0.7003
Epoch 29/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.2572 - acc: 0.5615Epoch 00029: val_loss improved from 0.96226 to 0.91413, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 1.2576 - acc: 0.5620 - val_loss: 0.9141 - val_acc: 0.7210
Epoch 30/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.1858 - acc: 0.5886Epoch 00030: val_loss improved from 0.91413 to 0.85288, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 1.1873 - acc: 0.5887 - val_loss: 0.8529 - val_acc: 0.7345
Epoch 31/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.1271 - acc: 0.6102Epoch 00031: val_loss improved from 0.85288 to 0.79068, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 1.1264 - acc: 0.6107 - val_loss: 0.7907 - val_acc: 0.7574
Epoch 32/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.0780 - acc: 0.6311Epoch 00032: val_loss improved from 0.79068 to 0.78877, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 1.0777 - acc: 0.6312 - val_loss: 0.7888 - val_acc: 0.7473
Epoch 33/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.0330 - acc: 0.6436Epoch 00033: val_loss improved from 0.78877 to 0.70791, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 1.0300 - acc: 0.6445 - val_loss: 0.7079 - val_acc: 0.7737
Epoch 34/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.9848 - acc: 0.6587Epoch 00034: val_loss improved from 0.70791 to 0.68879, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 0.9815 - acc: 0.6598 - val_loss: 0.6888 - val_acc: 0.7748
Epoch 35/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.9486 - acc: 0.6740Epoch 00035: val_loss improved from 0.68879 to 0.65157, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.9486 - acc: 0.6740 - val_loss: 0.6516 - val_acc: 0.7938
Epoch 36/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.9078 - acc: 0.6875Epoch 00036: val_loss improved from 0.65157 to 0.63245, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.9062 - acc: 0.6882 - val_loss: 0.6325 - val_acc: 0.7972
Epoch 37/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.8865 - acc: 0.6983Epoch 00037: val_loss improved from 0.63245 to 0.61138, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 0.8882 - acc: 0.6969 - val_loss: 0.6114 - val_acc: 0.8101
Epoch 38/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.8551 - acc: 0.7081Epoch 00038: val_loss improved from 0.61138 to 0.58480, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 0.8555 - acc: 0.7081 - val_loss: 0.5848 - val_acc: 0.8174
Epoch 39/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.8207 - acc: 0.7176Epoch 00039: val_loss improved from 0.58480 to 0.55119, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 0.8212 - acc: 0.7179 - val_loss: 0.5512 - val_acc: 0.8269
Epoch 40/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.8118 - acc: 0.7260Epoch 00040: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.8104 - acc: 0.7262 - val_loss: 0.5553 - val_acc: 0.8297
Epoch 41/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.7803 - acc: 0.7359Epoch 00041: val_loss improved from 0.55119 to 0.54562, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.7814 - acc: 0.7357 - val_loss: 0.5456 - val_acc: 0.8258
Epoch 42/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.7644 - acc: 0.7423Epoch 00042: val_loss improved from 0.54562 to 0.51958, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 94us/step - loss: 0.7631 - acc: 0.7427 - val_loss: 0.5196 - val_acc: 0.8482
Epoch 43/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.7456 - acc: 0.7466Epoch 00043: val_loss improved from 0.51958 to 0.50785, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 94us/step - loss: 0.7439 - acc: 0.7462 - val_loss: 0.5078 - val_acc: 0.8403
Epoch 44/250
17408/17895 [============================&gt;.] - ETA: 0s - loss: 0.7410 - acc: 0.7486Epoch 00044: val_loss improved from 0.50785 to 0.49947, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 95us/step - loss: 0.7406 - acc: 0.7485 - val_loss: 0.4995 - val_acc: 0.8443
Epoch 45/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.7310 - acc: 0.7524Epoch 00045: val_loss improved from 0.49947 to 0.48908, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.7284 - acc: 0.7535 - val_loss: 0.4891 - val_acc: 0.8521
Epoch 46/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6940 - acc: 0.7666Epoch 00046: val_loss improved from 0.48908 to 0.47158, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.6953 - acc: 0.7664 - val_loss: 0.4716 - val_acc: 0.8588
Epoch 47/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6916 - acc: 0.7670- ETA: 1s - loss: 0.Epoch 00047: val_loss improved from 0.47158 to 0.47042, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.6912 - acc: 0.7666 - val_loss: 0.4704 - val_acc: 0.8560
Epoch 48/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6690 - acc: 0.7697Epoch 00048: val_loss improved from 0.47042 to 0.45586, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.6673 - acc: 0.7699 - val_loss: 0.4559 - val_acc: 0.8583
Epoch 49/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6616 - acc: 0.7743Epoch 00049: val_loss improved from 0.45586 to 0.45008, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.6614 - acc: 0.7740 - val_loss: 0.4501 - val_acc: 0.8605
Epoch 50/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6459 - acc: 0.7807Epoch 00050: val_loss improved from 0.45008 to 0.44045, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.6478 - acc: 0.7804 - val_loss: 0.4405 - val_acc: 0.8622
Epoch 51/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6271 - acc: 0.7888Epoch 00051: val_loss improved from 0.44045 to 0.43234, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.6278 - acc: 0.7888 - val_loss: 0.4323 - val_acc: 0.8655
Epoch 52/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6225 - acc: 0.7875Epoch 00052: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.6227 - acc: 0.7872 - val_loss: 0.4351 - val_acc: 0.8611
Epoch 53/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6109 - acc: 0.7930Epoch 00053: val_loss improved from 0.43234 to 0.42389, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.6133 - acc: 0.7924 - val_loss: 0.4239 - val_acc: 0.8734
Epoch 54/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6084 - acc: 0.7942Epoch 00054: val_loss improved from 0.42389 to 0.41390, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.6085 - acc: 0.7945 - val_loss: 0.4139 - val_acc: 0.8734
Epoch 55/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6057 - acc: 0.7939- ETA: 1s - loss: 0.6Epoch 00055: val_loss improved from 0.41390 to 0.41174, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.6040 - acc: 0.7947 - val_loss: 0.4117 - val_acc: 0.8734
Epoch 56/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5808 - acc: 0.8016Epoch 00056: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.5810 - acc: 0.8015 - val_loss: 0.4129 - val_acc: 0.8711
Epoch 57/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5731 - acc: 0.8042Epoch 00057: val_loss improved from 0.41174 to 0.39688, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 90us/step - loss: 0.5719 - acc: 0.8047 - val_loss: 0.3969 - val_acc: 0.8796
Epoch 58/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5570 - acc: 0.8102Epoch 00058: val_loss improved from 0.39688 to 0.38776, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.5580 - acc: 0.8096 - val_loss: 0.3878 - val_acc: 0.8784
Epoch 59/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5594 - acc: 0.8127- ETA: 1s - loss: 0.549 - ETA: 0s - loss: 0.5593 - acc: 0.81Epoch 00059: val_loss improved from 0.38776 to 0.38648, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.5593 - acc: 0.8120 - val_loss: 0.3865 - val_acc: 0.8835
Epoch 60/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5645 - acc: 0.8120- ETA: 0s - loss: 0.5640 - acc: 0.812Epoch 00060: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.5612 - acc: 0.8137 - val_loss: 0.3929 - val_acc: 0.8824
Epoch 61/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5365 - acc: 0.8159Epoch 00061: val_loss improved from 0.38648 to 0.38237, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.5388 - acc: 0.8151 - val_loss: 0.3824 - val_acc: 0.8790
Epoch 62/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5372 - acc: 0.8193Epoch 00062: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.5387 - acc: 0.8191 - val_loss: 0.3831 - val_acc: 0.8840
Epoch 63/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5196 - acc: 0.8235Epoch 00063: val_loss improved from 0.38237 to 0.37891, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.5198 - acc: 0.8235 - val_loss: 0.3789 - val_acc: 0.8818
Epoch 64/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5144 - acc: 0.8246- ETA: 0s - loss: 0.5174 - acc: 0.8Epoch 00064: val_loss improved from 0.37891 to 0.37075, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.5158 - acc: 0.8251 - val_loss: 0.3707 - val_acc: 0.8885
Epoch 65/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5098 - acc: 0.8280Epoch 00065: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.5103 - acc: 0.8278 - val_loss: 0.3737 - val_acc: 0.8874
Epoch 66/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5064 - acc: 0.8280Epoch 00066: val_loss improved from 0.37075 to 0.36766, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.5071 - acc: 0.8281 - val_loss: 0.3677 - val_acc: 0.8863
Epoch 67/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5040 - acc: 0.8298Epoch 00067: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.5028 - acc: 0.8302 - val_loss: 0.3766 - val_acc: 0.8824
Epoch 68/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4948 - acc: 0.8331Epoch 00068: val_loss improved from 0.36766 to 0.36404, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4955 - acc: 0.8332 - val_loss: 0.3640 - val_acc: 0.8891
Epoch 69/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4879 - acc: 0.8373- ETA: 1s - loss: 0.478Epoch 00069: val_loss improved from 0.36404 to 0.35127, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4904 - acc: 0.8361 - val_loss: 0.3513 - val_acc: 0.8919
Epoch 70/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4903 - acc: 0.8345Epoch 00070: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.4924 - acc: 0.8341 - val_loss: 0.3588 - val_acc: 0.8874
Epoch 71/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4842 - acc: 0.8404- ETA: 0s - loss: 0.4859 - acc:Epoch 00071: val_loss improved from 0.35127 to 0.34580, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4838 - acc: 0.8407 - val_loss: 0.3458 - val_acc: 0.8941
Epoch 72/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4755 - acc: 0.8411Epoch 00072: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.4759 - acc: 0.8405 - val_loss: 0.3487 - val_acc: 0.8919
Epoch 73/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4657 - acc: 0.8449Epoch 00073: val_loss improved from 0.34580 to 0.34455, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4683 - acc: 0.8438 - val_loss: 0.3446 - val_acc: 0.8969
Epoch 74/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4659 - acc: 0.8410Epoch 00074: val_loss improved from 0.34455 to 0.33861, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4661 - acc: 0.8414 - val_loss: 0.3386 - val_acc: 0.8924
Epoch 75/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4687 - acc: 0.8423Epoch 00075: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.4671 - acc: 0.8425 - val_loss: 0.3392 - val_acc: 0.8964
Epoch 76/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4562 - acc: 0.8464Epoch 00076: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.4556 - acc: 0.8466 - val_loss: 0.3440 - val_acc: 0.8913
Epoch 77/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4398 - acc: 0.8512Epoch 00077: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.4394 - acc: 0.8510 - val_loss: 0.3403 - val_acc: 0.8964
Epoch 78/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4410 - acc: 0.8506Epoch 00078: val_loss improved from 0.33861 to 0.33270, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4415 - acc: 0.8509 - val_loss: 0.3327 - val_acc: 0.8947
Epoch 79/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4491 - acc: 0.8499Epoch 00079: val_loss improved from 0.33270 to 0.33029, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4492 - acc: 0.8496 - val_loss: 0.3303 - val_acc: 0.9014
Epoch 80/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4430 - acc: 0.8475Epoch 00080: val_loss improved from 0.33029 to 0.32696, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4431 - acc: 0.8478 - val_loss: 0.3270 - val_acc: 0.8964
Epoch 81/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4339 - acc: 0.8514Epoch 00081: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.4345 - acc: 0.8518 - val_loss: 0.3355 - val_acc: 0.8992
Epoch 82/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4318 - acc: 0.8555Epoch 00082: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.4319 - acc: 0.8554 - val_loss: 0.3282 - val_acc: 0.9020
Epoch 83/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4237 - acc: 0.8559Epoch 00083: val_loss improved from 0.32696 to 0.32226, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4226 - acc: 0.8563 - val_loss: 0.3223 - val_acc: 0.9036
Epoch 84/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4232 - acc: 0.8575Epoch 00084: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.4248 - acc: 0.8567 - val_loss: 0.3279 - val_acc: 0.9025
Epoch 85/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4264 - acc: 0.8555Epoch 00085: val_loss improved from 0.32226 to 0.31850, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 0.4258 - acc: 0.8555 - val_loss: 0.3185 - val_acc: 0.9025
Epoch 86/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4143 - acc: 0.8600Epoch 00086: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.4131 - acc: 0.8597 - val_loss: 0.3195 - val_acc: 0.8997
Epoch 87/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4130 - acc: 0.8593Epoch 00087: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.4128 - acc: 0.8598 - val_loss: 0.3208 - val_acc: 0.8992
Epoch 88/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4043 - acc: 0.8640Epoch 00088: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.4044 - acc: 0.8642 - val_loss: 0.3224 - val_acc: 0.9014
Epoch 89/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4142 - acc: 0.8622Epoch 00089: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.4126 - acc: 0.8624 - val_loss: 0.3213 - val_acc: 0.9036
Epoch 90/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4024 - acc: 0.8619Epoch 00090: val_loss improved from 0.31850 to 0.31777, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4003 - acc: 0.8631 - val_loss: 0.3178 - val_acc: 0.9064
Epoch 91/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4042 - acc: 0.8635Epoch 00091: val_loss improved from 0.31777 to 0.31482, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4058 - acc: 0.8630 - val_loss: 0.3148 - val_acc: 0.9014
Epoch 92/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3913 - acc: 0.8667Epoch 00092: val_loss improved from 0.31482 to 0.31310, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.3928 - acc: 0.8664 - val_loss: 0.3131 - val_acc: 0.9014
Epoch 93/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3993 - acc: 0.8661Epoch 00093: val_loss improved from 0.31310 to 0.30887, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4010 - acc: 0.8655 - val_loss: 0.3089 - val_acc: 0.9087
Epoch 94/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3995 - acc: 0.8630Epoch 00094: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.4002 - acc: 0.8636 - val_loss: 0.3141 - val_acc: 0.9059
Epoch 95/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3859 - acc: 0.8693Epoch 00095: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3869 - acc: 0.8693 - val_loss: 0.3116 - val_acc: 0.9053
Epoch 96/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3909 - acc: 0.8678- ETA: 0s - loss: 0.3923 - Epoch 00096: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3889 - acc: 0.8687 - val_loss: 0.3093 - val_acc: 0.9076
Epoch 97/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3861 - acc: 0.8698Epoch 00097: val_loss improved from 0.30887 to 0.30861, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.3852 - acc: 0.8708 - val_loss: 0.3086 - val_acc: 0.9076
Epoch 98/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3814 - acc: 0.8733Epoch 00098: val_loss improved from 0.30861 to 0.30453, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.3825 - acc: 0.8728 - val_loss: 0.3045 - val_acc: 0.9081
Epoch 99/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3766 - acc: 0.8717Epoch 00099: val_loss improved from 0.30453 to 0.30451, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.3777 - acc: 0.8713 - val_loss: 0.3045 - val_acc: 0.9070
Epoch 100/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3804 - acc: 0.8735Epoch 00100: val_loss improved from 0.30451 to 0.30437, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.3783 - acc: 0.8740 - val_loss: 0.3044 - val_acc: 0.9081
Epoch 101/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3709 - acc: 0.8736Epoch 00101: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3698 - acc: 0.8742 - val_loss: 0.3051 - val_acc: 0.9076
Epoch 102/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3781 - acc: 0.8703Epoch 00102: val_loss improved from 0.30437 to 0.29577, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.3773 - acc: 0.8709 - val_loss: 0.2958 - val_acc: 0.9081
Epoch 103/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3718 - acc: 0.8712Epoch 00103: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3727 - acc: 0.8712 - val_loss: 0.3025 - val_acc: 0.9064
Epoch 104/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3637 - acc: 0.8766Epoch 00104: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.3615 - acc: 0.8771 - val_loss: 0.2976 - val_acc: 0.9076
Epoch 105/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3666 - acc: 0.8762Epoch 00105: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3674 - acc: 0.8758 - val_loss: 0.3011 - val_acc: 0.9048
Epoch 106/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3557 - acc: 0.8811Epoch 00106: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.3555 - acc: 0.8807 - val_loss: 0.2958 - val_acc: 0.9076
Epoch 107/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3583 - acc: 0.8791Epoch 00107: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3566 - acc: 0.8798 - val_loss: 0.3005 - val_acc: 0.9132
Epoch 108/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3597 - acc: 0.8789Epoch 00108: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.3590 - acc: 0.8796 - val_loss: 0.3054 - val_acc: 0.9092
Epoch 109/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3627 - acc: 0.8757Epoch 00109: val_loss improved from 0.29577 to 0.29326, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.3618 - acc: 0.8760 - val_loss: 0.2933 - val_acc: 0.9076
Epoch 110/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3501 - acc: 0.8826- ETA: 1s - loss: 0.3370 - acc: - ETA: 0s - loss: 0.3510 - acEpoch 00110: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3483 - acc: 0.8830 - val_loss: 0.2995 - val_acc: 0.9098
Epoch 111/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3526 - acc: 0.8781Epoch 00111: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.3541 - acc: 0.8781 - val_loss: 0.2971 - val_acc: 0.9109
Epoch 112/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3509 - acc: 0.8814Epoch 00112: val_loss improved from 0.29326 to 0.28998, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.3513 - acc: 0.8815 - val_loss: 0.2900 - val_acc: 0.9064
Epoch 113/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3545 - acc: 0.8808Epoch 00113: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3507 - acc: 0.8816 - val_loss: 0.2947 - val_acc: 0.9064
Epoch 114/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3356 - acc: 0.8857Epoch 00114: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.3362 - acc: 0.8855 - val_loss: 0.2927 - val_acc: 0.9087
Epoch 115/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3472 - acc: 0.8818Epoch 00115: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3476 - acc: 0.8821 - val_loss: 0.2963 - val_acc: 0.9087
Epoch 116/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3429 - acc: 0.8818Epoch 00116: val_loss improved from 0.28998 to 0.28936, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.3435 - acc: 0.8811 - val_loss: 0.2894 - val_acc: 0.9098
Epoch 117/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3402 - acc: 0.8859Epoch 00117: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3396 - acc: 0.8857 - val_loss: 0.2953 - val_acc: 0.9098
Epoch 118/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3298 - acc: 0.8891- ETA: 0s - loss: 0.3361 - aEpoch 00118: val_loss improved from 0.28936 to 0.28810, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.3293 - acc: 0.8893 - val_loss: 0.2881 - val_acc: 0.9132
Epoch 119/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3439 - acc: 0.8835Epoch 00119: val_loss improved from 0.28810 to 0.28768, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.3454 - acc: 0.8830 - val_loss: 0.2877 - val_acc: 0.9120
Epoch 120/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3372 - acc: 0.8825Epoch 00120: val_loss improved from 0.28768 to 0.28693, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.3370 - acc: 0.8832 - val_loss: 0.2869 - val_acc: 0.9132
Epoch 121/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3278 - acc: 0.8870Epoch 00121: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3271 - acc: 0.8873 - val_loss: 0.2890 - val_acc: 0.9148
Epoch 122/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3310 - acc: 0.8885Epoch 00122: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3325 - acc: 0.8883 - val_loss: 0.2881 - val_acc: 0.9115
Epoch 123/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3279 - acc: 0.8888Epoch 00123: val_loss improved from 0.28693 to 0.28379, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.3277 - acc: 0.8895 - val_loss: 0.2838 - val_acc: 0.9143
Epoch 124/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3250 - acc: 0.8880Epoch 00124: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3229 - acc: 0.8886 - val_loss: 0.2914 - val_acc: 0.9182
Epoch 125/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3217 - acc: 0.8892Epoch 00125: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3238 - acc: 0.8885 - val_loss: 0.2862 - val_acc: 0.9104
Epoch 126/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3278 - acc: 0.8906Epoch 00126: val_loss improved from 0.28379 to 0.28363, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.3296 - acc: 0.8901 - val_loss: 0.2836 - val_acc: 0.9182
Epoch 127/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3202 - acc: 0.8907Epoch 00127: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3197 - acc: 0.8906 - val_loss: 0.2888 - val_acc: 0.9126
Epoch 128/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3247 - acc: 0.8879Epoch 00128: val_loss improved from 0.28363 to 0.27930, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.3266 - acc: 0.8871 - val_loss: 0.2793 - val_acc: 0.9165
Epoch 129/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3233 - acc: 0.8879Epoch 00129: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.3239 - acc: 0.8879 - val_loss: 0.2843 - val_acc: 0.9137
Epoch 130/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3138 - acc: 0.8929Epoch 00130: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.3146 - acc: 0.8927 - val_loss: 0.2850 - val_acc: 0.9132
Epoch 131/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3147 - acc: 0.8935Epoch 00131: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.3133 - acc: 0.8936 - val_loss: 0.2914 - val_acc: 0.9109
Epoch 132/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3091 - acc: 0.8959Epoch 00132: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.3119 - acc: 0.8953 - val_loss: 0.2835 - val_acc: 0.9115
Epoch 133/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3073 - acc: 0.8933Epoch 00133: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.3092 - acc: 0.8929 - val_loss: 0.2857 - val_acc: 0.9148
Epoch 134/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3081 - acc: 0.8956Epoch 00134: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.3077 - acc: 0.8961 - val_loss: 0.2885 - val_acc: 0.9143
Epoch 135/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3044 - acc: 0.8967Epoch 00135: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.3056 - acc: 0.8965 - val_loss: 0.2836 - val_acc: 0.9182
Epoch 136/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3010 - acc: 0.9007- ETA: 1s - loss: 0.2980 - acc: 0.90 - ETA: 1s - loss: 0.29Epoch 00136: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.3029 - acc: 0.8998 - val_loss: 0.2842 - val_acc: 0.9160
Epoch 137/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3148 - acc: 0.8928Epoch 00137: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3148 - acc: 0.8932 - val_loss: 0.2824 - val_acc: 0.9176
Epoch 138/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2986 - acc: 0.8976Epoch 00138: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3003 - acc: 0.8969 - val_loss: 0.2830 - val_acc: 0.9176
Epoch 139/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3019 - acc: 0.8970Epoch 00139: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.3039 - acc: 0.8958 - val_loss: 0.2812 - val_acc: 0.9182
Epoch 140/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3034 - acc: 0.8979- ETA: 0s - loss: 0.3059 - acc: Epoch 00140: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.3012 - acc: 0.8983 - val_loss: 0.2801 - val_acc: 0.9199
Epoch 141/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3134 - acc: 0.8916Epoch 00141: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.3120 - acc: 0.8918 - val_loss: 0.2799 - val_acc: 0.9154
Epoch 142/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2976 - acc: 0.8980Epoch 00142: val_loss improved from 0.27930 to 0.27523, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.2990 - acc: 0.8978 - val_loss: 0.2752 - val_acc: 0.9143
Epoch 143/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2922 - acc: 0.8990Epoch 00143: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2916 - acc: 0.8990 - val_loss: 0.2790 - val_acc: 0.9188
Epoch 144/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2909 - acc: 0.9017- ETA: 0s - loss: 0.2906 - acc: 0.901Epoch 00144: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2925 - acc: 0.9011 - val_loss: 0.2783 - val_acc: 0.9154
Epoch 145/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2976 - acc: 0.8977Epoch 00145: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2980 - acc: 0.8980 - val_loss: 0.2802 - val_acc: 0.9188
Epoch 146/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2934 - acc: 0.8998Epoch 00146: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2953 - acc: 0.8992 - val_loss: 0.2783 - val_acc: 0.9188
Epoch 147/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2868 - acc: 0.9009Epoch 00147: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2871 - acc: 0.9005 - val_loss: 0.2815 - val_acc: 0.9132
Epoch 148/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2949 - acc: 0.9011Epoch 00148: val_loss improved from 0.27523 to 0.27237, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.2940 - acc: 0.9012 - val_loss: 0.2724 - val_acc: 0.9210
Epoch 149/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2906 - acc: 0.8997Epoch 00149: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2899 - acc: 0.9001 - val_loss: 0.2816 - val_acc: 0.9193
Epoch 150/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2890 - acc: 0.9004Epoch 00150: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2891 - acc: 0.9004 - val_loss: 0.2765 - val_acc: 0.9165
Epoch 151/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2888 - acc: 0.9021Epoch 00151: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2902 - acc: 0.9013 - val_loss: 0.2845 - val_acc: 0.9165
Epoch 152/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2925 - acc: 0.9014Epoch 00152: val_loss improved from 0.27237 to 0.27137, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.2904 - acc: 0.9018 - val_loss: 0.2714 - val_acc: 0.9182
Epoch 153/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2824 - acc: 0.9051Epoch 00153: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2816 - acc: 0.9051 - val_loss: 0.2789 - val_acc: 0.9188
Epoch 154/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2787 - acc: 0.9041Epoch 00154: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2776 - acc: 0.9045 - val_loss: 0.2820 - val_acc: 0.9154
Epoch 155/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2883 - acc: 0.9029Epoch 00155: val_loss improved from 0.27137 to 0.27127, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.2888 - acc: 0.9022 - val_loss: 0.2713 - val_acc: 0.9193
Epoch 156/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2860 - acc: 0.9042- ETA: 0s - loss: 0.2904 - acEpoch 00156: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2846 - acc: 0.9044 - val_loss: 0.2738 - val_acc: 0.9182
Epoch 157/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2863 - acc: 0.9023- ETA: 1s - loss: 0.2Epoch 00157: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2857 - acc: 0.9022 - val_loss: 0.2745 - val_acc: 0.9176
Epoch 158/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2733 - acc: 0.9096Epoch 00158: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2740 - acc: 0.9091 - val_loss: 0.2797 - val_acc: 0.9160
Epoch 159/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2780 - acc: 0.9048Epoch 00159: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2786 - acc: 0.9043 - val_loss: 0.2770 - val_acc: 0.9199
Epoch 160/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2779 - acc: 0.9042- ETA: 0s - loss: 0.2837 - accEpoch 00160: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2788 - acc: 0.9039 - val_loss: 0.2788 - val_acc: 0.9182
Epoch 161/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2732 - acc: 0.9069Epoch 00161: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2739 - acc: 0.9066 - val_loss: 0.2760 - val_acc: 0.9193
Epoch 162/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2734 - acc: 0.9060Epoch 00162: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2746 - acc: 0.9054 - val_loss: 0.2829 - val_acc: 0.9188
Epoch 163/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2819 - acc: 0.9033Epoch 00163: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2812 - acc: 0.9039 - val_loss: 0.2812 - val_acc: 0.9171
Epoch 164/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2750 - acc: 0.9064Epoch 00164: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2755 - acc: 0.9062 - val_loss: 0.2732 - val_acc: 0.9204
Epoch 165/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2663 - acc: 0.9067- ETA: 1s - loss: 0.2759 - acc: 0.90 - ETA: 0s - loss: 0.2656 -Epoch 00165: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2661 - acc: 0.9068 - val_loss: 0.2745 - val_acc: 0.9188
Epoch 166/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2739 - acc: 0.9052Epoch 00166: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2721 - acc: 0.9058 - val_loss: 0.2833 - val_acc: 0.9221
Epoch 167/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2691 - acc: 0.9069Epoch 00167: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2689 - acc: 0.9066 - val_loss: 0.2771 - val_acc: 0.9176
Epoch 168/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2633 - acc: 0.9088Epoch 00168: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2636 - acc: 0.9084 - val_loss: 0.2727 - val_acc: 0.9199
Epoch 169/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2606 - acc: 0.9114Epoch 00169: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2620 - acc: 0.9109 - val_loss: 0.2811 - val_acc: 0.9171
Epoch 170/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2692 - acc: 0.9084Epoch 00170: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2698 - acc: 0.9084 - val_loss: 0.2745 - val_acc: 0.9154
Epoch 171/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2666 - acc: 0.9100Epoch 00171: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2667 - acc: 0.9098 - val_loss: 0.2783 - val_acc: 0.9176
Epoch 172/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2604 - acc: 0.9084Epoch 00172: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2611 - acc: 0.9081 - val_loss: 0.2813 - val_acc: 0.9193
Epoch 173/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2651 - acc: 0.9101Epoch 00173: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2647 - acc: 0.9100 - val_loss: 0.2813 - val_acc: 0.9182
Epoch 174/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2650 - acc: 0.9091Epoch 00174: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2654 - acc: 0.9090 - val_loss: 0.2727 - val_acc: 0.9199
Epoch 175/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2618 - acc: 0.9099- ETA: 1s - loss: 0Epoch 00175: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2618 - acc: 0.9100 - val_loss: 0.2781 - val_acc: 0.9193
Epoch 176/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2591 - acc: 0.9083Epoch 00176: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2609 - acc: 0.9083 - val_loss: 0.2812 - val_acc: 0.9193
Epoch 177/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2646 - acc: 0.9102Epoch 00177: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2657 - acc: 0.9103 - val_loss: 0.2738 - val_acc: 0.9232
Epoch 178/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2642 - acc: 0.9093Epoch 00178: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2623 - acc: 0.9100 - val_loss: 0.2788 - val_acc: 0.9176
Epoch 179/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2647 - acc: 0.9094- ETA: 0s - loss: 0.2647 - acc:Epoch 00179: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2658 - acc: 0.9089 - val_loss: 0.2800 - val_acc: 0.9193
Epoch 180/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2663 - acc: 0.9113Epoch 00180: val_loss improved from 0.27127 to 0.27097, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.2670 - acc: 0.9108 - val_loss: 0.2710 - val_acc: 0.9210
Epoch 181/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2593 - acc: 0.9117Epoch 00181: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2576 - acc: 0.9118 - val_loss: 0.2821 - val_acc: 0.9160
Epoch 182/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2558 - acc: 0.9103Epoch 00182: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2565 - acc: 0.9101 - val_loss: 0.2759 - val_acc: 0.9171
Epoch 183/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2565 - acc: 0.9089Epoch 00183: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2570 - acc: 0.9088 - val_loss: 0.2734 - val_acc: 0.9210
Epoch 184/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2560 - acc: 0.9117Epoch 00184: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2551 - acc: 0.9120 - val_loss: 0.2771 - val_acc: 0.9204
Epoch 185/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2571 - acc: 0.9131Epoch 00185: val_loss improved from 0.27097 to 0.26991, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.2568 - acc: 0.9132 - val_loss: 0.2699 - val_acc: 0.9193
Epoch 186/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2484 - acc: 0.9150Epoch 00186: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2493 - acc: 0.9147 - val_loss: 0.2727 - val_acc: 0.9238
Epoch 187/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2581 - acc: 0.9118Epoch 00187: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2579 - acc: 0.9116 - val_loss: 0.2719 - val_acc: 0.9227
Epoch 188/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2537 - acc: 0.9143Epoch 00188: val_loss improved from 0.26991 to 0.26972, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 0.2537 - acc: 0.9138 - val_loss: 0.2697 - val_acc: 0.9193
Epoch 189/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2526 - acc: 0.9146Epoch 00189: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2517 - acc: 0.9144 - val_loss: 0.2835 - val_acc: 0.9148
Epoch 190/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2473 - acc: 0.9159Epoch 00190: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2460 - acc: 0.9162 - val_loss: 0.2777 - val_acc: 0.9182
Epoch 191/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2414 - acc: 0.9166Epoch 00191: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2413 - acc: 0.9172 - val_loss: 0.2817 - val_acc: 0.9199
Epoch 192/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2473 - acc: 0.9146Epoch 00192: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2456 - acc: 0.9155 - val_loss: 0.2711 - val_acc: 0.9210
Epoch 193/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2489 - acc: 0.9137- ETA: 0s - loss: 0.2387 -Epoch 00193: val_loss improved from 0.26972 to 0.26915, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.2504 - acc: 0.9132 - val_loss: 0.2691 - val_acc: 0.9244
Epoch 194/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2493 - acc: 0.9139Epoch 00194: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2501 - acc: 0.9136 - val_loss: 0.2744 - val_acc: 0.9221
Epoch 195/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2537 - acc: 0.9151- ETA: 0s - loss: 0.2548 - acc: 0.91Epoch 00195: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2521 - acc: 0.9157 - val_loss: 0.2795 - val_acc: 0.9221
Epoch 196/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2519 - acc: 0.9127Epoch 00196: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2539 - acc: 0.9117 - val_loss: 0.2722 - val_acc: 0.9210
Epoch 197/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2492 - acc: 0.9144Epoch 00197: val_loss improved from 0.26915 to 0.26404, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.2496 - acc: 0.9138 - val_loss: 0.2640 - val_acc: 0.9227
Epoch 198/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2489 - acc: 0.9149Epoch 00198: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2498 - acc: 0.9144 - val_loss: 0.2690 - val_acc: 0.9216
Epoch 199/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2444 - acc: 0.9166- ETA: 0s - loss: 0.2446 Epoch 00199: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2462 - acc: 0.9163 - val_loss: 0.2667 - val_acc: 0.9216
Epoch 200/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2414 - acc: 0.9173- ETA: 1s - loss: 0.24Epoch 00200: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2406 - acc: 0.9179 - val_loss: 0.2697 - val_acc: 0.9227
Epoch 201/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2454 - acc: 0.9137Epoch 00201: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2435 - acc: 0.9143 - val_loss: 0.2805 - val_acc: 0.9188
Epoch 202/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2462 - acc: 0.9172Epoch 00202: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2493 - acc: 0.9160 - val_loss: 0.2729 - val_acc: 0.9199
Epoch 203/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2430 - acc: 0.9160Epoch 00203: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2456 - acc: 0.9150 - val_loss: 0.2778 - val_acc: 0.9188
Epoch 204/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2414 - acc: 0.9171Epoch 00204: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2416 - acc: 0.9170 - val_loss: 0.2685 - val_acc: 0.9210
Epoch 205/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2405 - acc: 0.9180Epoch 00205: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2418 - acc: 0.9181 - val_loss: 0.2666 - val_acc: 0.9216
Epoch 206/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2327 - acc: 0.9207Epoch 00206: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2333 - acc: 0.9208 - val_loss: 0.2769 - val_acc: 0.9232
Epoch 207/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2422 - acc: 0.9169Epoch 00207: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2431 - acc: 0.9165 - val_loss: 0.2694 - val_acc: 0.9227
Epoch 208/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2380 - acc: 0.9175Epoch 00208: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2374 - acc: 0.9177 - val_loss: 0.2754 - val_acc: 0.9216
Epoch 209/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2316 - acc: 0.9187- ETA: 0s - loss: 0.2321 - acc:Epoch 00209: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2304 - acc: 0.9190 - val_loss: 0.2742 - val_acc: 0.9216
Epoch 210/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2431 - acc: 0.9175Epoch 00210: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2431 - acc: 0.9172 - val_loss: 0.2700 - val_acc: 0.9210
Epoch 211/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2290 - acc: 0.922 - ETA: 0s - loss: 0.2305 - acc: 0.9223Epoch 00211: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2306 - acc: 0.9218 - val_loss: 0.2755 - val_acc: 0.9199
Epoch 212/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2367 - acc: 0.9191Epoch 00212: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2388 - acc: 0.9182 - val_loss: 0.2694 - val_acc: 0.9221
Epoch 213/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2396 - acc: 0.9195Epoch 00213: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2404 - acc: 0.9193 - val_loss: 0.2669 - val_acc: 0.9227
Epoch 214/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2323 - acc: 0.9211Epoch 00214: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2322 - acc: 0.9213 - val_loss: 0.2664 - val_acc: 0.9244
Epoch 215/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2272 - acc: 0.9212Epoch 00215: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2278 - acc: 0.9213 - val_loss: 0.2756 - val_acc: 0.9204
Epoch 216/250
17664/17895 [============================&gt;.] - ETA: 0s - loss: 0.2382 - acc: 0.9180Epoch 00216: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2380 - acc: 0.9182 - val_loss: 0.2688 - val_acc: 0.9216
Epoch 217/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2337 - acc: 0.9237Epoch 00217: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2321 - acc: 0.9239 - val_loss: 0.2731 - val_acc: 0.9204
Epoch 218/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2368 - acc: 0.9182Epoch 00218: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2358 - acc: 0.9185 - val_loss: 0.2668 - val_acc: 0.9249
Epoch 219/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2334 - acc: 0.9183Epoch 00219: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2325 - acc: 0.9187 - val_loss: 0.2720 - val_acc: 0.9232
Epoch 220/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2331 - acc: 0.9215Epoch 00220: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2336 - acc: 0.9207 - val_loss: 0.2696 - val_acc: 0.9221
Epoch 221/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2273 - acc: 0.9227Epoch 00221: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2276 - acc: 0.9224 - val_loss: 0.2750 - val_acc: 0.9232
Epoch 222/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2289 - acc: 0.9226Epoch 00222: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2293 - acc: 0.9227 - val_loss: 0.2747 - val_acc: 0.9232
Epoch 223/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2305 - acc: 0.9209Epoch 00223: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2300 - acc: 0.9210 - val_loss: 0.2685 - val_acc: 0.9227
Epoch 224/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2270 - acc: 0.9231Epoch 00224: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2271 - acc: 0.9234 - val_loss: 0.2711 - val_acc: 0.9227
Epoch 225/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2313 - acc: 0.9174Epoch 00225: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2310 - acc: 0.9177 - val_loss: 0.2738 - val_acc: 0.9227
Epoch 226/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2212 - acc: 0.9245Epoch 00226: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2231 - acc: 0.9237 - val_loss: 0.2727 - val_acc: 0.9227
Epoch 227/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2200 - acc: 0.9232Epoch 00227: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2231 - acc: 0.9223 - val_loss: 0.2680 - val_acc: 0.9232
Epoch 228/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2238 - acc: 0.9250Epoch 00228: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2242 - acc: 0.9247 - val_loss: 0.2694 - val_acc: 0.9238
Epoch 229/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2235 - acc: 0.9243Epoch 00229: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2232 - acc: 0.9242 - val_loss: 0.2683 - val_acc: 0.9232
Epoch 230/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2225 - acc: 0.9224Epoch 00230: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2237 - acc: 0.9222 - val_loss: 0.2758 - val_acc: 0.9199
Epoch 231/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2224 - acc: 0.9228Epoch 00231: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2245 - acc: 0.9223 - val_loss: 0.2716 - val_acc: 0.9244
Epoch 232/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2248 - acc: 0.9245Epoch 00232: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2242 - acc: 0.9243 - val_loss: 0.2701 - val_acc: 0.9204
Epoch 233/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2202 - acc: 0.9245Epoch 00233: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2201 - acc: 0.9252 - val_loss: 0.2642 - val_acc: 0.9255
Epoch 234/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2227 - acc: 0.9225Epoch 00234: val_loss improved from 0.26404 to 0.26333, saving model to weights9.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.2237 - acc: 0.9224 - val_loss: 0.2633 - val_acc: 0.9249
Epoch 235/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2288 - acc: 0.9204Epoch 00235: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2279 - acc: 0.9207 - val_loss: 0.2761 - val_acc: 0.9115
Epoch 236/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2219 - acc: 0.9240Epoch 00236: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2217 - acc: 0.9238 - val_loss: 0.2680 - val_acc: 0.9255
Epoch 237/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2161 - acc: 0.9229- ETA: 0s - loss: 0.2098 - aEpoch 00237: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2163 - acc: 0.9227 - val_loss: 0.2676 - val_acc: 0.9227
Epoch 238/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2208 - acc: 0.9261Epoch 00238: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2212 - acc: 0.9259 - val_loss: 0.2734 - val_acc: 0.9244
Epoch 239/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2210 - acc: 0.9248Epoch 00239: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.2214 - acc: 0.9248 - val_loss: 0.2642 - val_acc: 0.9232
Epoch 240/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2190 - acc: 0.9232Epoch 00240: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2204 - acc: 0.9226 - val_loss: 0.2640 - val_acc: 0.9232
Epoch 241/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2160 - acc: 0.9269Epoch 00241: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2166 - acc: 0.9267 - val_loss: 0.2734 - val_acc: 0.9210
Epoch 242/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2240 - acc: 0.9244Epoch 00242: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2249 - acc: 0.9244 - val_loss: 0.2685 - val_acc: 0.9204
Epoch 243/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2150 - acc: 0.925 - ETA: 0s - loss: 0.2161 - acc: 0.9250Epoch 00243: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2160 - acc: 0.9250 - val_loss: 0.2754 - val_acc: 0.9221
Epoch 244/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2125 - acc: 0.9281Epoch 00244: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2128 - acc: 0.9276 - val_loss: 0.2687 - val_acc: 0.9249
Epoch 245/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2145 - acc: 0.9265Epoch 00245: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2159 - acc: 0.9261 - val_loss: 0.2698 - val_acc: 0.9238
Epoch 246/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2113 - acc: 0.9278Epoch 00246: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2113 - acc: 0.9277 - val_loss: 0.2726 - val_acc: 0.9244
Epoch 247/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2212 - acc: 0.9251Epoch 00247: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2209 - acc: 0.9251 - val_loss: 0.2783 - val_acc: 0.9227
Epoch 248/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2253 - acc: 0.9209Epoch 00248: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2247 - acc: 0.9216 - val_loss: 0.2667 - val_acc: 0.9244
Epoch 249/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2154 - acc: 0.9275Epoch 00249: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2149 - acc: 0.9276 - val_loss: 0.2727 - val_acc: 0.9188
Epoch 250/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2138 - acc: 0.9277Epoch 00250: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.2169 - acc: 0.9269 - val_loss: 0.2652 - val_acc: 0.9204
10


 trianing 



Train on 17895 samples, validate on 1785 samples
Epoch 1/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 4.9490 - acc: 0.0910Epoch 00001: val_loss improved from inf to 2.48436, saving model to weights10.hdf5
17895/17895 [==============================] - 3s 169us/step - loss: 4.8484 - acc: 0.0913 - val_loss: 2.4844 - val_acc: 0.0824
Epoch 2/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.5010 - acc: 0.0925Epoch 00002: val_loss improved from 2.48436 to 2.46450, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.5000 - acc: 0.0929 - val_loss: 2.4645 - val_acc: 0.1109
Epoch 3/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4640 - acc: 0.1044Epoch 00003: val_loss improved from 2.46450 to 2.43618, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.4643 - acc: 0.1043 - val_loss: 2.4362 - val_acc: 0.1104
Epoch 4/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4457 - acc: 0.1072Epoch 00004: val_loss improved from 2.43618 to 2.42162, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.4460 - acc: 0.1067 - val_loss: 2.4216 - val_acc: 0.1104
Epoch 5/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4368 - acc: 0.1086- ETA: 0s - loss: 2.4370 - acc: 0.108Epoch 00005: val_loss improved from 2.42162 to 2.41714, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.4374 - acc: 0.1072 - val_loss: 2.4171 - val_acc: 0.1171
Epoch 6/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4256 - acc: 0.1170Epoch 00006: val_loss improved from 2.41714 to 2.40501, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.4245 - acc: 0.1165 - val_loss: 2.4050 - val_acc: 0.1199
Epoch 7/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4142 - acc: 0.1178Epoch 00007: val_loss improved from 2.40501 to 2.40105, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.4130 - acc: 0.1192 - val_loss: 2.4011 - val_acc: 0.1193
Epoch 8/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4079 - acc: 0.1203Epoch 00008: val_loss improved from 2.40105 to 2.39928, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.4090 - acc: 0.1192 - val_loss: 2.3993 - val_acc: 0.1193
Epoch 9/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4043 - acc: 0.1200Epoch 00009: val_loss improved from 2.39928 to 2.39788, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.4048 - acc: 0.1194 - val_loss: 2.3979 - val_acc: 0.1193
Epoch 10/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.4010 - acc: 0.1188- ETA: 0s - loss: 2.4018 - acc: Epoch 00010: val_loss improved from 2.39788 to 2.39653, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.4017 - acc: 0.1193 - val_loss: 2.3965 - val_acc: 0.1193
Epoch 11/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.3992 - acc: 0.1192Epoch 00011: val_loss improved from 2.39653 to 2.39539, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.3994 - acc: 0.1187 - val_loss: 2.3954 - val_acc: 0.1193
Epoch 12/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.3934 - acc: 0.1272Epoch 00012: val_loss improved from 2.39539 to 2.32978, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.3921 - acc: 0.1280 - val_loss: 2.3298 - val_acc: 0.1658
Epoch 13/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.3456 - acc: 0.1495Epoch 00013: val_loss improved from 2.32978 to 2.30222, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.3446 - acc: 0.1509 - val_loss: 2.3022 - val_acc: 0.1798
Epoch 14/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.2977 - acc: 0.1733Epoch 00014: val_loss improved from 2.30222 to 2.14661, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.2950 - acc: 0.1738 - val_loss: 2.1466 - val_acc: 0.2347
Epoch 15/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.1873 - acc: 0.2077- ETA: 1s - loss: 2.22Epoch 00015: val_loss improved from 2.14661 to 1.94943, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.1823 - acc: 0.2109 - val_loss: 1.9494 - val_acc: 0.2891
Epoch 16/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 2.0401 - acc: 0.2680Epoch 00016: val_loss improved from 1.94943 to 1.79743, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 2.0378 - acc: 0.2683 - val_loss: 1.7974 - val_acc: 0.3787
Epoch 17/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.9020 - acc: 0.3186Epoch 00017: val_loss improved from 1.79743 to 1.65354, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 1.8997 - acc: 0.3193 - val_loss: 1.6535 - val_acc: 0.4426
Epoch 18/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.7856 - acc: 0.3562Epoch 00018: val_loss improved from 1.65354 to 1.49982, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 1.7822 - acc: 0.3579 - val_loss: 1.4998 - val_acc: 0.4891
Epoch 19/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.6723 - acc: 0.3954Epoch 00019: val_loss improved from 1.49982 to 1.36797, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 1.6696 - acc: 0.3972 - val_loss: 1.3680 - val_acc: 0.5350
Epoch 20/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.5592 - acc: 0.4349Epoch 00020: val_loss improved from 1.36797 to 1.24887, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 1.5579 - acc: 0.4350 - val_loss: 1.2489 - val_acc: 0.5871
Epoch 21/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.4804 - acc: 0.4659Epoch 00021: val_loss improved from 1.24887 to 1.19068, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 1.4779 - acc: 0.4668 - val_loss: 1.1907 - val_acc: 0.5989
Epoch 22/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.3861 - acc: 0.5064Epoch 00022: val_loss improved from 1.19068 to 1.05246, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 1.3844 - acc: 0.5065 - val_loss: 1.0525 - val_acc: 0.6471
Epoch 23/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.2887 - acc: 0.5436Epoch 00023: val_loss improved from 1.05246 to 0.95231, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 1.2870 - acc: 0.5440 - val_loss: 0.9523 - val_acc: 0.6896
Epoch 24/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.2117 - acc: 0.5734Epoch 00024: val_loss improved from 0.95231 to 0.90818, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 1.2106 - acc: 0.5734 - val_loss: 0.9082 - val_acc: 0.6952
Epoch 25/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.1396 - acc: 0.6015Epoch 00025: val_loss improved from 0.90818 to 0.81490, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 1.1376 - acc: 0.6023 - val_loss: 0.8149 - val_acc: 0.7193
Epoch 26/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.0774 - acc: 0.6209Epoch 00026: val_loss improved from 0.81490 to 0.77387, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 1.0745 - acc: 0.6213 - val_loss: 0.7739 - val_acc: 0.7373
Epoch 27/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 1.0167 - acc: 0.6463Epoch 00027: val_loss improved from 0.77387 to 0.71429, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 1.0155 - acc: 0.6468 - val_loss: 0.7143 - val_acc: 0.7714
Epoch 28/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.9700 - acc: 0.6681Epoch 00028: val_loss improved from 0.71429 to 0.67997, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.9698 - acc: 0.6674 - val_loss: 0.6800 - val_acc: 0.7748
Epoch 29/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.9304 - acc: 0.6788- ETA: 0s - loss: 0.9304 - accEpoch 00029: val_loss improved from 0.67997 to 0.64218, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.9291 - acc: 0.6795 - val_loss: 0.6422 - val_acc: 0.7866
Epoch 30/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.8926 - acc: 0.6906Epoch 00030: val_loss improved from 0.64218 to 0.62163, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.8923 - acc: 0.6905 - val_loss: 0.6216 - val_acc: 0.7994
Epoch 31/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.8621 - acc: 0.7034Epoch 00031: val_loss improved from 0.62163 to 0.58891, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.8616 - acc: 0.7035 - val_loss: 0.5889 - val_acc: 0.8028
Epoch 32/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.8264 - acc: 0.7146Epoch 00032: val_loss improved from 0.58891 to 0.56533, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.8273 - acc: 0.7148 - val_loss: 0.5653 - val_acc: 0.8123
Epoch 33/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.8176 - acc: 0.7189Epoch 00033: val_loss improved from 0.56533 to 0.54320, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.8166 - acc: 0.7192 - val_loss: 0.5432 - val_acc: 0.8218
Epoch 34/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.7912 - acc: 0.7270Epoch 00034: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.7914 - acc: 0.7269 - val_loss: 0.5456 - val_acc: 0.8291
Epoch 35/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.7580 - acc: 0.7385Epoch 00035: val_loss improved from 0.54320 to 0.51265, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.7587 - acc: 0.7385 - val_loss: 0.5126 - val_acc: 0.8319
Epoch 36/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.7372 - acc: 0.7488Epoch 00036: val_loss improved from 0.51265 to 0.50377, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.7373 - acc: 0.7495 - val_loss: 0.5038 - val_acc: 0.8342
Epoch 37/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.7226 - acc: 0.7541Epoch 00037: val_loss improved from 0.50377 to 0.48784, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.7205 - acc: 0.7544 - val_loss: 0.4878 - val_acc: 0.8381
Epoch 38/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.7002 - acc: 0.7571Epoch 00038: val_loss improved from 0.48784 to 0.47437, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.6999 - acc: 0.7573 - val_loss: 0.4744 - val_acc: 0.8409
Epoch 39/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6907 - acc: 0.7648Epoch 00039: val_loss improved from 0.47437 to 0.46780, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.6902 - acc: 0.7647 - val_loss: 0.4678 - val_acc: 0.8398
Epoch 40/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6883 - acc: 0.7677Epoch 00040: val_loss improved from 0.46780 to 0.44663, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.6873 - acc: 0.7668 - val_loss: 0.4466 - val_acc: 0.8487
Epoch 41/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6632 - acc: 0.7752Epoch 00041: val_loss improved from 0.44663 to 0.43524, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.6620 - acc: 0.7760 - val_loss: 0.4352 - val_acc: 0.8532
Epoch 42/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6474 - acc: 0.7821- ETA: 0s - loss: 0.6562 -Epoch 00042: val_loss improved from 0.43524 to 0.42326, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.6455 - acc: 0.7831 - val_loss: 0.4233 - val_acc: 0.8560
Epoch 43/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6279 - acc: 0.7862Epoch 00043: val_loss improved from 0.42326 to 0.41792, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.6287 - acc: 0.7863 - val_loss: 0.4179 - val_acc: 0.8566
Epoch 44/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6264 - acc: 0.7898Epoch 00044: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.6273 - acc: 0.7897 - val_loss: 0.4290 - val_acc: 0.8566
Epoch 45/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.6144 - acc: 0.7937Epoch 00045: val_loss improved from 0.41792 to 0.41408, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.6131 - acc: 0.7942 - val_loss: 0.4141 - val_acc: 0.8605
Epoch 46/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5962 - acc: 0.8028Epoch 00046: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.5962 - acc: 0.8027 - val_loss: 0.4222 - val_acc: 0.8566
Epoch 47/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5915 - acc: 0.7995Epoch 00047: val_loss improved from 0.41408 to 0.39818, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.5928 - acc: 0.7991 - val_loss: 0.3982 - val_acc: 0.8672
Epoch 48/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5766 - acc: 0.8052Epoch 00048: val_loss improved from 0.39818 to 0.38485, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.5761 - acc: 0.8049 - val_loss: 0.3849 - val_acc: 0.8661
Epoch 49/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5705 - acc: 0.8088Epoch 00049: val_loss improved from 0.38485 to 0.38290, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 0.5698 - acc: 0.8087 - val_loss: 0.3829 - val_acc: 0.8667
Epoch 50/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5613 - acc: 0.8112Epoch 00050: val_loss improved from 0.38290 to 0.37707, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.5593 - acc: 0.8120 - val_loss: 0.3771 - val_acc: 0.8751
Epoch 51/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5555 - acc: 0.8128Epoch 00051: val_loss improved from 0.37707 to 0.36722, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 0.5550 - acc: 0.8131 - val_loss: 0.3672 - val_acc: 0.8784
Epoch 52/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5437 - acc: 0.8175Epoch 00052: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.5433 - acc: 0.8182 - val_loss: 0.3696 - val_acc: 0.8768
Epoch 53/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5369 - acc: 0.8190Epoch 00053: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.5385 - acc: 0.8193 - val_loss: 0.3675 - val_acc: 0.8773
Epoch 54/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5404 - acc: 0.8158Epoch 00054: val_loss improved from 0.36722 to 0.35874, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.5400 - acc: 0.8155 - val_loss: 0.3587 - val_acc: 0.8779
Epoch 55/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5199 - acc: 0.8264Epoch 00055: val_loss improved from 0.35874 to 0.35635, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 0.5204 - acc: 0.8263 - val_loss: 0.3563 - val_acc: 0.8779
Epoch 56/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5177 - acc: 0.8267Epoch 00056: val_loss improved from 0.35635 to 0.34104, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.5191 - acc: 0.8265 - val_loss: 0.3410 - val_acc: 0.8835
Epoch 57/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5130 - acc: 0.8274Epoch 00057: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.5134 - acc: 0.8277 - val_loss: 0.3446 - val_acc: 0.8863
Epoch 58/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.5137 - acc: 0.8313- ETA: 1s - loss: 0.510Epoch 00058: val_loss improved from 0.34104 to 0.33911, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.5121 - acc: 0.8314 - val_loss: 0.3391 - val_acc: 0.8846
Epoch 59/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4973 - acc: 0.8325Epoch 00059: val_loss improved from 0.33911 to 0.33732, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4975 - acc: 0.8328 - val_loss: 0.3373 - val_acc: 0.8868
Epoch 60/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4976 - acc: 0.8320Epoch 00060: val_loss improved from 0.33732 to 0.33717, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4986 - acc: 0.8312 - val_loss: 0.3372 - val_acc: 0.8908
Epoch 61/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4826 - acc: 0.8363Epoch 00061: val_loss improved from 0.33717 to 0.32953, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4833 - acc: 0.8360 - val_loss: 0.3295 - val_acc: 0.8891
Epoch 62/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4791 - acc: 0.8407- ETA: 0s - loss: 0.4817 - acc: 0.Epoch 00062: val_loss improved from 0.32953 to 0.31868, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4805 - acc: 0.8404 - val_loss: 0.3187 - val_acc: 0.8908
Epoch 63/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4760 - acc: 0.8431Epoch 00063: val_loss improved from 0.31868 to 0.31171, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4760 - acc: 0.8430 - val_loss: 0.3117 - val_acc: 0.8941
Epoch 64/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4730 - acc: 0.8426Epoch 00064: val_loss did not improve
17895/17895 [==============================] - 2s 89us/step - loss: 0.4729 - acc: 0.8425 - val_loss: 0.3124 - val_acc: 0.8964
Epoch 65/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4699 - acc: 0.8421- ETA: 1s - loss: 0.Epoch 00065: val_loss did not improve
17895/17895 [==============================] - 2s 90us/step - loss: 0.4711 - acc: 0.8417 - val_loss: 0.3122 - val_acc: 0.8986
Epoch 66/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4528 - acc: 0.8485Epoch 00066: val_loss improved from 0.31171 to 0.30919, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4526 - acc: 0.8479 - val_loss: 0.3092 - val_acc: 0.8975
Epoch 67/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4613 - acc: 0.8465Epoch 00067: val_loss improved from 0.30919 to 0.30810, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 91us/step - loss: 0.4596 - acc: 0.8471 - val_loss: 0.3081 - val_acc: 0.8980
Epoch 68/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4433 - acc: 0.8479Epoch 00068: val_loss improved from 0.30810 to 0.29814, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.4447 - acc: 0.8477 - val_loss: 0.2981 - val_acc: 0.9048
Epoch 69/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4407 - acc: 0.8509Epoch 00069: val_loss improved from 0.29814 to 0.29129, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.4410 - acc: 0.8510 - val_loss: 0.2913 - val_acc: 0.9053
Epoch 70/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4341 - acc: 0.8552Epoch 00070: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.4358 - acc: 0.8547 - val_loss: 0.2972 - val_acc: 0.9014
Epoch 71/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4369 - acc: 0.8524Epoch 00071: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.4378 - acc: 0.8520 - val_loss: 0.2985 - val_acc: 0.9020
Epoch 72/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4260 - acc: 0.8591Epoch 00072: val_loss improved from 0.29129 to 0.28493, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.4280 - acc: 0.8584 - val_loss: 0.2849 - val_acc: 0.9053
Epoch 73/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4244 - acc: 0.8590Epoch 00073: val_loss improved from 0.28493 to 0.28213, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 0.4247 - acc: 0.8588 - val_loss: 0.2821 - val_acc: 0.9042
Epoch 74/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4241 - acc: 0.8556Epoch 00074: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.4249 - acc: 0.8552 - val_loss: 0.2872 - val_acc: 0.9048
Epoch 75/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4197 - acc: 0.8586Epoch 00075: val_loss improved from 0.28213 to 0.28121, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.4183 - acc: 0.8588 - val_loss: 0.2812 - val_acc: 0.9042
Epoch 76/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4221 - acc: 0.8554Epoch 00076: val_loss improved from 0.28121 to 0.27784, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.4203 - acc: 0.8561 - val_loss: 0.2778 - val_acc: 0.9048
Epoch 77/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4152 - acc: 0.8612Epoch 00077: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.4161 - acc: 0.8611 - val_loss: 0.2817 - val_acc: 0.9020
Epoch 78/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4002 - acc: 0.8660Epoch 00078: val_loss improved from 0.27784 to 0.27604, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.3986 - acc: 0.8667 - val_loss: 0.2760 - val_acc: 0.9092
Epoch 79/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4148 - acc: 0.8607Epoch 00079: val_loss improved from 0.27604 to 0.27350, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 94us/step - loss: 0.4145 - acc: 0.8604 - val_loss: 0.2735 - val_acc: 0.9087
Epoch 80/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4023 - acc: 0.8658Epoch 00080: val_loss improved from 0.27350 to 0.26788, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.4047 - acc: 0.8649 - val_loss: 0.2679 - val_acc: 0.9137
Epoch 81/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3998 - acc: 0.8649Epoch 00081: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.3999 - acc: 0.8654 - val_loss: 0.2706 - val_acc: 0.9126
Epoch 82/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3973 - acc: 0.8649Epoch 00082: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.3974 - acc: 0.8650 - val_loss: 0.2715 - val_acc: 0.9076
Epoch 83/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.4013 - acc: 0.8620Epoch 00083: val_loss improved from 0.26788 to 0.26719, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.4000 - acc: 0.8626 - val_loss: 0.2672 - val_acc: 0.9115
Epoch 84/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3865 - acc: 0.8710Epoch 00084: val_loss improved from 0.26719 to 0.26398, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.3854 - acc: 0.8712 - val_loss: 0.2640 - val_acc: 0.9070
Epoch 85/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3911 - acc: 0.8677Epoch 00085: val_loss improved from 0.26398 to 0.26318, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 94us/step - loss: 0.3909 - acc: 0.8680 - val_loss: 0.2632 - val_acc: 0.9171
Epoch 86/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3844 - acc: 0.8706Epoch 00086: val_loss improved from 0.26318 to 0.25737, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 0.3844 - acc: 0.8705 - val_loss: 0.2574 - val_acc: 0.9171
Epoch 87/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3854 - acc: 0.8702Epoch 00087: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.3832 - acc: 0.8711 - val_loss: 0.2665 - val_acc: 0.9126
Epoch 88/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3818 - acc: 0.8707Epoch 00088: val_loss improved from 0.25737 to 0.25645, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 0.3833 - acc: 0.8705 - val_loss: 0.2564 - val_acc: 0.9154
Epoch 89/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3835 - acc: 0.8692Epoch 00089: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.3823 - acc: 0.8701 - val_loss: 0.2615 - val_acc: 0.9143
Epoch 90/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3725 - acc: 0.8750Epoch 00090: val_loss improved from 0.25645 to 0.25571, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.3740 - acc: 0.8745 - val_loss: 0.2557 - val_acc: 0.9165
Epoch 91/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3693 - acc: 0.8737Epoch 00091: val_loss improved from 0.25571 to 0.25379, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.3684 - acc: 0.8741 - val_loss: 0.2538 - val_acc: 0.9109
Epoch 92/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3689 - acc: 0.8744Epoch 00092: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.3691 - acc: 0.8745 - val_loss: 0.2579 - val_acc: 0.9154
Epoch 93/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3731 - acc: 0.8731Epoch 00093: val_loss improved from 0.25379 to 0.25264, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 94us/step - loss: 0.3751 - acc: 0.8724 - val_loss: 0.2526 - val_acc: 0.9160
Epoch 94/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3667 - acc: 0.8786Epoch 00094: val_loss improved from 0.25264 to 0.24470, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 94us/step - loss: 0.3660 - acc: 0.8784 - val_loss: 0.2447 - val_acc: 0.9182
Epoch 95/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3634 - acc: 0.8758Epoch 00095: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.3650 - acc: 0.8749 - val_loss: 0.2459 - val_acc: 0.9165
Epoch 96/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3600 - acc: 0.8784Epoch 00096: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.3611 - acc: 0.8779 - val_loss: 0.2455 - val_acc: 0.9227
Epoch 97/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3552 - acc: 0.8797Epoch 00097: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.3542 - acc: 0.8797 - val_loss: 0.2524 - val_acc: 0.9148
Epoch 98/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3598 - acc: 0.8811Epoch 00098: val_loss improved from 0.24470 to 0.24391, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 94us/step - loss: 0.3604 - acc: 0.8805 - val_loss: 0.2439 - val_acc: 0.9182
Epoch 99/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3490 - acc: 0.8800Epoch 00099: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.3495 - acc: 0.8802 - val_loss: 0.2446 - val_acc: 0.9210
Epoch 100/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3515 - acc: 0.8808Epoch 00100: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.3513 - acc: 0.8810 - val_loss: 0.2455 - val_acc: 0.9210
Epoch 101/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3559 - acc: 0.8798Epoch 00101: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.3551 - acc: 0.8797 - val_loss: 0.2465 - val_acc: 0.9176
Epoch 102/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3402 - acc: 0.8860Epoch 00102: val_loss did not improve
17895/17895 [==============================] - 2s 93us/step - loss: 0.3403 - acc: 0.8862 - val_loss: 0.2521 - val_acc: 0.9171
Epoch 103/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3449 - acc: 0.8824Epoch 00103: val_loss improved from 0.24391 to 0.24202, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 94us/step - loss: 0.3455 - acc: 0.8825 - val_loss: 0.2420 - val_acc: 0.9171
Epoch 104/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3447 - acc: 0.8836Epoch 00104: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.3456 - acc: 0.8837 - val_loss: 0.2538 - val_acc: 0.9115
Epoch 105/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3393 - acc: 0.8860Epoch 00105: val_loss improved from 0.24202 to 0.23520, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.3386 - acc: 0.8866 - val_loss: 0.2352 - val_acc: 0.9204
Epoch 106/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3357 - acc: 0.8871Epoch 00106: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.3361 - acc: 0.8871 - val_loss: 0.2369 - val_acc: 0.9249
Epoch 107/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3422 - acc: 0.8858Epoch 00107: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.3445 - acc: 0.8846 - val_loss: 0.2390 - val_acc: 0.9210
Epoch 108/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3347 - acc: 0.8845Epoch 00108: val_loss improved from 0.23520 to 0.23485, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.3353 - acc: 0.8840 - val_loss: 0.2348 - val_acc: 0.9221
Epoch 109/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3323 - acc: 0.8883Epoch 00109: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.3303 - acc: 0.8891 - val_loss: 0.2398 - val_acc: 0.9210
Epoch 110/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3331 - acc: 0.8923Epoch 00110: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.3331 - acc: 0.8918 - val_loss: 0.2371 - val_acc: 0.9176
Epoch 111/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3248 - acc: 0.8918Epoch 00111: val_loss improved from 0.23485 to 0.23078, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 0.3251 - acc: 0.8915 - val_loss: 0.2308 - val_acc: 0.9277
Epoch 112/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3329 - acc: 0.8871Epoch 00112: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.3313 - acc: 0.8877 - val_loss: 0.2382 - val_acc: 0.9188
Epoch 113/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3219 - acc: 0.8911Epoch 00113: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.3237 - acc: 0.8907 - val_loss: 0.2386 - val_acc: 0.9188
Epoch 114/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3229 - acc: 0.8902Epoch 00114: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.3267 - acc: 0.8885 - val_loss: 0.2327 - val_acc: 0.9277
Epoch 115/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3254 - acc: 0.8896Epoch 00115: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.3255 - acc: 0.8898 - val_loss: 0.2319 - val_acc: 0.9238
Epoch 116/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3204 - acc: 0.8921Epoch 00116: val_loss improved from 0.23078 to 0.22625, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 0.3182 - acc: 0.8929 - val_loss: 0.2262 - val_acc: 0.9266
Epoch 117/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3224 - acc: 0.8888Epoch 00117: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.3221 - acc: 0.8889 - val_loss: 0.2274 - val_acc: 0.9272
Epoch 118/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3210 - acc: 0.8941Epoch 00118: val_loss improved from 0.22625 to 0.22582, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.3210 - acc: 0.8943 - val_loss: 0.2258 - val_acc: 0.9255
Epoch 119/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3237 - acc: 0.8908Epoch 00119: val_loss improved from 0.22582 to 0.22536, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.3210 - acc: 0.8912 - val_loss: 0.2254 - val_acc: 0.9261
Epoch 120/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3089 - acc: 0.8946Epoch 00120: val_loss improved from 0.22536 to 0.22242, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.3109 - acc: 0.8946 - val_loss: 0.2224 - val_acc: 0.9244
Epoch 121/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3046 - acc: 0.8956Epoch 00121: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.3031 - acc: 0.8961 - val_loss: 0.2266 - val_acc: 0.9277
Epoch 122/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3163 - acc: 0.8918Epoch 00122: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.3159 - acc: 0.8918 - val_loss: 0.2239 - val_acc: 0.9283
Epoch 123/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3059 - acc: 0.8950Epoch 00123: val_loss improved from 0.22242 to 0.22167, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 94us/step - loss: 0.3055 - acc: 0.8950 - val_loss: 0.2217 - val_acc: 0.9272
Epoch 124/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3117 - acc: 0.8938Epoch 00124: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.3117 - acc: 0.8937 - val_loss: 0.2258 - val_acc: 0.9261
Epoch 125/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3010 - acc: 0.8984Epoch 00125: val_loss improved from 0.22167 to 0.22041, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 94us/step - loss: 0.2982 - acc: 0.8993 - val_loss: 0.2204 - val_acc: 0.9261
Epoch 126/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3014 - acc: 0.8970Epoch 00126: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2998 - acc: 0.8973 - val_loss: 0.2231 - val_acc: 0.9221
Epoch 127/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3080 - acc: 0.8946Epoch 00127: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.3087 - acc: 0.8940 - val_loss: 0.2257 - val_acc: 0.9244
Epoch 128/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2937 - acc: 0.9001Epoch 00128: val_loss improved from 0.22041 to 0.22027, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.2938 - acc: 0.8993 - val_loss: 0.2203 - val_acc: 0.9277
Epoch 129/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2951 - acc: 0.8993Epoch 00129: val_loss improved from 0.22027 to 0.22013, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.2981 - acc: 0.8982 - val_loss: 0.2201 - val_acc: 0.9294
Epoch 130/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2906 - acc: 0.9001Epoch 00130: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2926 - acc: 0.8992 - val_loss: 0.2222 - val_acc: 0.9294
Epoch 131/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.3014 - acc: 0.8987Epoch 00131: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.3002 - acc: 0.8987 - val_loss: 0.2212 - val_acc: 0.9300
Epoch 132/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2955 - acc: 0.8977Epoch 00132: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2952 - acc: 0.8979 - val_loss: 0.2217 - val_acc: 0.9283
Epoch 133/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2890 - acc: 0.9000Epoch 00133: val_loss improved from 0.22013 to 0.21692, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 0.2891 - acc: 0.9000 - val_loss: 0.2169 - val_acc: 0.9300
Epoch 134/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2934 - acc: 0.9005Epoch 00134: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2937 - acc: 0.9008 - val_loss: 0.2216 - val_acc: 0.9261
Epoch 135/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2921 - acc: 0.8997Epoch 00135: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2936 - acc: 0.8994 - val_loss: 0.2201 - val_acc: 0.9272
Epoch 136/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2913 - acc: 0.9021Epoch 00136: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2933 - acc: 0.9016 - val_loss: 0.2228 - val_acc: 0.9249
Epoch 137/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2892 - acc: 0.8994Epoch 00137: val_loss improved from 0.21692 to 0.21546, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.2910 - acc: 0.8995 - val_loss: 0.2155 - val_acc: 0.9300
Epoch 138/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2833 - acc: 0.9042Epoch 00138: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2843 - acc: 0.9038 - val_loss: 0.2161 - val_acc: 0.9277
Epoch 139/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2875 - acc: 0.9020Epoch 00139: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2894 - acc: 0.9012 - val_loss: 0.2208 - val_acc: 0.9294
Epoch 140/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2703 - acc: 0.9061Epoch 00140: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2714 - acc: 0.9056 - val_loss: 0.2157 - val_acc: 0.9289
Epoch 141/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2873 - acc: 0.9011Epoch 00141: val_loss improved from 0.21546 to 0.20676, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.2875 - acc: 0.9009 - val_loss: 0.2068 - val_acc: 0.9328
Epoch 142/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2809 - acc: 0.9040Epoch 00142: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2790 - acc: 0.9043 - val_loss: 0.2118 - val_acc: 0.9345
Epoch 143/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2860 - acc: 0.9025Epoch 00143: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2864 - acc: 0.9023 - val_loss: 0.2169 - val_acc: 0.9266
Epoch 144/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2778 - acc: 0.9046Epoch 00144: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2785 - acc: 0.9042 - val_loss: 0.2133 - val_acc: 0.9322
Epoch 145/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2727 - acc: 0.9082Epoch 00145: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2711 - acc: 0.9087 - val_loss: 0.2175 - val_acc: 0.9311
Epoch 146/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2733 - acc: 0.9075Epoch 00146: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2739 - acc: 0.9070 - val_loss: 0.2111 - val_acc: 0.9328
Epoch 147/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2769 - acc: 0.9064Epoch 00147: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2757 - acc: 0.9066 - val_loss: 0.2120 - val_acc: 0.9333
Epoch 148/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2777 - acc: 0.9040Epoch 00148: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2804 - acc: 0.9033 - val_loss: 0.2132 - val_acc: 0.9294
Epoch 149/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2684 - acc: 0.9074Epoch 00149: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2699 - acc: 0.9072 - val_loss: 0.2097 - val_acc: 0.9367
Epoch 150/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2731 - acc: 0.9076Epoch 00150: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2722 - acc: 0.9079 - val_loss: 0.2072 - val_acc: 0.9333
Epoch 151/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2693 - acc: 0.9084Epoch 00151: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2695 - acc: 0.9080 - val_loss: 0.2135 - val_acc: 0.9289
Epoch 152/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2708 - acc: 0.9049Epoch 00152: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2705 - acc: 0.9056 - val_loss: 0.2084 - val_acc: 0.9317
Epoch 153/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2687 - acc: 0.9079Epoch 00153: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2697 - acc: 0.9076 - val_loss: 0.2105 - val_acc: 0.9317
Epoch 154/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2754 - acc: 0.9061Epoch 00154: val_loss improved from 0.20676 to 0.20466, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.2753 - acc: 0.9063 - val_loss: 0.2047 - val_acc: 0.9350
Epoch 155/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2691 - acc: 0.9088Epoch 00155: val_loss improved from 0.20466 to 0.20429, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.2698 - acc: 0.9088 - val_loss: 0.2043 - val_acc: 0.9356
Epoch 156/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2639 - acc: 0.9102Epoch 00156: val_loss improved from 0.20429 to 0.20306, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.2639 - acc: 0.9101 - val_loss: 0.2031 - val_acc: 0.9333
Epoch 157/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2682 - acc: 0.9070Epoch 00157: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2684 - acc: 0.9065 - val_loss: 0.2095 - val_acc: 0.9339
Epoch 158/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2643 - acc: 0.9097Epoch 00158: val_loss improved from 0.20306 to 0.20239, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.2660 - acc: 0.9091 - val_loss: 0.2024 - val_acc: 0.9328
Epoch 159/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2679 - acc: 0.9076Epoch 00159: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2671 - acc: 0.9079 - val_loss: 0.2036 - val_acc: 0.9322
Epoch 160/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2664 - acc: 0.9076Epoch 00160: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2667 - acc: 0.9073 - val_loss: 0.2114 - val_acc: 0.9283
Epoch 161/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2624 - acc: 0.9093Epoch 00161: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2623 - acc: 0.9091 - val_loss: 0.2081 - val_acc: 0.9328
Epoch 162/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2643 - acc: 0.9089Epoch 00162: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2650 - acc: 0.9087 - val_loss: 0.2041 - val_acc: 0.9361
Epoch 163/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2576 - acc: 0.9104Epoch 00163: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2568 - acc: 0.9108 - val_loss: 0.2077 - val_acc: 0.9272
Epoch 164/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2546 - acc: 0.9118Epoch 00164: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2534 - acc: 0.9125 - val_loss: 0.2054 - val_acc: 0.9322
Epoch 165/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2630 - acc: 0.9115Epoch 00165: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2638 - acc: 0.9112 - val_loss: 0.2053 - val_acc: 0.9328
Epoch 166/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2492 - acc: 0.9151Epoch 00166: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2507 - acc: 0.9144 - val_loss: 0.2025 - val_acc: 0.9311
Epoch 167/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2614 - acc: 0.9094Epoch 00167: val_loss improved from 0.20239 to 0.19885, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 94us/step - loss: 0.2606 - acc: 0.9097 - val_loss: 0.1988 - val_acc: 0.9373
Epoch 168/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2508 - acc: 0.9163Epoch 00168: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2503 - acc: 0.9165 - val_loss: 0.2013 - val_acc: 0.9300
Epoch 169/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2495 - acc: 0.9132Epoch 00169: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2495 - acc: 0.9132 - val_loss: 0.2055 - val_acc: 0.9345
Epoch 170/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2611 - acc: 0.9096Epoch 00170: val_loss improved from 0.19885 to 0.19811, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.2613 - acc: 0.9094 - val_loss: 0.1981 - val_acc: 0.9322
Epoch 171/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2501 - acc: 0.9168Epoch 00171: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2509 - acc: 0.9163 - val_loss: 0.1994 - val_acc: 0.9378
Epoch 172/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2560 - acc: 0.9142Epoch 00172: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2551 - acc: 0.9143 - val_loss: 0.2041 - val_acc: 0.9322
Epoch 173/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2515 - acc: 0.9151Epoch 00173: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2531 - acc: 0.9149 - val_loss: 0.1986 - val_acc: 0.9361
Epoch 174/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2510 - acc: 0.9168Epoch 00174: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2503 - acc: 0.9170 - val_loss: 0.2019 - val_acc: 0.9339
Epoch 175/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2430 - acc: 0.9157Epoch 00175: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2436 - acc: 0.9161 - val_loss: 0.2043 - val_acc: 0.9328
Epoch 176/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2495 - acc: 0.9151Epoch 00176: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2482 - acc: 0.9157 - val_loss: 0.2021 - val_acc: 0.9328
Epoch 177/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2456 - acc: 0.9151Epoch 00177: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2474 - acc: 0.9147 - val_loss: 0.2047 - val_acc: 0.9300
Epoch 178/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2512 - acc: 0.9141Epoch 00178: val_loss improved from 0.19811 to 0.19617, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 94us/step - loss: 0.2509 - acc: 0.9143 - val_loss: 0.1962 - val_acc: 0.9350
Epoch 179/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2401 - acc: 0.9175Epoch 00179: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2390 - acc: 0.9176 - val_loss: 0.2040 - val_acc: 0.9305
Epoch 180/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2489 - acc: 0.9144Epoch 00180: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2496 - acc: 0.9142 - val_loss: 0.2009 - val_acc: 0.9361
Epoch 181/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2451 - acc: 0.9166Epoch 00181: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2463 - acc: 0.9158 - val_loss: 0.2048 - val_acc: 0.9328
Epoch 182/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2440 - acc: 0.9176Epoch 00182: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2446 - acc: 0.9175 - val_loss: 0.2009 - val_acc: 0.9339
Epoch 183/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2395 - acc: 0.9192Epoch 00183: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2401 - acc: 0.9189 - val_loss: 0.1997 - val_acc: 0.9328
Epoch 184/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2365 - acc: 0.9195Epoch 00184: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2361 - acc: 0.9196 - val_loss: 0.1998 - val_acc: 0.9328
Epoch 185/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2491 - acc: 0.9158Epoch 00185: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2477 - acc: 0.9161 - val_loss: 0.1981 - val_acc: 0.9333
Epoch 186/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2397 - acc: 0.9190Epoch 00186: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2397 - acc: 0.9191 - val_loss: 0.2052 - val_acc: 0.9289
Epoch 187/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2302 - acc: 0.9228Epoch 00187: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2281 - acc: 0.9233 - val_loss: 0.1988 - val_acc: 0.9328
Epoch 188/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2349 - acc: 0.9218Epoch 00188: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2376 - acc: 0.9208 - val_loss: 0.2004 - val_acc: 0.9345
Epoch 189/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2317 - acc: 0.9201Epoch 00189: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2302 - acc: 0.9208 - val_loss: 0.1964 - val_acc: 0.9384
Epoch 190/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2413 - acc: 0.9183Epoch 00190: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2413 - acc: 0.9181 - val_loss: 0.2021 - val_acc: 0.9322
Epoch 191/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2379 - acc: 0.9198Epoch 00191: val_loss improved from 0.19617 to 0.19476, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 92us/step - loss: 0.2368 - acc: 0.9204 - val_loss: 0.1948 - val_acc: 0.9356
Epoch 192/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2333 - acc: 0.9203Epoch 00192: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2326 - acc: 0.9204 - val_loss: 0.1956 - val_acc: 0.9350
Epoch 193/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2331 - acc: 0.9227Epoch 00193: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2338 - acc: 0.9226 - val_loss: 0.1977 - val_acc: 0.9356
Epoch 194/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2317 - acc: 0.9190Epoch 00194: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2323 - acc: 0.9189 - val_loss: 0.1958 - val_acc: 0.9345
Epoch 195/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2304 - acc: 0.9241Epoch 00195: val_loss improved from 0.19476 to 0.19163, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.2315 - acc: 0.9236 - val_loss: 0.1916 - val_acc: 0.9389
Epoch 196/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2317 - acc: 0.9216Epoch 00196: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2314 - acc: 0.9215 - val_loss: 0.1994 - val_acc: 0.9361
Epoch 197/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2367 - acc: 0.9205Epoch 00197: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2361 - acc: 0.9209 - val_loss: 0.1974 - val_acc: 0.9356
Epoch 198/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2326 - acc: 0.9216Epoch 00198: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2322 - acc: 0.9216 - val_loss: 0.1925 - val_acc: 0.9373
Epoch 199/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2374 - acc: 0.9189Epoch 00199: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2381 - acc: 0.9189 - val_loss: 0.1982 - val_acc: 0.9317
Epoch 200/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2267 - acc: 0.9225Epoch 00200: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2257 - acc: 0.9225 - val_loss: 0.1997 - val_acc: 0.9328
Epoch 201/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2311 - acc: 0.9206Epoch 00201: val_loss improved from 0.19163 to 0.18984, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 93us/step - loss: 0.2303 - acc: 0.9214 - val_loss: 0.1898 - val_acc: 0.9401
Epoch 202/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2373 - acc: 0.9198Epoch 00202: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2381 - acc: 0.9196 - val_loss: 0.2030 - val_acc: 0.9361
Epoch 203/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2262 - acc: 0.9229Epoch 00203: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2241 - acc: 0.9237 - val_loss: 0.2030 - val_acc: 0.9300
Epoch 204/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2323 - acc: 0.9186Epoch 00204: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2327 - acc: 0.9186 - val_loss: 0.1941 - val_acc: 0.9384
Epoch 205/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2290 - acc: 0.9218Epoch 00205: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2296 - acc: 0.9217 - val_loss: 0.1932 - val_acc: 0.9378
Epoch 206/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2263 - acc: 0.9230Epoch 00206: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2247 - acc: 0.9234 - val_loss: 0.1941 - val_acc: 0.9361
Epoch 207/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2310 - acc: 0.9219Epoch 00207: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2310 - acc: 0.9218 - val_loss: 0.1975 - val_acc: 0.9361
Epoch 208/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2345 - acc: 0.9218Epoch 00208: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2325 - acc: 0.9224 - val_loss: 0.1930 - val_acc: 0.9384
Epoch 209/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2233 - acc: 0.9260Epoch 00209: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2244 - acc: 0.9258 - val_loss: 0.1953 - val_acc: 0.9389
Epoch 210/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2292 - acc: 0.9230Epoch 00210: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2279 - acc: 0.9237 - val_loss: 0.1919 - val_acc: 0.9406
Epoch 211/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2136 - acc: 0.9293Epoch 00211: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2135 - acc: 0.9291 - val_loss: 0.1900 - val_acc: 0.9373
Epoch 212/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2236 - acc: 0.9245Epoch 00212: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2222 - acc: 0.9248 - val_loss: 0.1954 - val_acc: 0.9395
Epoch 213/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2230 - acc: 0.9238Epoch 00213: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2217 - acc: 0.9243 - val_loss: 0.1950 - val_acc: 0.9345
Epoch 214/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2264 - acc: 0.9234Epoch 00214: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2277 - acc: 0.9228 - val_loss: 0.1912 - val_acc: 0.9378
Epoch 215/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2240 - acc: 0.9250Epoch 00215: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2225 - acc: 0.9253 - val_loss: 0.1969 - val_acc: 0.9350
Epoch 216/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2207 - acc: 0.9259Epoch 00216: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2189 - acc: 0.9259 - val_loss: 0.1904 - val_acc: 0.9361
Epoch 217/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2189 - acc: 0.9239Epoch 00217: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2183 - acc: 0.9245 - val_loss: 0.1939 - val_acc: 0.9384
Epoch 218/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2206 - acc: 0.9241Epoch 00218: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2214 - acc: 0.9244 - val_loss: 0.1959 - val_acc: 0.9356
Epoch 219/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2184 - acc: 0.9232Epoch 00219: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2198 - acc: 0.9224 - val_loss: 0.1981 - val_acc: 0.9317
Epoch 220/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2108 - acc: 0.9292Epoch 00220: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2112 - acc: 0.9286 - val_loss: 0.1978 - val_acc: 0.9345
Epoch 221/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2101 - acc: 0.9264Epoch 00221: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2112 - acc: 0.9258 - val_loss: 0.1964 - val_acc: 0.9356
Epoch 222/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2159 - acc: 0.9250Epoch 00222: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2161 - acc: 0.9255 - val_loss: 0.1933 - val_acc: 0.9395
Epoch 223/250
17664/17895 [============================&gt;.] - ETA: 0s - loss: 0.2155 - acc: 0.9275Epoch 00223: val_loss improved from 0.18984 to 0.18712, saving model to weights10.hdf5
17895/17895 [==============================] - 2s 94us/step - loss: 0.2156 - acc: 0.9272 - val_loss: 0.1871 - val_acc: 0.9412
Epoch 224/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2192 - acc: 0.9236Epoch 00224: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2185 - acc: 0.9239 - val_loss: 0.1902 - val_acc: 0.9389
Epoch 225/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2133 - acc: 0.9275Epoch 00225: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2115 - acc: 0.9282 - val_loss: 0.1959 - val_acc: 0.9389
Epoch 226/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2223 - acc: 0.9242Epoch 00226: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2220 - acc: 0.9241 - val_loss: 0.1930 - val_acc: 0.9401
Epoch 227/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2090 - acc: 0.9292Epoch 00227: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2088 - acc: 0.9290 - val_loss: 0.1914 - val_acc: 0.9395
Epoch 228/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2098 - acc: 0.9288Epoch 00228: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2115 - acc: 0.9283 - val_loss: 0.1999 - val_acc: 0.9367
Epoch 229/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2075 - acc: 0.9284Epoch 00229: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2082 - acc: 0.9288 - val_loss: 0.1959 - val_acc: 0.9412
Epoch 230/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2135 - acc: 0.9247Epoch 00230: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2131 - acc: 0.9247 - val_loss: 0.1942 - val_acc: 0.9378
Epoch 231/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2174 - acc: 0.9264Epoch 00231: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2176 - acc: 0.9265 - val_loss: 0.1882 - val_acc: 0.9423
Epoch 232/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2141 - acc: 0.9256Epoch 00232: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2131 - acc: 0.9261 - val_loss: 0.1915 - val_acc: 0.9389
Epoch 233/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2051 - acc: 0.9304Epoch 00233: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2052 - acc: 0.9303 - val_loss: 0.1948 - val_acc: 0.9378
Epoch 234/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2084 - acc: 0.9274Epoch 00234: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2094 - acc: 0.9271 - val_loss: 0.1877 - val_acc: 0.9395
Epoch 235/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2132 - acc: 0.9265Epoch 00235: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2139 - acc: 0.9265 - val_loss: 0.1925 - val_acc: 0.9423
Epoch 236/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2016 - acc: 0.9297Epoch 00236: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2027 - acc: 0.9290 - val_loss: 0.1907 - val_acc: 0.9395
Epoch 237/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2057 - acc: 0.9314Epoch 00237: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2074 - acc: 0.9309 - val_loss: 0.1963 - val_acc: 0.9356
Epoch 238/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2088 - acc: 0.9267Epoch 00238: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2081 - acc: 0.9274 - val_loss: 0.1981 - val_acc: 0.9384
Epoch 239/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2090 - acc: 0.9286Epoch 00239: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2080 - acc: 0.9289 - val_loss: 0.1980 - val_acc: 0.9373
Epoch 240/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2038 - acc: 0.9291Epoch 00240: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2055 - acc: 0.9287 - val_loss: 0.1951 - val_acc: 0.9367
Epoch 241/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2029 - acc: 0.9327Epoch 00241: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2008 - acc: 0.9334 - val_loss: 0.1970 - val_acc: 0.9384
Epoch 242/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2067 - acc: 0.9280Epoch 00242: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2073 - acc: 0.9277 - val_loss: 0.1925 - val_acc: 0.9373
Epoch 243/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2010 - acc: 0.9320Epoch 00243: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2014 - acc: 0.9320 - val_loss: 0.1886 - val_acc: 0.9384
Epoch 244/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2041 - acc: 0.9307Epoch 00244: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2049 - acc: 0.9303 - val_loss: 0.1918 - val_acc: 0.9373
Epoch 245/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2001 - acc: 0.9294Epoch 00245: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.1994 - acc: 0.9296 - val_loss: 0.1949 - val_acc: 0.9395
Epoch 246/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2119 - acc: 0.9271Epoch 00246: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2097 - acc: 0.9277 - val_loss: 0.1954 - val_acc: 0.9361
Epoch 247/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2030 - acc: 0.9310Epoch 00247: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2020 - acc: 0.9312 - val_loss: 0.1976 - val_acc: 0.9373
Epoch 248/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2082 - acc: 0.9293Epoch 00248: val_loss did not improve
17895/17895 [==============================] - 2s 92us/step - loss: 0.2080 - acc: 0.9293 - val_loss: 0.1908 - val_acc: 0.9389
Epoch 249/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.2061 - acc: 0.9300Epoch 00249: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.2062 - acc: 0.9297 - val_loss: 0.1924 - val_acc: 0.9356
Epoch 250/250
17152/17895 [===========================&gt;..] - ETA: 0s - loss: 0.1985 - acc: 0.9315Epoch 00250: val_loss did not improve
17895/17895 [==============================] - 2s 91us/step - loss: 0.1975 - acc: 0.9318 - val_loss: 0.1921 - val_acc: 0.9378
11


 trianing 



Train on 17896 samples, validate on 1784 samples
Epoch 1/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 5.8212 - acc: 0.0876Epoch 00001: val_loss improved from inf to 2.48433, saving model to weights11.hdf5
17896/17896 [==============================] - 3s 185us/step - loss: 5.6858 - acc: 0.0881 - val_loss: 2.4843 - val_acc: 0.0998
Epoch 2/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.5072 - acc: 0.0938Epoch 00002: val_loss improved from 2.48433 to 2.47445, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 92us/step - loss: 2.5071 - acc: 0.0940 - val_loss: 2.4744 - val_acc: 0.1076
Epoch 3/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4629 - acc: 0.1047Epoch 00003: val_loss improved from 2.47445 to 2.43427, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4623 - acc: 0.1044 - val_loss: 2.4343 - val_acc: 0.1076
Epoch 4/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4413 - acc: 0.1072Epoch 00004: val_loss improved from 2.43427 to 2.43161, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4398 - acc: 0.1070 - val_loss: 2.4316 - val_acc: 0.1076
Epoch 5/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4335 - acc: 0.1072Epoch 00005: val_loss improved from 2.43161 to 2.43065, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4341 - acc: 0.1068 - val_loss: 2.4306 - val_acc: 0.1076
Epoch 6/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4326 - acc: 0.1091Epoch 00006: val_loss improved from 2.43065 to 2.42988, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4317 - acc: 0.1089 - val_loss: 2.4299 - val_acc: 0.1076
Epoch 7/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4289 - acc: 0.1095Epoch 00007: val_loss improved from 2.42988 to 2.42922, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4292 - acc: 0.1094 - val_loss: 2.4292 - val_acc: 0.1076
Epoch 8/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4273 - acc: 0.1086Epoch 00008: val_loss improved from 2.42922 to 2.42866, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4277 - acc: 0.1085 - val_loss: 2.4287 - val_acc: 0.1076
Epoch 9/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4273 - acc: 0.1076Epoch 00009: val_loss improved from 2.42866 to 2.42821, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4276 - acc: 0.1078 - val_loss: 2.4282 - val_acc: 0.1076
Epoch 10/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4258 - acc: 0.1095Epoch 00010: val_loss improved from 2.42821 to 2.42778, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4261 - acc: 0.1091 - val_loss: 2.4278 - val_acc: 0.1076
Epoch 11/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4264 - acc: 0.1088Epoch 00011: val_loss improved from 2.42778 to 2.42743, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4256 - acc: 0.1088 - val_loss: 2.4274 - val_acc: 0.1076
Epoch 12/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4259 - acc: 0.1057Epoch 00012: val_loss improved from 2.42743 to 2.42717, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.4257 - acc: 0.1060 - val_loss: 2.4272 - val_acc: 0.1076
Epoch 13/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4253 - acc: 0.1097Epoch 00013: val_loss improved from 2.42717 to 2.42689, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4249 - acc: 0.1094 - val_loss: 2.4269 - val_acc: 0.1076
Epoch 14/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4243 - acc: 0.1084Epoch 00014: val_loss improved from 2.42689 to 2.42666, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4247 - acc: 0.1087 - val_loss: 2.4267 - val_acc: 0.1076
Epoch 15/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4244 - acc: 0.1091Epoch 00015: val_loss improved from 2.42666 to 2.42642, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.4241 - acc: 0.1089 - val_loss: 2.4264 - val_acc: 0.1076
Epoch 16/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4253 - acc: 0.1087Epoch 00016: val_loss improved from 2.42642 to 2.42626, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4241 - acc: 0.1089 - val_loss: 2.4263 - val_acc: 0.1076
Epoch 17/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4244 - acc: 0.1098Epoch 00017: val_loss improved from 2.42626 to 2.42606, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4237 - acc: 0.1096 - val_loss: 2.4261 - val_acc: 0.1076
Epoch 18/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4237 - acc: 0.1100Epoch 00018: val_loss improved from 2.42606 to 2.42594, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4233 - acc: 0.1096 - val_loss: 2.4259 - val_acc: 0.1076
Epoch 19/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4244 - acc: 0.1091Epoch 00019: val_loss improved from 2.42594 to 2.42584, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4236 - acc: 0.1090 - val_loss: 2.4258 - val_acc: 0.1076
Epoch 20/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4239 - acc: 0.1086Epoch 00020: val_loss improved from 2.42584 to 2.42579, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4238 - acc: 0.1090 - val_loss: 2.4258 - val_acc: 0.1076
Epoch 21/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4233 - acc: 0.1089Epoch 00021: val_loss improved from 2.42579 to 2.42563, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.4232 - acc: 0.1085 - val_loss: 2.4256 - val_acc: 0.1076
Epoch 22/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4231 - acc: 0.1075Epoch 00022: val_loss improved from 2.42563 to 2.42545, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.4226 - acc: 0.1081 - val_loss: 2.4255 - val_acc: 0.1076
Epoch 23/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4225 - acc: 0.1095Epoch 00023: val_loss improved from 2.42545 to 2.42535, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.4228 - acc: 0.1093 - val_loss: 2.4254 - val_acc: 0.1082
Epoch 24/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4218 - acc: 0.1095Epoch 00024: val_loss improved from 2.42535 to 2.42531, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4227 - acc: 0.1094 - val_loss: 2.4253 - val_acc: 0.1082
Epoch 25/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4226 - acc: 0.1096Epoch 00025: val_loss improved from 2.42531 to 2.42520, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4228 - acc: 0.1092 - val_loss: 2.4252 - val_acc: 0.1082
Epoch 26/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4224 - acc: 0.1098Epoch 00026: val_loss improved from 2.42520 to 2.42500, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4219 - acc: 0.1095 - val_loss: 2.4250 - val_acc: 0.1082
Epoch 27/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4216 - acc: 0.1085Epoch 00027: val_loss improved from 2.42500 to 2.42495, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4220 - acc: 0.1078 - val_loss: 2.4250 - val_acc: 0.1082
Epoch 28/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4229 - acc: 0.1093Epoch 00028: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 2.4228 - acc: 0.1094 - val_loss: 2.4250 - val_acc: 0.1082
Epoch 29/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4231 - acc: 0.1086Epoch 00029: val_loss improved from 2.42495 to 2.42485, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.4224 - acc: 0.1090 - val_loss: 2.4249 - val_acc: 0.1082
Epoch 30/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4232 - acc: 0.1086Epoch 00030: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 2.4230 - acc: 0.1083 - val_loss: 2.4249 - val_acc: 0.1082
Epoch 31/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4219 - acc: 0.1095Epoch 00031: val_loss improved from 2.42485 to 2.42478, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.4220 - acc: 0.1099 - val_loss: 2.4248 - val_acc: 0.1082
Epoch 32/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4215 - acc: 0.1094Epoch 00032: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 2.4219 - acc: 0.1089 - val_loss: 2.4248 - val_acc: 0.1082
Epoch 33/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 2.4219 - acc: 0.1090Epoch 00033: val_loss improved from 2.42478 to 2.42464, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 96us/step - loss: 2.4223 - acc: 0.1095 - val_loss: 2.4246 - val_acc: 0.1082
Epoch 34/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4221 - acc: 0.1097Epoch 00034: val_loss improved from 2.42464 to 2.42461, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4225 - acc: 0.1094 - val_loss: 2.4246 - val_acc: 0.1082
Epoch 35/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4220 - acc: 0.1094Epoch 00035: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4219 - acc: 0.1091 - val_loss: 2.4246 - val_acc: 0.1082
Epoch 36/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4225 - acc: 0.1080Epoch 00036: val_loss did not improve
17896/17896 [==============================] - 2s 94us/step - loss: 2.4221 - acc: 0.1079 - val_loss: 2.4247 - val_acc: 0.1082
Epoch 37/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4207 - acc: 0.1096Epoch 00037: val_loss improved from 2.42461 to 2.42454, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.4214 - acc: 0.1095 - val_loss: 2.4245 - val_acc: 0.1082
Epoch 38/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4211 - acc: 0.1090Epoch 00038: val_loss improved from 2.42454 to 2.42450, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 2.4217 - acc: 0.1086 - val_loss: 2.4245 - val_acc: 0.1082
Epoch 39/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4217 - acc: 0.1083Epoch 00039: val_loss improved from 2.42450 to 2.42444, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4216 - acc: 0.1083 - val_loss: 2.4244 - val_acc: 0.1082
Epoch 40/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4236 - acc: 0.1093Epoch 00040: val_loss did not improve
17896/17896 [==============================] - 2s 94us/step - loss: 2.4218 - acc: 0.1096 - val_loss: 2.4245 - val_acc: 0.1082
Epoch 41/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4224 - acc: 0.1096Epoch 00041: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4219 - acc: 0.1096 - val_loss: 2.4244 - val_acc: 0.1082
Epoch 42/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4219 - acc: 0.1090Epoch 00042: val_loss improved from 2.42444 to 2.42442, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 2.4218 - acc: 0.1092 - val_loss: 2.4244 - val_acc: 0.1082
Epoch 43/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4226 - acc: 0.1081Epoch 00043: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 2.4219 - acc: 0.1077 - val_loss: 2.4244 - val_acc: 0.1082
Epoch 44/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4216 - acc: 0.1089Epoch 00044: val_loss did not improve
17896/17896 [==============================] - 2s 94us/step - loss: 2.4221 - acc: 0.1089 - val_loss: 2.4244 - val_acc: 0.1082
Epoch 45/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4229 - acc: 0.1086Epoch 00045: val_loss improved from 2.42442 to 2.42438, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.4219 - acc: 0.1096 - val_loss: 2.4244 - val_acc: 0.1082
Epoch 46/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4228 - acc: 0.1090Epoch 00046: val_loss improved from 2.42438 to 2.42434, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.4221 - acc: 0.1094 - val_loss: 2.4243 - val_acc: 0.1082
Epoch 47/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4223 - acc: 0.1091Epoch 00047: val_loss improved from 2.42434 to 2.42431, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.4215 - acc: 0.1095 - val_loss: 2.4243 - val_acc: 0.1082
Epoch 48/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4211 - acc: 0.1097Epoch 00048: val_loss improved from 2.42431 to 2.42430, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 2.4216 - acc: 0.1097 - val_loss: 2.4243 - val_acc: 0.1082
Epoch 49/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4221 - acc: 0.1078Epoch 00049: val_loss improved from 2.42430 to 2.42428, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 2.4217 - acc: 0.1088 - val_loss: 2.4243 - val_acc: 0.1082
Epoch 50/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4232 - acc: 0.1093Epoch 00050: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4218 - acc: 0.1095 - val_loss: 2.4243 - val_acc: 0.1082
Epoch 51/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4218 - acc: 0.1082Epoch 00051: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4219 - acc: 0.1087 - val_loss: 2.4243 - val_acc: 0.1082
Epoch 52/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 2.4217 - acc: 0.1080Epoch 00052: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 2.4221 - acc: 0.1082 - val_loss: 2.4243 - val_acc: 0.1082
Epoch 53/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4216 - acc: 0.1093Epoch 00053: val_loss improved from 2.42428 to 2.42426, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.4216 - acc: 0.1094 - val_loss: 2.4243 - val_acc: 0.1082
Epoch 54/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4219 - acc: 0.1083Epoch 00054: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4216 - acc: 0.1085 - val_loss: 2.4243 - val_acc: 0.1082
Epoch 55/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4223 - acc: 0.1090Epoch 00055: val_loss improved from 2.42426 to 2.42424, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 2.4218 - acc: 0.1091 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 56/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4218 - acc: 0.1089Epoch 00056: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4223 - acc: 0.1094 - val_loss: 2.4243 - val_acc: 0.1082
Epoch 57/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4215 - acc: 0.1079Epoch 00057: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4214 - acc: 0.1078 - val_loss: 2.4243 - val_acc: 0.1082
Epoch 58/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4218 - acc: 0.1090Epoch 00058: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4211 - acc: 0.1093 - val_loss: 2.4243 - val_acc: 0.1082
Epoch 59/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4207 - acc: 0.1101Epoch 00059: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 2.4216 - acc: 0.1094 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 60/250
17408/17896 [============================&gt;.] - ETA: 0s - loss: 2.4206 - acc: 0.1102Epoch 00060: val_loss improved from 2.42424 to 2.42424, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 2.4212 - acc: 0.1099 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 61/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4210 - acc: 0.1091Epoch 00061: val_loss improved from 2.42424 to 2.42423, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.4213 - acc: 0.1085 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 62/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4224 - acc: 0.1093Epoch 00062: val_loss improved from 2.42423 to 2.42422, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4217 - acc: 0.1095 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 63/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4230 - acc: 0.1094Epoch 00063: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4218 - acc: 0.1100 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 64/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4213 - acc: 0.1101Epoch 00064: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4216 - acc: 0.1095 - val_loss: 2.4243 - val_acc: 0.1082
Epoch 65/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4230 - acc: 0.1082Epoch 00065: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 2.4213 - acc: 0.1089 - val_loss: 2.4243 - val_acc: 0.1082
Epoch 66/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4215 - acc: 0.1064Epoch 00066: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 2.4218 - acc: 0.1080 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 67/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4210 - acc: 0.1091Epoch 00067: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4213 - acc: 0.1095 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 68/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4220 - acc: 0.1095Epoch 00068: val_loss improved from 2.42422 to 2.42421, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.4217 - acc: 0.1095 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 69/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4224 - acc: 0.1069Epoch 00069: val_loss improved from 2.42421 to 2.42421, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4218 - acc: 0.1077 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 70/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4206 - acc: 0.1114Epoch 00070: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 2.4218 - acc: 0.1099 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 71/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4215 - acc: 0.1069Epoch 00071: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4216 - acc: 0.1073 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 72/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 2.4215 - acc: 0.1096Epoch 00072: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 2.4217 - acc: 0.1095 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 73/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4217 - acc: 0.1086Epoch 00073: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 2.4214 - acc: 0.1094 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 74/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4217 - acc: 0.1100Epoch 00074: val_loss improved from 2.42421 to 2.42420, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 2.4215 - acc: 0.1095 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 75/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4218 - acc: 0.1072Epoch 00075: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4216 - acc: 0.1079 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 76/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 2.4223 - acc: 0.1096Epoch 00076: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 2.4217 - acc: 0.1097 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 77/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4211 - acc: 0.1089Epoch 00077: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 2.4214 - acc: 0.1097 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 78/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 2.4210 - acc: 0.1095Epoch 00078: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 2.4212 - acc: 0.1094 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 79/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4203 - acc: 0.1107Epoch 00079: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4213 - acc: 0.1100 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 80/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4213 - acc: 0.1095Epoch 00080: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4223 - acc: 0.1091 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 81/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4208 - acc: 0.1102Epoch 00081: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4212 - acc: 0.1094 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 82/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4216 - acc: 0.1083Epoch 00082: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 2.4213 - acc: 0.1096 - val_loss: 2.4242 - val_acc: 0.1082
Epoch 83/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4200 - acc: 0.1126Epoch 00083: val_loss improved from 2.42420 to 2.42392, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.4208 - acc: 0.1112 - val_loss: 2.4239 - val_acc: 0.1087
Epoch 84/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4200 - acc: 0.1147Epoch 00084: val_loss improved from 2.42392 to 2.42226, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 96us/step - loss: 2.4195 - acc: 0.1149 - val_loss: 2.4223 - val_acc: 0.1104
Epoch 85/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.4102 - acc: 0.1200Epoch 00085: val_loss improved from 2.42226 to 2.40051, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 2.4102 - acc: 0.1199 - val_loss: 2.4005 - val_acc: 0.1267
Epoch 86/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.3990 - acc: 0.1287Epoch 00086: val_loss improved from 2.40051 to 2.35370, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 2.3991 - acc: 0.1287 - val_loss: 2.3537 - val_acc: 0.1541
Epoch 87/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.3702 - acc: 0.1446Epoch 00087: val_loss improved from 2.35370 to 2.32513, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.3690 - acc: 0.1447 - val_loss: 2.3251 - val_acc: 0.1676
Epoch 88/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.3330 - acc: 0.1607Epoch 00088: val_loss improved from 2.32513 to 2.31268, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 2.3320 - acc: 0.1608 - val_loss: 2.3127 - val_acc: 0.1900
Epoch 89/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.2714 - acc: 0.1869Epoch 00089: val_loss improved from 2.31268 to 2.14237, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 96us/step - loss: 2.2700 - acc: 0.1873 - val_loss: 2.1424 - val_acc: 0.2556
Epoch 90/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 2.1657 - acc: 0.2341Epoch 00090: val_loss improved from 2.14237 to 2.02535, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 96us/step - loss: 2.1657 - acc: 0.2341 - val_loss: 2.0253 - val_acc: 0.3156
Epoch 91/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 2.0185 - acc: 0.2797Epoch 00091: val_loss improved from 2.02535 to 1.79426, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 96us/step - loss: 2.0175 - acc: 0.2805 - val_loss: 1.7943 - val_acc: 0.3862
Epoch 92/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 1.8895 - acc: 0.3247Epoch 00092: val_loss improved from 1.79426 to 1.63835, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 1.8884 - acc: 0.3262 - val_loss: 1.6383 - val_acc: 0.4557
Epoch 93/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 1.7741 - acc: 0.3587Epoch 00093: val_loss improved from 1.63835 to 1.47595, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 1.7708 - acc: 0.3613 - val_loss: 1.4759 - val_acc: 0.4899
Epoch 94/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 1.6511 - acc: 0.4162Epoch 00094: val_loss improved from 1.47595 to 1.34373, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 1.6483 - acc: 0.4162 - val_loss: 1.3437 - val_acc: 0.5566
Epoch 95/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 1.5306 - acc: 0.4567Epoch 00095: val_loss improved from 1.34373 to 1.22010, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 96us/step - loss: 1.5288 - acc: 0.4575 - val_loss: 1.2201 - val_acc: 0.5914
Epoch 96/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 1.4211 - acc: 0.5001Epoch 00096: val_loss improved from 1.22010 to 1.11108, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 96us/step - loss: 1.4215 - acc: 0.5001 - val_loss: 1.1111 - val_acc: 0.6345
Epoch 97/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 1.3352 - acc: 0.5302Epoch 00097: val_loss improved from 1.11108 to 1.01667, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 1.3341 - acc: 0.5308 - val_loss: 1.0167 - val_acc: 0.6732
Epoch 98/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 1.2525 - acc: 0.5585Epoch 00098: val_loss improved from 1.01667 to 0.94994, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 1.2500 - acc: 0.5586 - val_loss: 0.9499 - val_acc: 0.6962
Epoch 99/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 1.1817 - acc: 0.5871Epoch 00099: val_loss improved from 0.94994 to 0.88260, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 1.1806 - acc: 0.5880 - val_loss: 0.8826 - val_acc: 0.7152
Epoch 100/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 1.1285 - acc: 0.6067Epoch 00100: val_loss improved from 0.88260 to 0.83236, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 1.1242 - acc: 0.6078 - val_loss: 0.8324 - val_acc: 0.7360
Epoch 101/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 1.0794 - acc: 0.6258Epoch 00101: val_loss improved from 0.83236 to 0.80748, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 1.0768 - acc: 0.6271 - val_loss: 0.8075 - val_acc: 0.7365
Epoch 102/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 1.0230 - acc: 0.6437Epoch 00102: val_loss improved from 0.80748 to 0.76118, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 96us/step - loss: 1.0236 - acc: 0.6437 - val_loss: 0.7612 - val_acc: 0.7539
Epoch 103/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.9846 - acc: 0.6568Epoch 00103: val_loss improved from 0.76118 to 0.74475, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 96us/step - loss: 0.9842 - acc: 0.6566 - val_loss: 0.7447 - val_acc: 0.7578
Epoch 104/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.9638 - acc: 0.6713Epoch 00104: val_loss improved from 0.74475 to 0.70905, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.9627 - acc: 0.6718 - val_loss: 0.7090 - val_acc: 0.7758
Epoch 105/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.9204 - acc: 0.6836Epoch 00105: val_loss improved from 0.70905 to 0.70476, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 0.9231 - acc: 0.6822 - val_loss: 0.7048 - val_acc: 0.7741
Epoch 106/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.8945 - acc: 0.6905Epoch 00106: val_loss improved from 0.70476 to 0.65329, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.8924 - acc: 0.6909 - val_loss: 0.6533 - val_acc: 0.7831
Epoch 107/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.8684 - acc: 0.7022Epoch 00107: val_loss improved from 0.65329 to 0.65109, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 0.8688 - acc: 0.7025 - val_loss: 0.6511 - val_acc: 0.7904
Epoch 108/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.8556 - acc: 0.7073Epoch 00108: val_loss improved from 0.65109 to 0.63005, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 96us/step - loss: 0.8525 - acc: 0.7085 - val_loss: 0.6300 - val_acc: 0.7853
Epoch 109/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.8215 - acc: 0.7189Epoch 00109: val_loss improved from 0.63005 to 0.60536, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 0.8215 - acc: 0.7189 - val_loss: 0.6054 - val_acc: 0.8021
Epoch 110/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.8130 - acc: 0.7233Epoch 00110: val_loss improved from 0.60536 to 0.59423, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.8109 - acc: 0.7240 - val_loss: 0.5942 - val_acc: 0.8083
Epoch 111/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.7867 - acc: 0.7338Epoch 00111: val_loss improved from 0.59423 to 0.57830, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 0.7852 - acc: 0.7342 - val_loss: 0.5783 - val_acc: 0.8100
Epoch 112/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.7784 - acc: 0.7304Epoch 00112: val_loss improved from 0.57830 to 0.57349, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 0.7767 - acc: 0.7309 - val_loss: 0.5735 - val_acc: 0.8089
Epoch 113/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.7421 - acc: 0.7459Epoch 00113: val_loss improved from 0.57349 to 0.55773, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.7422 - acc: 0.7461 - val_loss: 0.5577 - val_acc: 0.8145
Epoch 114/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.7423 - acc: 0.7505Epoch 00114: val_loss improved from 0.55773 to 0.55638, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.7418 - acc: 0.7501 - val_loss: 0.5564 - val_acc: 0.8212
Epoch 115/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.7232 - acc: 0.7526Epoch 00115: val_loss improved from 0.55638 to 0.53332, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 0.7213 - acc: 0.7532 - val_loss: 0.5333 - val_acc: 0.8246
Epoch 116/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.7118 - acc: 0.7572Epoch 00116: val_loss improved from 0.53332 to 0.53006, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.7125 - acc: 0.7564 - val_loss: 0.5301 - val_acc: 0.8229
Epoch 117/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.6911 - acc: 0.7650Epoch 00117: val_loss improved from 0.53006 to 0.50546, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 0.6920 - acc: 0.7645 - val_loss: 0.5055 - val_acc: 0.8313
Epoch 118/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.6888 - acc: 0.7647Epoch 00118: val_loss improved from 0.50546 to 0.50151, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 0.6879 - acc: 0.7650 - val_loss: 0.5015 - val_acc: 0.8380
Epoch 119/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.6739 - acc: 0.7715Epoch 00119: val_loss improved from 0.50151 to 0.48656, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.6735 - acc: 0.7717 - val_loss: 0.4866 - val_acc: 0.8419
Epoch 120/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.6689 - acc: 0.7743Epoch 00120: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.6682 - acc: 0.7745 - val_loss: 0.4874 - val_acc: 0.8414
Epoch 121/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.6447 - acc: 0.7807Epoch 00121: val_loss improved from 0.48656 to 0.46767, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.6464 - acc: 0.7798 - val_loss: 0.4677 - val_acc: 0.8492
Epoch 122/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.6468 - acc: 0.7825Epoch 00122: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 0.6466 - acc: 0.7829 - val_loss: 0.4733 - val_acc: 0.8447
Epoch 123/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.6294 - acc: 0.7850Epoch 00123: val_loss improved from 0.46767 to 0.45631, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.6259 - acc: 0.7861 - val_loss: 0.4563 - val_acc: 0.8543
Epoch 124/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.6103 - acc: 0.7931Epoch 00124: val_loss improved from 0.45631 to 0.45185, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.6118 - acc: 0.7924 - val_loss: 0.4519 - val_acc: 0.8509
Epoch 125/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.6072 - acc: 0.7928Epoch 00125: val_loss improved from 0.45185 to 0.44993, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 0.6092 - acc: 0.7919 - val_loss: 0.4499 - val_acc: 0.8582
Epoch 126/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.6144 - acc: 0.7955Epoch 00126: val_loss improved from 0.44993 to 0.43477, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 0.6143 - acc: 0.7957 - val_loss: 0.4348 - val_acc: 0.8615
Epoch 127/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.5914 - acc: 0.8010Epoch 00127: val_loss did not improve
17896/17896 [==============================] - 2s 94us/step - loss: 0.5915 - acc: 0.8013 - val_loss: 0.4354 - val_acc: 0.8604
Epoch 128/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.5921 - acc: 0.7972Epoch 00128: val_loss improved from 0.43477 to 0.42370, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 0.5924 - acc: 0.7966 - val_loss: 0.4237 - val_acc: 0.8660
Epoch 129/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.5778 - acc: 0.8066Epoch 00129: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.5754 - acc: 0.8073 - val_loss: 0.4245 - val_acc: 0.8621
Epoch 130/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.5763 - acc: 0.8038Epoch 00130: val_loss improved from 0.42370 to 0.41045, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.5746 - acc: 0.8044 - val_loss: 0.4104 - val_acc: 0.8733
Epoch 131/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.5558 - acc: 0.8110Epoch 00131: val_loss improved from 0.41045 to 0.40981, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.5570 - acc: 0.8105 - val_loss: 0.4098 - val_acc: 0.8688
Epoch 132/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.5548 - acc: 0.8109Epoch 00132: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 0.5559 - acc: 0.8105 - val_loss: 0.4129 - val_acc: 0.8683
Epoch 133/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.5495 - acc: 0.8145Epoch 00133: val_loss improved from 0.40981 to 0.40208, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 0.5494 - acc: 0.8140 - val_loss: 0.4021 - val_acc: 0.8688
Epoch 134/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.5470 - acc: 0.8174Epoch 00134: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.5484 - acc: 0.8182 - val_loss: 0.4097 - val_acc: 0.8722
Epoch 135/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.5391 - acc: 0.8154Epoch 00135: val_loss improved from 0.40208 to 0.38953, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 0.5366 - acc: 0.8157 - val_loss: 0.3895 - val_acc: 0.8772
Epoch 136/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.5268 - acc: 0.8218Epoch 00136: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.5275 - acc: 0.8220 - val_loss: 0.3967 - val_acc: 0.8700
Epoch 137/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 0.5202 - acc: 0.8238Epoch 00137: val_loss improved from 0.38953 to 0.38561, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.5200 - acc: 0.8241 - val_loss: 0.3856 - val_acc: 0.8778
Epoch 138/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 0.5118 - acc: 0.8285Epoch 00138: val_loss improved from 0.38561 to 0.38502, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 0.5119 - acc: 0.8284 - val_loss: 0.3850 - val_acc: 0.8756
Epoch 139/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.5108 - acc: 0.8277Epoch 00139: val_loss did not improve
17896/17896 [==============================] - 2s 94us/step - loss: 0.5144 - acc: 0.8269 - val_loss: 0.3901 - val_acc: 0.8744
Epoch 140/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.5085 - acc: 0.8303Epoch 00140: val_loss improved from 0.38502 to 0.37980, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.5063 - acc: 0.8309 - val_loss: 0.3798 - val_acc: 0.8767
Epoch 141/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.5002 - acc: 0.8312Epoch 00141: val_loss improved from 0.37980 to 0.37078, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.5008 - acc: 0.8310 - val_loss: 0.3708 - val_acc: 0.8834
Epoch 142/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.5011 - acc: 0.8305Epoch 00142: val_loss improved from 0.37078 to 0.36531, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.5007 - acc: 0.8303 - val_loss: 0.3653 - val_acc: 0.8901
Epoch 143/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 0.4980 - acc: 0.8319Epoch 00143: val_loss did not improve
17896/17896 [==============================] - 2s 94us/step - loss: 0.4983 - acc: 0.8320 - val_loss: 0.3718 - val_acc: 0.8795
Epoch 144/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4890 - acc: 0.8359Epoch 00144: val_loss improved from 0.36531 to 0.36021, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.4904 - acc: 0.8359 - val_loss: 0.3602 - val_acc: 0.8834
Epoch 145/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4850 - acc: 0.8348Epoch 00145: val_loss improved from 0.36021 to 0.35976, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.4841 - acc: 0.8350 - val_loss: 0.3598 - val_acc: 0.8806
Epoch 146/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4768 - acc: 0.8370Epoch 00146: val_loss improved from 0.35976 to 0.35849, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.4762 - acc: 0.8371 - val_loss: 0.3585 - val_acc: 0.8784
Epoch 147/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4724 - acc: 0.8396Epoch 00147: val_loss improved from 0.35849 to 0.35420, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.4731 - acc: 0.8391 - val_loss: 0.3542 - val_acc: 0.8812
Epoch 148/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4732 - acc: 0.8394Epoch 00148: val_loss improved from 0.35420 to 0.34830, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.4737 - acc: 0.8397 - val_loss: 0.3483 - val_acc: 0.8896
Epoch 149/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4547 - acc: 0.8445Epoch 00149: val_loss improved from 0.34830 to 0.34343, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 0.4557 - acc: 0.8445 - val_loss: 0.3434 - val_acc: 0.8885
Epoch 150/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4493 - acc: 0.8478Epoch 00150: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 0.4478 - acc: 0.8485 - val_loss: 0.3449 - val_acc: 0.8845
Epoch 151/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4638 - acc: 0.8407Epoch 00151: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.4632 - acc: 0.8409 - val_loss: 0.3446 - val_acc: 0.8907
Epoch 152/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4536 - acc: 0.8479Epoch 00152: val_loss improved from 0.34343 to 0.34194, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.4543 - acc: 0.8477 - val_loss: 0.3419 - val_acc: 0.8857
Epoch 153/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4542 - acc: 0.8473Epoch 00153: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 0.4552 - acc: 0.8469 - val_loss: 0.3441 - val_acc: 0.8868
Epoch 154/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4450 - acc: 0.8520Epoch 00154: val_loss improved from 0.34194 to 0.33436, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.4456 - acc: 0.8520 - val_loss: 0.3344 - val_acc: 0.8918
Epoch 155/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4438 - acc: 0.8495Epoch 00155: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.4467 - acc: 0.8492 - val_loss: 0.3380 - val_acc: 0.8868
Epoch 156/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4452 - acc: 0.8513Epoch 00156: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.4442 - acc: 0.8513 - val_loss: 0.3365 - val_acc: 0.8901
Epoch 157/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4265 - acc: 0.8566Epoch 00157: val_loss improved from 0.33436 to 0.32187, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.4266 - acc: 0.8567 - val_loss: 0.3219 - val_acc: 0.8963
Epoch 158/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4298 - acc: 0.8540Epoch 00158: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 0.4305 - acc: 0.8542 - val_loss: 0.3240 - val_acc: 0.8941
Epoch 159/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4212 - acc: 0.8534Epoch 00159: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.4213 - acc: 0.8532 - val_loss: 0.3346 - val_acc: 0.8952
Epoch 160/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4272 - acc: 0.8572Epoch 00160: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 0.4279 - acc: 0.8563 - val_loss: 0.3396 - val_acc: 0.8918
Epoch 161/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4202 - acc: 0.8584Epoch 00161: val_loss did not improve
17896/17896 [==============================] - 2s 94us/step - loss: 0.4232 - acc: 0.8572 - val_loss: 0.3255 - val_acc: 0.8935
Epoch 162/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4132 - acc: 0.8601Epoch 00162: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.4153 - acc: 0.8599 - val_loss: 0.3350 - val_acc: 0.8901
Epoch 163/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 0.4167 - acc: 0.8562Epoch 00163: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 0.4165 - acc: 0.8558 - val_loss: 0.3320 - val_acc: 0.8907
Epoch 164/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 0.4120 - acc: 0.8600Epoch 00164: val_loss improved from 0.32187 to 0.32145, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 0.4125 - acc: 0.8600 - val_loss: 0.3214 - val_acc: 0.8974
Epoch 165/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4102 - acc: 0.8625Epoch 00165: val_loss improved from 0.32145 to 0.30801, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.4087 - acc: 0.8629 - val_loss: 0.3080 - val_acc: 0.8974
Epoch 166/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 0.4161 - acc: 0.8605Epoch 00166: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 0.4162 - acc: 0.8607 - val_loss: 0.3196 - val_acc: 0.8946
Epoch 167/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4113 - acc: 0.8596Epoch 00167: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 0.4092 - acc: 0.8606 - val_loss: 0.3198 - val_acc: 0.8918
Epoch 168/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.4026 - acc: 0.8630Epoch 00168: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 0.4028 - acc: 0.8632 - val_loss: 0.3128 - val_acc: 0.8963
Epoch 169/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3975 - acc: 0.8649Epoch 00169: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 0.3985 - acc: 0.8647 - val_loss: 0.3134 - val_acc: 0.8991
Epoch 170/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3959 - acc: 0.8671Epoch 00170: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 0.3956 - acc: 0.8675 - val_loss: 0.3163 - val_acc: 0.8952
Epoch 171/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3911 - acc: 0.8681Epoch 00171: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 0.3891 - acc: 0.8686 - val_loss: 0.3087 - val_acc: 0.8946
Epoch 172/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3904 - acc: 0.8662Epoch 00172: val_loss improved from 0.30801 to 0.30293, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 95us/step - loss: 0.3924 - acc: 0.8658 - val_loss: 0.3029 - val_acc: 0.9030
Epoch 173/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3870 - acc: 0.8695Epoch 00173: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.3885 - acc: 0.8689 - val_loss: 0.3054 - val_acc: 0.9025
Epoch 174/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3849 - acc: 0.8717Epoch 00174: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3879 - acc: 0.8704 - val_loss: 0.3252 - val_acc: 0.8974
Epoch 175/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3813 - acc: 0.8696Epoch 00175: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3807 - acc: 0.8696 - val_loss: 0.3046 - val_acc: 0.9019
Epoch 176/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3673 - acc: 0.8782Epoch 00176: val_loss improved from 0.30293 to 0.29886, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 0.3677 - acc: 0.8780 - val_loss: 0.2989 - val_acc: 0.9013
Epoch 177/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3758 - acc: 0.8724Epoch 00177: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3752 - acc: 0.8726 - val_loss: 0.3121 - val_acc: 0.8980
Epoch 178/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3673 - acc: 0.8742Epoch 00178: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 0.3665 - acc: 0.8747 - val_loss: 0.3103 - val_acc: 0.8963
Epoch 179/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 0.3797 - acc: 0.8739Epoch 00179: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 0.3795 - acc: 0.8735 - val_loss: 0.3062 - val_acc: 0.8974
Epoch 180/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 0.3698 - acc: 0.8759Epoch 00180: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.3717 - acc: 0.8753 - val_loss: 0.3066 - val_acc: 0.8935
Epoch 181/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3654 - acc: 0.8785Epoch 00181: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3652 - acc: 0.8782 - val_loss: 0.3025 - val_acc: 0.9002
Epoch 182/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3659 - acc: 0.8792Epoch 00182: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3662 - acc: 0.8789 - val_loss: 0.3075 - val_acc: 0.8985
Epoch 183/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3574 - acc: 0.8805Epoch 00183: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.3573 - acc: 0.8802 - val_loss: 0.2995 - val_acc: 0.9019
Epoch 184/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3571 - acc: 0.8786Epoch 00184: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.3552 - acc: 0.8797 - val_loss: 0.3041 - val_acc: 0.9013
Epoch 185/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3568 - acc: 0.8798Epoch 00185: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3576 - acc: 0.8789 - val_loss: 0.3097 - val_acc: 0.8985
Epoch 186/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3550 - acc: 0.8803Epoch 00186: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3531 - acc: 0.8808 - val_loss: 0.3005 - val_acc: 0.9047
Epoch 187/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3580 - acc: 0.8782Epoch 00187: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3579 - acc: 0.8782 - val_loss: 0.3016 - val_acc: 0.9036
Epoch 188/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3539 - acc: 0.8800Epoch 00188: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3526 - acc: 0.8805 - val_loss: 0.3040 - val_acc: 0.8997
Epoch 189/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3484 - acc: 0.8846Epoch 00189: val_loss improved from 0.29886 to 0.29793, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 0.3499 - acc: 0.8838 - val_loss: 0.2979 - val_acc: 0.9036
Epoch 190/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3418 - acc: 0.8843Epoch 00190: val_loss improved from 0.29793 to 0.29273, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 92us/step - loss: 0.3431 - acc: 0.8844 - val_loss: 0.2927 - val_acc: 0.9047
Epoch 191/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3492 - acc: 0.8837Epoch 00191: val_loss improved from 0.29273 to 0.28813, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 92us/step - loss: 0.3480 - acc: 0.8843 - val_loss: 0.2881 - val_acc: 0.9030
Epoch 192/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3470 - acc: 0.8826Epoch 00192: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.3450 - acc: 0.8835 - val_loss: 0.3028 - val_acc: 0.9019
Epoch 193/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3464 - acc: 0.8830Epoch 00193: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3458 - acc: 0.8832 - val_loss: 0.3070 - val_acc: 0.9019
Epoch 194/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3419 - acc: 0.8839Epoch 00194: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3423 - acc: 0.8836 - val_loss: 0.2969 - val_acc: 0.9013
Epoch 195/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3377 - acc: 0.8864Epoch 00195: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3385 - acc: 0.8862 - val_loss: 0.3011 - val_acc: 0.9019
Epoch 196/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3359 - acc: 0.8844Epoch 00196: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3373 - acc: 0.8839 - val_loss: 0.3032 - val_acc: 0.9041
Epoch 197/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3374 - acc: 0.8827Epoch 00197: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3383 - acc: 0.8825 - val_loss: 0.2903 - val_acc: 0.9058
Epoch 198/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3427 - acc: 0.8864Epoch 00198: val_loss improved from 0.28813 to 0.28463, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 0.3412 - acc: 0.8871 - val_loss: 0.2846 - val_acc: 0.9086
Epoch 199/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3266 - acc: 0.8882Epoch 00199: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3267 - acc: 0.8888 - val_loss: 0.2946 - val_acc: 0.9013
Epoch 200/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3319 - acc: 0.8872Epoch 00200: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3305 - acc: 0.8877 - val_loss: 0.2877 - val_acc: 0.9064
Epoch 201/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3301 - acc: 0.8879Epoch 00201: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3315 - acc: 0.8877 - val_loss: 0.2854 - val_acc: 0.9053
Epoch 202/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3225 - acc: 0.8868Epoch 00202: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3217 - acc: 0.8870 - val_loss: 0.2914 - val_acc: 0.9064
Epoch 203/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3200 - acc: 0.8909Epoch 00203: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.3185 - acc: 0.8914 - val_loss: 0.2869 - val_acc: 0.9092
Epoch 204/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3283 - acc: 0.8883Epoch 00204: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3299 - acc: 0.8886 - val_loss: 0.2957 - val_acc: 0.9081
Epoch 205/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3249 - acc: 0.8885Epoch 00205: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3251 - acc: 0.8889 - val_loss: 0.2861 - val_acc: 0.9092
Epoch 206/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3126 - acc: 0.8938Epoch 00206: val_loss improved from 0.28463 to 0.28408, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 0.3144 - acc: 0.8928 - val_loss: 0.2841 - val_acc: 0.9013
Epoch 207/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3239 - acc: 0.8917Epoch 00207: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.3240 - acc: 0.8917 - val_loss: 0.2896 - val_acc: 0.9075
Epoch 208/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3160 - acc: 0.8941Epoch 00208: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3139 - acc: 0.8947 - val_loss: 0.2913 - val_acc: 0.9036
Epoch 209/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3152 - acc: 0.8935Epoch 00209: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3182 - acc: 0.8925 - val_loss: 0.2861 - val_acc: 0.9070
Epoch 210/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3119 - acc: 0.8927Epoch 00210: val_loss did not improve
17896/17896 [==============================] - 2s 90us/step - loss: 0.3115 - acc: 0.8931 - val_loss: 0.2854 - val_acc: 0.9064
Epoch 211/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3087 - acc: 0.8967Epoch 00211: val_loss improved from 0.28408 to 0.28159, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 0.3096 - acc: 0.8960 - val_loss: 0.2816 - val_acc: 0.9058
Epoch 212/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3150 - acc: 0.8921Epoch 00212: val_loss improved from 0.28159 to 0.27804, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 0.3141 - acc: 0.8926 - val_loss: 0.2780 - val_acc: 0.9086
Epoch 213/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3112 - acc: 0.8963Epoch 00213: val_loss improved from 0.27804 to 0.27568, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 0.3109 - acc: 0.8966 - val_loss: 0.2757 - val_acc: 0.9103
Epoch 214/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3115 - acc: 0.8940Epoch 00214: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3104 - acc: 0.8937 - val_loss: 0.2797 - val_acc: 0.9103
Epoch 215/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3060 - acc: 0.8957Epoch 00215: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3063 - acc: 0.8957 - val_loss: 0.2871 - val_acc: 0.9098
Epoch 216/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3112 - acc: 0.8966Epoch 00216: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3132 - acc: 0.8957 - val_loss: 0.2810 - val_acc: 0.9098
Epoch 217/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3011 - acc: 0.8986Epoch 00217: val_loss improved from 0.27568 to 0.27553, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 94us/step - loss: 0.3022 - acc: 0.8984 - val_loss: 0.2755 - val_acc: 0.9081
Epoch 218/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3015 - acc: 0.8978Epoch 00218: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3020 - acc: 0.8975 - val_loss: 0.2803 - val_acc: 0.9109
Epoch 219/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3057 - acc: 0.8954Epoch 00219: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.3051 - acc: 0.8953 - val_loss: 0.2877 - val_acc: 0.9075
Epoch 220/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2983 - acc: 0.8975Epoch 00220: val_loss improved from 0.27553 to 0.27336, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 0.2985 - acc: 0.8971 - val_loss: 0.2734 - val_acc: 0.9137
Epoch 221/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2941 - acc: 0.8997Epoch 00221: val_loss improved from 0.27336 to 0.27065, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 0.2921 - acc: 0.9006 - val_loss: 0.2706 - val_acc: 0.9142
Epoch 222/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2940 - acc: 0.9029Epoch 00222: val_loss did not improve
17896/17896 [==============================] - 2s 90us/step - loss: 0.2948 - acc: 0.9027 - val_loss: 0.2781 - val_acc: 0.9114
Epoch 223/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2971 - acc: 0.8980Epoch 00223: val_loss did not improve
17896/17896 [==============================] - 2s 90us/step - loss: 0.2978 - acc: 0.8979 - val_loss: 0.2769 - val_acc: 0.9114
Epoch 224/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2989 - acc: 0.8979Epoch 00224: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.2965 - acc: 0.8989 - val_loss: 0.2837 - val_acc: 0.9081
Epoch 225/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2962 - acc: 0.8998Epoch 00225: val_loss did not improve
17896/17896 [==============================] - 2s 90us/step - loss: 0.2973 - acc: 0.8990 - val_loss: 0.2728 - val_acc: 0.9098
Epoch 226/250
17664/17896 [============================&gt;.] - ETA: 0s - loss: 0.3008 - acc: 0.8983Epoch 00226: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3010 - acc: 0.8981 - val_loss: 0.2747 - val_acc: 0.9114
Epoch 227/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.3007 - acc: 0.8981Epoch 00227: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.3003 - acc: 0.8983 - val_loss: 0.2787 - val_acc: 0.9109
Epoch 228/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2992 - acc: 0.9005Epoch 00228: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.2997 - acc: 0.8999 - val_loss: 0.2800 - val_acc: 0.9064
Epoch 229/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2920 - acc: 0.8991Epoch 00229: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.2919 - acc: 0.8990 - val_loss: 0.2849 - val_acc: 0.9075
Epoch 230/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2939 - acc: 0.8982Epoch 00230: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.2952 - acc: 0.8980 - val_loss: 0.2764 - val_acc: 0.9064
Epoch 231/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2839 - acc: 0.9045Epoch 00231: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.2853 - acc: 0.9040 - val_loss: 0.2832 - val_acc: 0.9041
Epoch 232/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2853 - acc: 0.9020Epoch 00232: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.2844 - acc: 0.9027 - val_loss: 0.2762 - val_acc: 0.9126
Epoch 233/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2822 - acc: 0.9027Epoch 00233: val_loss did not improve
17896/17896 [==============================] - 2s 93us/step - loss: 0.2815 - acc: 0.9027 - val_loss: 0.2763 - val_acc: 0.9098
Epoch 234/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2872 - acc: 0.9003Epoch 00234: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.2874 - acc: 0.9003 - val_loss: 0.2844 - val_acc: 0.9064
Epoch 235/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2762 - acc: 0.9075Epoch 00235: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.2790 - acc: 0.9063 - val_loss: 0.2793 - val_acc: 0.9103
Epoch 236/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2803 - acc: 0.9030Epoch 00236: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.2812 - acc: 0.9028 - val_loss: 0.2744 - val_acc: 0.9126
Epoch 237/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2795 - acc: 0.9055Epoch 00237: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.2800 - acc: 0.9051 - val_loss: 0.2777 - val_acc: 0.9109
Epoch 238/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2729 - acc: 0.9077Epoch 00238: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.2743 - acc: 0.9071 - val_loss: 0.2770 - val_acc: 0.9103
Epoch 239/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2857 - acc: 0.9018Epoch 00239: val_loss improved from 0.27065 to 0.27019, saving model to weights11.hdf5
17896/17896 [==============================] - 2s 93us/step - loss: 0.2855 - acc: 0.9017 - val_loss: 0.2702 - val_acc: 0.9137
Epoch 240/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2728 - acc: 0.9067Epoch 00240: val_loss did not improve
17896/17896 [==============================] - 2s 90us/step - loss: 0.2716 - acc: 0.9072 - val_loss: 0.2838 - val_acc: 0.9103
Epoch 241/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2844 - acc: 0.9032Epoch 00241: val_loss did not improve
17896/17896 [==============================] - 2s 89us/step - loss: 0.2834 - acc: 0.9034 - val_loss: 0.2754 - val_acc: 0.9126
Epoch 242/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2723 - acc: 0.9071Epoch 00242: val_loss did not improve
17896/17896 [==============================] - 2s 90us/step - loss: 0.2729 - acc: 0.9071 - val_loss: 0.2766 - val_acc: 0.9109
Epoch 243/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2786 - acc: 0.9057- ETA: 1s - loss: 0.2Epoch 00243: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.2785 - acc: 0.9057 - val_loss: 0.2711 - val_acc: 0.9142
Epoch 244/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2726 - acc: 0.9083Epoch 00244: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.2718 - acc: 0.9083 - val_loss: 0.2722 - val_acc: 0.9131
Epoch 245/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2768 - acc: 0.9056Epoch 00245: val_loss did not improve
17896/17896 [==============================] - 2s 92us/step - loss: 0.2779 - acc: 0.9052 - val_loss: 0.2728 - val_acc: 0.9159
Epoch 246/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2763 - acc: 0.9039Epoch 00246: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.2757 - acc: 0.9040 - val_loss: 0.2711 - val_acc: 0.9109
Epoch 247/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2661 - acc: 0.9069Epoch 00247: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.2662 - acc: 0.9067 - val_loss: 0.2736 - val_acc: 0.9120
Epoch 248/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2645 - acc: 0.9102Epoch 00248: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.2655 - acc: 0.9098 - val_loss: 0.2739 - val_acc: 0.9137
Epoch 249/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2735 - acc: 0.9069Epoch 00249: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.2723 - acc: 0.9069 - val_loss: 0.2779 - val_acc: 0.9081
Epoch 250/250
17152/17896 [===========================&gt;..] - ETA: 0s - loss: 0.2679 - acc: 0.9062Epoch 00250: val_loss did not improve
17896/17896 [==============================] - 2s 91us/step - loss: 0.2662 - acc: 0.9069 - val_loss: 0.2735 - val_acc: 0.9098
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Test-Model-using-kfold">Test Model using kfold<a class="anchor-link" href="#Test-Model-using-kfold">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[25]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">cvscores</span> <span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">12</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">&#39;weights&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;.hdf5&#39;</span><span class="p">)</span>
    <span class="n">y_predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">example</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span> <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">20</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">1</span><span class="p">)]</span>
    <span class="n">accuracy</span> <span class="o">=</span><span class="mi">100</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_predictions</span><span class="p">)</span><span class="o">==</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y_predictions</span><span class="p">)</span>
    <span class="n">cvscores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;test accurac of &#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39; : </span><span class="si">%.4f%%</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">accuracy</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;</span><span class="si">%.2f%%</span><span class="s2"> (+/-  </span><span class="si">%.2f%%</span><span class="s2">)&quot;</span> <span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cvscores</span><span class="p">)</span>  <span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">cvscores</span><span class="p">))</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>test accurac of 1 : 92.3889%
test accurac of 2 : 92.4718%
test accurac of 3 : 92.3533%
test accurac of 4 : 92.7801%
test accurac of 5 : 92.7801%
test accurac of 6 : 92.7564%
test accurac of 7 : 93.1239%
test accurac of 8 : 93.0053%
test accurac of 9 : 92.7208%
test accurac of 10 : 92.8038%
test accurac of 11 : 91.8909%
92.64% (+/-  0.33%)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Prepare-Kaggle-submission">Prepare Kaggle submission<a class="anchor-link" href="#Prepare-Kaggle-submission">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[37]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">test</span> <span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="c1">#path=r&quot;C:\Users\abdal_000\Downloads\data-set\test\audio\clip_0a77ea19a.wav&quot;</span>
    <span class="n">mfcc_feature</span><span class="o">=</span><span class="n">convert_wave_to_mfcc</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="c1">#print(mfcc_feature.shape)</span>
    
    <span class="n">pre</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">mfcc_feature</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">pre</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">csv</span>
<span class="n">audfiles</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;C:\Users\abdal_000\Downloads\data-set\test\audio&quot;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">audfiles</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;sample_submission.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">csvfile</span><span class="p">:</span>
    <span class="n">fieldnames</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;fname&#39;</span><span class="p">,</span><span class="s1">&#39;label&#39;</span><span class="p">]</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">csvfile</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">fieldnames</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">audio</span> <span class="ow">in</span> <span class="n">audfiles</span><span class="p">:</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;C:\Users\abdal_000\Downloads\data-set\test\audio</span><span class="se">\\</span><span class="s2">&quot;</span><span class="o">+</span><span class="n">audio</span><span class="p">)</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">({</span><span class="s1">&#39;fname&#39;</span><span class="p">:</span><span class="n">audio</span><span class="p">,</span><span class="s1">&#39;label&#39;</span><span class="p">:</span><span class="n">labels</span><span class="p">[</span><span class="n">pred</span><span class="p">]})</span>
    
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    </div>
  </div>
</body>

 

<script type="application/vnd.jupyter.widget-state+json">
{"state": {}, "version_major": 2, "version_minor": 0}
</script>


</html>
